{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('good day, good life.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = classifier(\"The course was really great, the mentor explained every concept in detail\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### text-generation\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generator(\n",
    "    \"once upon a time there was a king \",\n",
    "    max_length = 100,\n",
    "    num_return_sequences=2\n",
    ")\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"{'generated_text': 'once upon a time there was a king ㅠㅠㅠㅠㅠㅠㅠㅠㅠ㙠ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ�'},\n",
    " {'generated_text': \"once upon a time there was a king \\u0bfe\\u0bfe\\u0b81\\u0bfe\\u0bfe.\\n‹ I have not even heard much of the history of what is happening in India. How shall we ever be informed of such an event, if any?\\nI don't know, how far away we can be before the world's eye is set, but it will be on us. What does this mean? I think that it is because in a nation governed\"}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### zero-shot-classification\n",
    "classifier = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = classifier(\n",
    "    \"this is course about python list comprehension\",\n",
    "    candidate_labels = ['Education', 'politics', 'Engineering']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer / Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  pipeline\n",
    "from transformers import  AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = classifier('The restaurant is not terrible')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"Tokenizers split input text into smaller units called tokens. These tokens can be words, subwords, or even characters.\"\n",
    "toks = tokenizer(sq)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [\"Tokenizers split input text into smaller units called tokens. These tokens can be words, subwords, or even characters.\",\n",
    "           \"The goal is to convert raw text into a format that machine learning models can process. Since models typically work with numerical data, tokenizers play a crucial role in this conversion.\",\n",
    "           \"Incredible Chinese, Japanese, and Sushi dishes. The ambiance is modern and chic.\",\n",
    "           \"Can get crowded during peak hours.\",\n",
    "           \"Elegant Indian cuisine with a colonial touch.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Normal flow\n",
    "classifier(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### with batches\n",
    "\n",
    "batch = tokenizer(x_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### inferance in pytorch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving\n",
    "save_dir = './tokenzr' \n",
    "tokenizer.save_pretrained(save_directory=save_dir)\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading\n",
    "tok = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FineTune**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Prepare dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import os \n",
    "if os.path.exists('./yelp_review_full'):\n",
    "    print(\"loading from disk : ./yelp_review_full\")\n",
    "    dataset = load_from_disk('./yelp_review_full')\n",
    "else:\n",
    "    dataset = load_dataset(\"yelp_review_full\")\n",
    "    dataset.save_to_disk(\"./yelp_review_full\")\n",
    "\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[\"train\"]), type(dataset[\"train\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Tokenization*\n",
    "Need tokenizer to process the text and include a padding and truncation strategy to handle any variable sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`padding='max_length':`\n",
    "the tokenizer pads the tokenized text with zeros (i.e., [PAD] tokens) to make all sequences in the batch have the same length.\n",
    "If the original sentence length exceeds max_length after appending [CLS] and [SEP] tokens, padding is applied to reach the specified max_length.\n",
    "For example, if you set max_length=10, the tokenized text might look like: [101, 2026, 2171, 2003, 11754, 102, 0, 0, 0, 0], where 101 represents the [CLS] token and 102 represents the [SEP] token.\n",
    "\n",
    "\n",
    "\n",
    "`truncate=True:`\n",
    "When truncate=True, longer sentences are truncated to exactly max_length.\n",
    "This ensures that all input sequences have consistent lengths, which is crucial for tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# To process dataset in one step, use Datasets map method to apply a preprocessing function over the entire dataset\n",
    "\n",
    "if os.path.exists('./tokenized_datasets'):\n",
    "    print(\"loading from disk : ./tokenized_datasets\")\n",
    "    tokenized_datasets = load_from_disk('./tokenized_datasets')\n",
    "else:\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets.save_to_disk(\"./tokenized_datasets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Train with PyTorch Trainer*\n",
    "\n",
    "##### fine-tuning for sequence classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "### Start by loading your model and specify the number of expected labels\n",
    "### There are 5 lables\n",
    "\n",
    "if os.path.exists('./google-bert_bert-base-cased'):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('./google-bert_bert-base-cased', num_labels=5)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "    model.save_pretrained('./google-bert_bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  *Training hyperparameters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to create a TrainingArguments class which contains all the hyperparameters.\n",
    "here we are using default training hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify where to save the checkpoints from your training:\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Evaluate*<br>\n",
    "`Trainer` does not automatically evaluate model performance during training. we need to pass Trainer a function to compute and report metrics.<br>\n",
    "The `Evaluate` library provides a simple accuracy function you can load with the `evaluate.load()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call compute on metric to calculate the accuracy of your predictions. Before passing your predictions to compute, you need to convert the logits to predictions (remember all 🤗 Transformers models return logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor the evaluation metrics during fine-tuning, specify the `evaluation_strategy` parameter in your training arguments to report the evaluation metric at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Trainer*\n",
    "Create a `Trainer` object with the model, training arguments, training and test datasets, and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then fine-tune the model by calling train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./bert_base_cased_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'eval_loss': 1.0237584114074707, 'eval_accuracy': 0.578, 'eval_runtime': 19.2998, 'eval_samples_per_second': 51.814, 'eval_steps_per_second': 6.477, 'epoch': 3.0}\n",
    "{'train_runtime': 211.9045, 'train_samples_per_second': 14.157, 'train_steps_per_second': 1.77, 'train_loss': 1.033949951171875, 'epoch': 3.0}\n",
    "\n",
    "TrainOutput(global_step=375, training_loss=1.033949951171875, metrics={'train_runtime': 211.9045, 'train_samples_per_second': 14.157, 'train_steps_per_second': 1.77, 'total_flos': 789354427392000.0, 'train_loss': 1.033949951171875, 'epoch': 3.0})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer #, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"   \n",
    "\n",
    "do_ocr = True\n",
    "if do_ocr:\n",
    "    llmsherpa_api_url = llmsherpa_api_url + \"&applyOcr=yes\"\n",
    "\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc_obj = pdf_reader.read_pdf(file_path)\n",
    "    text_data = ''\n",
    "    for chunk in doc_obj.chunks():\n",
    "        text_data += chunk.to_text()\n",
    "    return text_data\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer_flan = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "\n",
    "# Process PDF data from books and user manuals\n",
    "pdf_data_path = \"./test_file\"\n",
    "text_data = \"\"\n",
    "\n",
    "for filename in os.listdir(pdf_data_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        with open(os.path.join(pdf_data_path, filename), \"rb\") as file:\n",
    "            pdf_text = extract_text_from_pdf(file)\n",
    "            text_data += pdf_text\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer_flan(text_data, return_tensors=\"pt\")\n",
    "\n",
    "# Create a TextDataset from the tokenized text\n",
    "dataset = TextDataset(tokenized_text, tokenizer=tokenizer_flan)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define Trainer for unsupervised fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer_flan),\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Perform unsupervised fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
