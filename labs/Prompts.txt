we are building a Generative ai based Web application, following are the details
 
Project Name: Workbench for SDLC Automation
Type: Web Application
Scope: Build a user-centric workbench that automates SDLC processes, featuring modular UI components and backend APIs with seamless integration.

 
for the this project the following are the modules to be built using Generative Ai, these modules are continued steps one after the other.
 
    1. Pre-Solutioning
        Description: The first step of user interaction with model. Here, the model will collect data (context for the User requirement) by asking set of predefined questions regarding the users requirement one after the other.
        There is prompt for this step, where the model will ask questions and collect the data from the user and returns the data in JSON format.
    2. Question Generation
        Description: This is the second step, the User will provide additional context by uploading a Document, giving some text and also provides a number. 
            The model take the text from the Document, user provided text and processes it and generates number of questions that user specified.
            These questions will cover different stages of Software development life cycle, which will help to get more information from the user as he needs to answer these questions.
        There is a prompt for this task which will take the input from the previous step and gives number of questions as output with category for each questions in JSON fromat.
    3. Question Re-Generation 
        Description: This is next step in the process, where the model needs to generate few more questions if the user is not satisfied by the questions that are generated in the above step. 
        Write a prompt for this step with following requirements
        - The model MUST take the questions generated in the previous step and the user's feedback.
        - The Model MUST generate new questions, user will specify how Questions to be generated.
        - Newly generated questions MUST NOT MATCH with the previous lists of Questions.
        - This should be an Iterative process until the user is satisfied with the questions.
        - Prompt should accept the following inputs and use in the prompt wherever required
            1. Number of questions
            2. Lists of previous questions
            3. Context for the Question generation 

make it as a chain of thought prompt with few examples in the prompt,
make sure the prompt will have the best formatting to make the LLM MUST follow the guidelines and will not hallucinate.
 
model should return all the questions and answers in JSON format

 
 

 ----------------------------------------------------------- Prompt 1
 Here is the prompt for the **Question Re-Generation** step, considering the requirements you've provided:

---

**Prompt for Question Re-Generation:**

---

**Objective:** 
We are here to assist you by generating new questions that better meet your needs based on the context and feedback you provide. 

### Instructions:

1. **Input from the Previous Step:**
   - Please provide the list of **previously generated questions** from the last step. These questions have already been proposed to you, but we want to refine them.
   
2. **User Feedback:**
   - You will also need to provide **feedback** on the questions you have received. Please tell us if any of the questions are unclear, irrelevant, or if they need to be more specific to certain stages of the Software Development Life Cycle (SDLC). This feedback will help us improve the questions we generate.
   
3. **Context for the Question Generation:**
   - If you have specific details or context youâ€™d like to be incorporated into the questions (such as project scope, technology stack, team size, etc.), please share them now. This context will be used to tailor the questions to your specific situation.
   
4. **Number of Questions to Generate:**
   - How many additional questions would you like us to generate? Specify the **number of questions** you would like to receive, keeping in mind that these questions will cover different aspects of the SDLC.

---

### Output Requirements:

- **New Questions:** We will generate **new questions** based on your feedback and context. These questions **must not repeat** any questions from the previous list.
- **Iterative Process:** This process can be repeated until you are satisfied with the questions. After each generation, you can provide feedback and request further changes.
- **Format:** The generated questions will be categorized into different stages of SDLC (e.g., Planning, Design, Development, Testing, Deployment, Maintenance). The output will be provided in **JSON format** as follows:

```json
{
  "questions": [
    {
      "question": "New question generated",
      "category": "Category for the question"
    },
    {
      "question": "Another new question",
      "category": "Category for the question"
    }
  ]
}
```

---

**Example Input:**

1. **Previous Questions:**  
   - "What is the scope of the project?"
   - "What is your preferred technology stack?"
   
2. **Feedback:**  
   - "The question about the scope is too broad; I need something more specific about the requirements."  
   - "The technology stack question is fine but could use more options like front-end vs back-end."

3. **Context:**  
   - "The project is focused on developing an enterprise-level web application using microservices architecture."

4. **Number of Questions:**  
   - "Please generate 5 additional questions."

---

Please provide the required information, and I will generate new questions for you. Let's refine this process to suit your needs!

---

### Model Actions:

- Use the **previous questions** and **user feedback** to modify and generate a new set of questions.
- Ensure that the new set of questions is **relevant to different SDLC stages** and tailored according to the **context** provided.
- Generate the **specified number of questions**, ensuring they are **unique** and **not duplicates** of earlier questions.

---

This prompt will guide the model to iteratively generate and refine questions based on user feedback, with each new set of questions becoming progressively more aligned with the user's requirements.




------------------------------------------------------------------------------------------------------- Prompt 2

Your are a Generative Ai assistant to help Autotmte Software Deployment Life Cycle process.
Your task is to generate a NEW set of Questions based on the following input.

Context to generate the Questions : {context}
Number of Questions : {number}
Previous Questions : {previous_questions}

You MUST have to follow the following guidelines while generating the Questions.
- You MUST understand and analyze the CONTEXT provided by the User to generate the Questions.
- Take the provided list of previous questions and ensure none of the new questions you generate are duplicates of the previous ones. 
    You MUST compare the newly generated questions with the list of previous questions and ensure they are unique.
- Generate new questions according to the number specified by the user (ensure the exact number is generated)
- You MUST categorize the questions based on different stages of the Software Development Life Cycle (SDLC). For instance, you might have questions related to requirements gathering, design, implementation, testing, etc.
- MUST return the Generated Questions in the following format: 
    {"question" : "category"}


---------------------------------------------------------------------------------------------------- Prompt 3

### **Prompt for Question Re-Generation**:


**Objective**:  
You are a **Generative Assistant** designed to help automate the **Software Development Life Cycle (SDLC)** process. Your task is to generate a **new set of questions** based on the provided context, previous questions, and user requirements.

### **Inputs Provided**:
1. **Context to Generate the Questions**: A description of the project, requirements, or specific guidelines relevant to the SDLC.
2. **Number of Questions**: The exact number of questions the user wants to generate.
3. **Previous Questions**: A list of questions generated in the previous step.

### **Guidelines**:  
- **Understand the context**: You MUST analyze the provided context carefully to generate relevant questions. This includes any specific requirements or goals mentioned by the user.
  
- **Avoid duplication**: Compare the new questions you generate with the previous questions and **ensure that no question repeats**. You MUST generate **entirely new questions** that were not part of the previous list.

- **Exact number of questions**: You MUST generate the exact number of questions specified by the user. If the user requests 5 questions, provide exactly 5. 

- **Categorization**: Your questions should be **categorized** based on different stages of the Software Development Life Cycle (SDLC). The categories may include:
  - **Requirements Gathering**
  - **Design**
  - **Implementation**
  - **Testing**
  - **Deployment**
  - **Maintenance**
  
  Each question must be relevant to the corresponding SDLC stage.
--1
sk
--2
-or-v1-
--3
eb3327ac4fc
--4
526f886627e
--5
ebfad80465e
--6
49fee9b63ca
--7
15d3bf9f802
--8
94e7677e3


---------- grq
gsk_
-----
jAgMeunx5a
-----
7tFYGPFYr6
-----
WGdyb3FYD9
-----
GIt8KU2Qij
-----
6X38QJts8
-----
ZXu

- **Return format**: Return the generated questions in the following JSON format, with each question paired with its corresponding category:
  ```json
  {"question": "category"}
  ```







questions = [
    "What are the primary business objectives?",
    "Who are the key stakeholders?",
    "What is the expected timeline for implementation?",
    "What tech stack will be used for the project?",
    "What specific features do you want in the application?",
    "Are there any existing systems that need integration?",
    "What are your security and compliance requirements?",
    "What is your preferred user interface style?",
    "How do you envision the development process? (e.g., Agile, Waterfall)",
    "What types of testing do you expect to be performed?",
    "How often do you expect updates or maintenance to occur after deployment?",
    "What support channels do you prefer for ongoing issues or enhancements?"
]
Prompt = f"""
    You are a Generative Ai assistant.
    Your task to ask the following question, collect the responses from the user and after receiving the last question response return all the data in Json Format.
    

    {questions}
 
"""
from groq import Groq

client = Groq()
completion = client.chat.completions.create(
    model="llama-3.1-70b-versatile",
    messages=[{
            "role": "user",
            "content": "HI\n"
        }
        ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=True,
    stop=None,
)

for chunk in completion:
    print(chunk.choices[0].delta.content or "", end="")

messages = []



from groq import Groq
import json

def collect_user_input(prompt):
    """Collects user input based on the questions in the prompt."""
    user_messages = []
    questions = prompt.strip().split('\n')
    for question in questions:
        if not question:  # Skip empty lines
            continue
        user_input = input(question + '\n')
        user_messages.append(user_input)
    return user_messages

def get_llm_response(prompt, user_messages):
    """Creates a Groq client and uses it to get an LLM response."""
    client = Groq()
    messages = [
        {"role": "user", "content": prompt + '\n'},
        {"role": "assistant", "content": '\n'.join(user_messages) + '\n'}
    ]
    completion = client.chat.completions.create(
        model="llama-3.1-70b-versatile",
        messages=messages,
        temperature=1,
        max_tokens=1024,
        top_p=1,
        stream=True,
        stop=None,
    )
    response = ''
    for chunk in completion:
        response += chunk.choices[0].delta.content or ''
    return response

def main():
    """Main function to execute the code."""
    # Your prompt with questions
    prompt = """
What is your name?
What is your age?
Where are you from?
"""
    user_input = collect_user_input(prompt)
    response = get_llm_response(prompt, user_input)
    output_json = {
        "prompt": prompt.strip(),
        "user_input": user_input,
        "llm_response": response.strip()
    }
    print(json.dumps(output_json, indent=4))

if __name__ == "__main__":
    main()

# In this updated code:

# *   We first define a function `collect_user_input` to collect user input based on the questions in the prompt.
# *   We then define a function `get_llm_response` to create a Groq client and use it to get an LLM response.
# *   In the `main` function, we define the prompt with questions, collect user input, get an LLM response, and then print the output in JSON format.

# Note: Make sure to install the `groq` library if it's not already installed. You can do this using pip: `pip install groq`. Also, update the `Groq` API credentials if required.
from groq import Groq

def get_llm_response(prompt):
    """Creates a Groq client and uses it to get an LLM response."""
    client = Groq()
    messages = [
        {"role": "assistant", "content": "Please answer the questions in the prompt:\n"},
        {"role": "user", "content": prompt + "\n"}
    ]
    completion = client.chat.completions.create(
        model="llama-3.1-70b-versatile",
        messages=messages,
        temperature=1,
        max_tokens=1024,
        top_p=1,
        stream=True,
        stop=None,
    )
    response = ''
    for chunk in completion:
        response += chunk.choices[0].delta.content or ''
    return {
        "prompt": prompt.strip(),
        "llm_response": response.strip()
    }

def main():
    """Main function to execute the code."""
    # Your prompt with questions
    prompt = """
What is your name?
What is your age?
Where are you from?
"""
    output_json = get_llm_response(prompt)
    print(json.dumps(output_json, indent=4))

if __name__ == "__main__":
    import json
    main()
Prompt = f"""
    You are a Generative Ai assistant.
    Your task to ask the following question, collect the responses from the user and after receiving the last question response return all the data in Json Format.

        "What are the primary business objectives?",
    "Who are the key stakeholders?",
    "What is the expected timeline for implementation?",
    "What tech stack will be used for the project?",
    "What specific features do you want in the application?",
    "Are there any existing systems that need integration?",
    "What are your security and compliance requirements?",
    "What is your preferred user interface style?",
    "How do you envision the development process? (e.g., Agile, Waterfall)",
    "What types of testing do you expect to be performed?",
    "How often do you expect updates or maintenance to occur after deployment?",
    "What support channels do you prefer for ongoing issues or enhancements?"

  
 
"""
from groq import Groq
import json

def main():
    """Main function to execute the code."""
    # Your prompt with questions
#     prompt = """
# What is your name?
# What is your age?
# Where are you from?
# """
    client = Groq()
    completion = client.chat.completions.create(
        model="llama-3.1-70b-versatile",
        messages=[
            {"role": "assistant", "content": Prompt},
            # {"role": "user", "content": Prompt + "\n"}
        ],
        temperature=1,
        max_tokens=1024,
        top_p=1,
        stream=False,
        stop=None,
    )
    output_json = {
        "prompt": Prompt.strip(),
        "llm_response": completion.choices[0].message.content.strip()
    }
    print(json.dumps(output_json, indent=4))

if __name__ == "__main__":
    main()
"""To write a system prompt for the Groq and make the LLM behave according to the prompt, you can use the following format:

```python
system_prompt = """
You are an expert at answering questions. 
Please respond in this format: 'Answer: [answer]' for each question. 
Do not provide explanations or elaborations. 
Only provide the answer.
"""
```

Then, you can use this system prompt in your Groq API call as follows:

```python
client = Groq()
completion = client.chat.completions.create(
    model="llama-3.1-70b-versatile",
    messages=[
        {"role": "assistant", "content": system_prompt},
        {"role": "user", "content": prompt + "\n"}
    ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
)
```

In this case, the LLM will respond according to the system prompt, providing answers in the specified format.

Here's a more complex system prompt that you can use:

```python
system_prompt = """
You are a conversational AI. 
You will respond to user input in a helpful and informative way. 
Please do not provide any information that is not relevant to the conversation. 
Please provide answers in a respectful and professional tone. 
If you are unsure or do not know the answer to a question, please say so. 
Do not make assumptions or provide speculative answers. 
Please respond in complete sentences and use proper grammar and spelling.
"""
```

You can also use system prompts to fine-tune the behavior of the LLM to suit your specific needs. For example, you can use a system prompt to:

*   Specify the tone or style of the response (e.g., formal, informal, humorous, etc.)
*   Define the scope of the conversation (e.g., limit the conversation to a specific topic or domain)
*   Provide context or background information that the LLM should be aware of
*   Specify the format or structure of the response (e.g., provide answers in a specific format or format)

Here are some more examples of system prompts:

```python
# Use a formal tone
system_prompt = """
You are a formal conversational AI. 
Please respond in a professional and respectful tone. 
Avoid using slang or colloquialisms. 
Use proper grammar and spelling at all times.
"""

# Be humorous and witty
system_prompt = """
You are a humorous conversational AI. 
Please respond in a lighthearted and playful way. 
Use humor and wit to make the conversation more enjoyable. 
But do not offend or make fun of the user.
"""

# Use technical jargon
system_prompt = """
You are a technical conversational AI. 
Please respond using technical terms and jargon. 
Assume the user has a basic understanding of the subject matter. 
Provide detailed explanations and definitions as needed.
"""

# Role-playing
system_prompt = """
You are a customer service representative for a company. 
Please respond in a friendly and helpful way. 
Provide information and assistance to the user as needed. 
Use the company's brand voice and tone.
"""
```"""
