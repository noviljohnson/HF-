{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer #, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"   \n",
    "\n",
    "do_ocr = True\n",
    "if do_ocr:\n",
    "    llmsherpa_api_url = llmsherpa_api_url + \"&applyOcr=yes\"\n",
    "\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc_obj = pdf_reader.read_pdf(file_path)\n",
    "    text_data = ''\n",
    "    for chunk in doc_obj.chunks():\n",
    "        text_data += chunk.to_text()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer_flan = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "# model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# # Process PDF data from books and user manuals\n",
    "# pdf_data_path = \"./test_file\"\n",
    "# text_data = \"\"\n",
    "\n",
    "# for filename in os.listdir(pdf_data_path):\n",
    "#     if filename.endswith(\".pdf\"):\n",
    "#         with open(os.path.join(pdf_data_path, filename), \"rb\") as file:\n",
    "#             pdf_text = extract_text_from_pdf(file)\n",
    "#             text_data += pdf_text\n",
    "\n",
    "# # Tokenize the text data\n",
    "# tokenized_text = tokenizer_flan(text_data, return_tensors=\"pt\")\n",
    "\n",
    "# # Create a TextDataset from the tokenized text\n",
    "# dataset = TextDataset(tokenized_text, tokenizer=tokenizer_flan)\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./output\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# # Define Trainer for unsupervised fine-tuning\n",
    "# trainer = Trainer(\n",
    "#     model=model_flan,\n",
    "#     args=training_args,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer_flan),\n",
    "#     train_dataset=dataset,\n",
    "# )\n",
    "\n",
    "# # Perform unsupervised fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "# 1. Text Extraction\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_data_path = './test_file'\n",
    "text_data = ''\n",
    "for filename in os.listdir(pdf_data_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        text_data += extract_text_from_pdf(os.path.join(pdf_data_path, filename))\n",
    "\n",
    "# 2. Text Cleaning and Preprocessing\n",
    "text_data = text_data.lower().strip()\n",
    "text_data = ''.join(char for char in text_data if char.isalnum() or char.isspace())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 4. Tokenization and Masking\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base', mask_token='[MASK]')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Dataset Formatting\n",
    "dataset = []\n",
    "for i in range(0, len(text_data) - 100, 50):\n",
    "    input_text = text_data[i:i+100]\n",
    "    expected_output = text_data[i+100:i+150]\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    dataset.append({\n",
    "        'input_ids': input_ids.squeeze(),\n",
    "        'input_text': input_text,\n",
    "        'expected_output': expected_output\n",
    "    })\n",
    "\n",
    "\n",
    "# # 3. Dataset Formatting\n",
    "# dataset = []\n",
    "# for i in range(0, len(text_data) - 100, 50):\n",
    "#     input_text = text_data[i:i+100]\n",
    "#     expected_output = text_data[i+100:i+150]\n",
    "    \n",
    "#     # Tokenize the input text\n",
    "#     input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "#     dataset.append({\n",
    "#         'input_ids': input_ids.squeeze(),\n",
    "#         'labels': tokenizer.encode(expected_output, return_tensors='pt').squeeze()\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Load the Pre-Trained Model\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 6. Define the Fine-Tuning Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./flan_t5_fine_tuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1530 [14:40<?, ?it/s]\n",
      "                                                  \n",
      " 33%|███▎      | 500/1530 [02:20<04:45,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5671, 'grad_norm': 10.753814697265625, 'learning_rate': 3.366013071895425e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 65%|██████▌   | 1000/1530 [04:36<02:43,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0844, 'grad_norm': 9.513123512268066, 'learning_rate': 1.7320261437908496e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 98%|█████████▊| 1500/1530 [06:53<00:09,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9277, 'grad_norm': 11.201403617858887, 'learning_rate': 9.80392156862745e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1530/1530 [07:02<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 422.1127, 'train_samples_per_second': 14.491, 'train_steps_per_second': 3.625, 'train_loss': 6.190517620011872, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1530, training_loss=6.190517620011872, metrics={'train_runtime': 422.1127, 'train_samples_per_second': 14.491, 'train_steps_per_second': 3.625, 'total_flos': 263305251016704.0, 'train_loss': 6.190517620011872, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 7. Perform Unsupervised Fine-Tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('./flan_t5_fine_tuned')\n",
    "trainer.model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./flan_t5_tokenizer\\\\tokenizer_config.json',\n",
       " './flan_t5_tokenizer\\\\special_tokens_map.json',\n",
       " './flan_t5_tokenizer\\\\spiece.model',\n",
       " './flan_t5_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained('./flan_t5_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rags can be used in a variety of ways ranging from rags to rags to rags to rags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = T5ForConditionalGeneration.from_pretrained('./flan_t5_fine_tuned')\n",
    "tokenizer = T5Tokenizer.from_pretrained('./flan_t5_tokenizer')\n",
    "\n",
    "# Example prompt for text generation\n",
    "prompt = \"what are some popular rag use cases?\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([13339,    13,     8,   460,  2658,  2542,    30, 23941,  2254,    16,\n",
       "            793,  1612,  3026,  1688,     3,  3916,  3301,  3264,     1]),\n",
       "  'input_text': 'proceedings of the 2021 conference on empirical methods in natural language processing  pages 989599',\n",
       "  'expected_output': '01\\nnovember 711 2021 c\\r2021 association for comput'},\n",
       " {'input_ids': tensor([    3,   189,    32,    26,     7,    16,   793,  1612,  3026,  1688,\n",
       "              3,  3916,  3301,  3264,  4542,     3,  5326, 18247,   489,  2596,\n",
       "            460,  2658,     3,    75,   460,  2658,  6028,    21,     3,   287,\n",
       "           2562,     1]),\n",
       "  'input_text': 'thods in natural language processing  pages 98959901\\nnovember 711 2021 c\\r2021 association for comput',\n",
       "  'expected_output': 'ational linguistics9895picard \\nparsing incremental'},\n",
       " {'input_ids': tensor([ 7088,     3,  5326, 18247,   489,  2596,   460,  2658,     3,    75,\n",
       "            460,  2658,  6028,    21, 25850,     3, 24703,     7,  3916,  3301,\n",
       "           6174,   986,   260,     7,    53, 28351,     1]),\n",
       "  'input_text': '01\\nnovember 711 2021 c\\r2021 association for computational linguistics9895picard \\nparsing incremental',\n",
       "  'expected_output': 'ly for constrained autoregressive decoding\\nfrom la'},\n",
       " {'input_ids': tensor([   44,  6318,     3, 24703,     7,  3916,  3301,  6174,   986,   260,\n",
       "              7,    53, 28351,   120,    21,   975, 22418,  8728,    15, 10292,\n",
       "            757,    20,  9886,    45,    50,     1]),\n",
       "  'input_text': 'ational linguistics9895picard \\nparsing incrementally for constrained autoregressive decoding\\nfrom la',\n",
       "  'expected_output': 'nguage models\\ntorsten scholak andnathan schucher a'},\n",
       " {'input_ids': tensor([    3,   120,    21,   975, 22418,  8728,    15, 10292,   757,    20,\n",
       "           9886,    45,  1612,  2250,    12,    52,  1913,     3,     7, 14297,\n",
       "           1639,    11,    29,     9,  6736,     3,     7,  8019,  1703,     3,\n",
       "              9,     1]),\n",
       "  'input_text': 'ly for constrained autoregressive decoding\\nfrom language models\\ntorsten scholak andnathan schucher a',\n",
       "  'expected_output': 'nddzmitry bahdanau\\nelementai a servicenow company\\n'},\n",
       " {'input_ids': tensor([    3,    29,  1744,   545,  2250,    12,    52,  1913,     3,     7,\n",
       "          14297,  1639,    11,    29,     9,  6736,     3,     7,  8019,  1703,\n",
       "             11,    26,   172,  1538,   651,     3, 17670,  3768,   402,  3282,\n",
       "              9,    23,     3,     9,   313,  7651,   349,     1]),\n",
       "  'input_text': 'nguage models\\ntorsten scholak andnathan schucher anddzmitry bahdanau\\nelementai a servicenow company\\n',\n",
       "  'expected_output': 'torstenscholakdzmitrybahdanauservicenowcom\\nabstrac'},\n",
       " {'input_ids': tensor([    3,   727,    26,   172,  1538,   651,     3, 17670,  3768,   402,\n",
       "           3282,     9,    23,     3,     9,   313,  7651,   349,    12,    52,\n",
       "           1913,     7, 14297,  1639,    26,   172,  1538,   651, 17670,  3768,\n",
       "            402,  5114,  7651,   287,   703,     7,  6471,     1]),\n",
       "  'input_text': 'nddzmitry bahdanau\\nelementai a servicenow company\\ntorstenscholakdzmitrybahdanauservicenowcom\\nabstrac',\n",
       "  'expected_output': 't\\nlarge pretrained language models for textual\\ndat'},\n",
       " {'input_ids': tensor([   12,    52,  1913,     7, 14297,  1639,    26,   172,  1538,   651,\n",
       "          17670,  3768,   402,  5114,  7651,   287,  9838,   508,  7140, 10761,\n",
       "           1612,  2250,    21,  1499,  3471,  3927,     1]),\n",
       "  'input_text': 'torstenscholakdzmitrybahdanauservicenowcom\\nabstract\\nlarge pretrained language models for textual\\ndat',\n",
       "  'expected_output': 'a have an unconstrained output space at\\neach decod'},\n",
       " {'input_ids': tensor([    3,    17,   508,  7140, 10761,  1612,  2250,    21,  1499,  3471,\n",
       "            331,    43,    46,    73,  1018, 22418,  3911,   628,    44,   284,\n",
       "             20,   509,    26,     1]),\n",
       "  'input_text': 't\\nlarge pretrained language models for textual\\ndata have an unconstrained output space at\\neach decod',\n",
       "  'expected_output': 'ing step they can produce any of\\n10000s of subword'},\n",
       " {'input_ids': tensor([    3,     9,    43,    46,    73,  1018, 22418,  3911,   628,    44,\n",
       "            284,    20,  9886,  1147,    79,    54,  1759,   136,    13,   335,\n",
       "           2313,     7,    13,   769,  6051,     1]),\n",
       "  'input_text': 'a have an unconstrained output space at\\neach decoding step they can produce any of\\n10000s of subword',\n",
       "  'expected_output': ' tokens when ﬁnetuned\\nto target constrained formal'},\n",
       " {'input_ids': tensor([    3,    53,  1147,    79,    54,  1759,   136,    13,   335,  2313,\n",
       "              7,    13,   769,  6051, 14145,     7,   116,  1399,    17,   444,\n",
       "             26,    12,  2387,   975, 22418,  4727,     1]),\n",
       "  'input_text': 'ing step they can produce any of\\n10000s of subword tokens when ﬁnetuned\\nto target constrained formal',\n",
       "  'expected_output': ' languages like\\nsql these models often generate in'},\n",
       " {'input_ids': tensor([14145,     7,   116,  1399,    17,   444,    26,    12,  2387,   975,\n",
       "          22418,  4727,  8024,   114, 11820,    40,   175,  2250,   557,  3806,\n",
       "             16,     1]),\n",
       "  'input_text': ' tokens when ﬁnetuned\\nto target constrained formal languages like\\nsql these models often generate in',\n",
       "  'expected_output': 'valid code\\nrendering it unusable we propose p icar'},\n",
       " {'input_ids': tensor([ 8024,   114, 11820,    40,   175,  2250,   557,  3806, 17070,  1081,\n",
       "          18968,    34,    73,   302,   179,    62,  4230,     3,   102,     3,\n",
       "             23,  1720,     1]),\n",
       "  'input_text': ' languages like\\nsql these models often generate invalid code\\nrendering it unusable we propose p icar',\n",
       "  'expected_output': 'd1\\na method for constraining autoregressive de\\ncod'},\n",
       " {'input_ids': tensor([ 3982,  1081, 18968,    34,    73,   302,   179,    62,  4230,     3,\n",
       "            102,     3,    23,  6043,   536,     3,     9,  1573,    21, 27354,\n",
       "             53,  8728,    15, 10292,   757,    20, 10763,     1]),\n",
       "  'input_text': 'valid code\\nrendering it unusable we propose p icard1\\na method for constraining autoregressive de\\ncod',\n",
       "  'expected_output': 'ers of language models through incremen\\ntal parsin'},\n",
       " {'input_ids': tensor([    3,    26,   536,     3,     9,  1573,    21, 27354,    53,  8728,\n",
       "             15, 10292,   757,    20,  1081,    52,     7,    13,  1612,  2250,\n",
       "            190,    16, 21416,    29,     3,  1947,   260,     7,    77,     1]),\n",
       "  'input_text': 'd1\\na method for constraining autoregressive de\\ncoders of language models through incremen\\ntal parsin',\n",
       "  'expected_output': 'g p icard helps to ﬁnd valid output\\nsequences by r'},\n",
       " {'input_ids': tensor([    3,   277,    13,  1612,  2250,   190,    16, 21416,    29,     3,\n",
       "           1947,   260,     7,    53,     3,   102,     3,    23,  6043,  1691,\n",
       "             12,   253,  3982,  3911,  5932,     7,    57,     3,    52,     1]),\n",
       "  'input_text': 'ers of language models through incremen\\ntal parsing p icard helps to ﬁnd valid output\\nsequences by r',\n",
       "  'expected_output': 'ejecting inadmissible tokens at\\neach decoding step'},\n",
       " {'input_ids': tensor([    3,   122,     3,   102,     3,    23,  6043,  1691,    12,   253,\n",
       "           3982,  3911,  5932,     7,    57, 15092,    53,    16, 20466,     7,\n",
       "              7,  2317, 14145,     7,    44,   284,    20,  9886,  1147,     1]),\n",
       "  'input_text': 'g p icard helps to ﬁnd valid output\\nsequences by rejecting inadmissible tokens at\\neach decoding step',\n",
       "  'expected_output': ' on the challenging spider\\nand cosql texttosql tra'},\n",
       " {'input_ids': tensor([    3,    15, 11827,    53,    16, 20466,     7,     7,  2317, 14145,\n",
       "              7,    44,   284,    20,  9886,  1147,    30,     8,  4421, 18612,\n",
       "             11,   576,     7,  1824,    40,  1499,   235,     7,  1824,    40,\n",
       "              3,  1313,     1]),\n",
       "  'input_text': 'ejecting inadmissible tokens at\\neach decoding step on the challenging spider\\nand cosql texttosql tra',\n",
       "  'expected_output': 'nslation tasks we\\nshow that p icard transforms ﬁne'},\n",
       " {'input_ids': tensor([   30,     8,  4421, 18612,    11,   576,     7,  1824,    40,  1499,\n",
       "            235,     7,  1824,    40,  7314,  4145,    62,   504,    24,     3,\n",
       "            102,     3,    23,  6043,  3343,     7,  1399,     1]),\n",
       "  'input_text': ' on the challenging spider\\nand cosql texttosql translation tasks we\\nshow that p icard transforms ﬁne',\n",
       "  'expected_output': 'tuned t5\\nmodels with passable performance into sta'},\n",
       " {'input_ids': tensor([   3,   29,    7, 6105, 4145,   62,  504,   24,    3,  102,    3,   23,\n",
       "          6043, 3343,    7, 1399,   17,  444,   26,    3,   17,  755, 2250,   28,\n",
       "          1903,  179,  821,  139, 3342,    1]),\n",
       "  'input_text': 'nslation tasks we\\nshow that p icard transforms ﬁnetuned t5\\nmodels with passable performance into sta',\n",
       "  'expected_output': 'te\\noftheart solutions\\n1 introduction\\nwhile there h'},\n",
       " {'input_ids': tensor([14007,     3,    17,   755,  2250,    28,  1903,   179,   821,   139,\n",
       "            538,    13,   532,  1408,  1275,   209,  5302,   298,   132,     3,\n",
       "            107,     1]),\n",
       "  'input_text': 'tuned t5\\nmodels with passable performance into state\\noftheart solutions\\n1 introduction\\nwhile there h',\n",
       "  'expected_output': 'ave been many successes in applying\\nlarge pretrain'},\n",
       " {'input_ids': tensor([    3,    17,    15,    13,   532,  1408,  1275,   209,  5302,   298,\n",
       "            132,    43,   118,   186, 21231,    16,  6247,   508,   554,  9719,\n",
       "              1]),\n",
       "  'input_text': 'te\\noftheart solutions\\n1 introduction\\nwhile there have been many successes in applying\\nlarge pretrain',\n",
       "  'expected_output': 'ed language models to downstream\\ntasks our ability'},\n",
       " {'input_ids': tensor([    3,     9,   162,   118,   186, 21231,    16,  6247,   508,  7140,\n",
       "          10761,  1612,  2250,    12, 26804,  4145,    69,  1418,     1]),\n",
       "  'input_text': 'ave been many successes in applying\\nlarge pretrained language models to downstream\\ntasks our ability',\n",
       "  'expected_output': ' to control and constrain the out\\nput of these mod'},\n",
       " {'input_ids': tensor([    3,    15,    26,  1612,  2250,    12, 26804,  4145,    69,  1418,\n",
       "             12,   610,    11, 27354,     8,    91,   474,    13,   175,  1794,\n",
       "              1]),\n",
       "  'input_text': 'ed language models to downstream\\ntasks our ability to control and constrain the out\\nput of these mod',\n",
       "  'expected_output': 'els is still very limited many\\nenterprise applicat'},\n",
       " {'input_ids': tensor([   12,   610,    11, 27354,     8,    91,   474,    13,   175,  2250,\n",
       "             19,   341,   182,  1643,   186,  5399,     3, 27515,  2138,     1]),\n",
       "  'input_text': ' to control and constrain the out\\nput of these models is still very limited many\\nenterprise applicat',\n",
       "  'expected_output': 'ions are out of reach because\\nthey require a degre'},\n",
       " {'input_ids': tensor([   3, 3573,   19,  341,  182, 1643,  186, 5399, 1564,   33,   91,   13,\n",
       "          1535,  250,   79, 1457,    3,    9,   20, 3584,    1]),\n",
       "  'input_text': 'els is still very limited many\\nenterprise applications are out of reach because\\nthey require a degre',\n",
       "  'expected_output': 'e of rigour and exactitude that\\nlanguage models ar'},\n",
       " {'input_ids': tensor([    3,  2865,    33,    91,    13,  1535,   250,    79,  1457,     3,\n",
       "              9,  1952,    13,     3,  3380,  1211,    11,  2883, 20341,    24,\n",
       "           1612,  2250,  1584,     1]),\n",
       "  'input_text': 'ions are out of reach because\\nthey require a degree of rigour and exactitude that\\nlanguage models ar',\n",
       "  'expected_output': 'e not able to deliver yet if the\\ntarget is a forma'},\n",
       " {'input_ids': tensor([    3,    15,    13,     3,  3380,  1211,    11,  2883, 20341,    24,\n",
       "           1612,  2250,    33,    59,     3,   179,    12,  2156,   780,     3,\n",
       "             99,     8,  2387,    19,     3,     9,  8066,     1]),\n",
       "  'input_text': 'e of rigour and exactitude that\\nlanguage models are not able to deliver yet if the\\ntarget is a forma',\n",
       "  'expected_output': 'l language like sql then we would\\nlike the model t'},\n",
       " {'input_ids': tensor([    3,    15,    59,     3,   179,    12,  2156,   780,     3,    99,\n",
       "              8,  2387,    19,     3,     9,  4727,  1612,   114, 11820,    40,\n",
       "            258,    62,   133,   114,     8,   825,     3,    17,     1]),\n",
       "  'input_text': 'e not able to deliver yet if the\\ntarget is a formal language like sql then we would\\nlike the model t',\n",
       "  'expected_output': 'o adhere exactly and provably to the\\nsql speciﬁcat'},\n",
       " {'input_ids': tensor([    3,    40,  1612,   114, 11820,    40,   258,    62,   133,   114,\n",
       "              8,   825,    12, 13859,  1776,    11,   813,   208,  3834,    12,\n",
       "              8, 11820,    40,   806,   144,     1]),\n",
       "  'input_text': 'l language like sql then we would\\nlike the model to adhere exactly and provably to the\\nsql speciﬁcat',\n",
       "  'expected_output': 'ion with all its lexical grammatical\\nlogical and s'},\n",
       " {'input_ids': tensor([    3,    32, 13859,  1776,    11,   813,   208,  3834,    12,     8,\n",
       "          11820,    40, 16726,    28,    66,   165,     3, 30949,   138,     3,\n",
       "           5096,  4992,   138,     3,  6207,    11,     3,     7,     1]),\n",
       "  'input_text': 'o adhere exactly and provably to the\\nsql speciﬁcation with all its lexical grammatical\\nlogical and s',\n",
       "  'expected_output': 'emantical constraints unfortunately\\nwith pretraini'},\n",
       " {'input_ids': tensor([    3,    23,   106,    28,    66,   165,     3, 30949,   138,     3,\n",
       "           5096,  4992,   138,     3,  6207,    11, 27632,   138, 17765, 12050,\n",
       "             28,   554,  9719,    23,     1]),\n",
       "  'input_text': 'ion with all its lexical grammatical\\nlogical and semantical constraints unfortunately\\nwith pretraini',\n",
       "  'expected_output': 'ng alone language models may not\\nsatisfy these cor'},\n",
       " {'input_ids': tensor([    3,    15,   348,    17,  1950, 17765, 12050,    28,   554, 13023,\n",
       "           2238,  1612,  2250,   164,    59, 11132,   175,  4301,     1]),\n",
       "  'input_text': 'emantical constraints unfortunately\\nwith pretraining alone language models may not\\nsatisfy these cor',\n",
       "  'expected_output': 'rectness requirements\\nfor texttosql translation th'},\n",
       " {'input_ids': tensor([    3,  1725,  2238,  1612,  2250,   164,    59, 11132,   175,  2024,\n",
       "            655,  1502,    21,  1499,   235,     7,  1824,    40,  7314,     3,\n",
       "            189,     1]),\n",
       "  'input_text': 'ng alone language models may not\\nsatisfy these correctness requirements\\nfor texttosql translation th',\n",
       "  'expected_output': 'e most widespread\\nsolution to constrained decoding'},\n",
       " {'input_ids': tensor([    3, 12621,   655,  1502,    21,  1499,   235,     7,  1824,    40,\n",
       "           7314,     8,   167, 14047,  1127,    12,   975, 22418,    20,  9886,\n",
       "              1]),\n",
       "  'input_text': 'rectness requirements\\nfor texttosql translation the most widespread\\nsolution to constrained decoding',\n",
       "  'expected_output': ' is to make invalid\\nsql unrepresentable for a whil'},\n",
       " {'input_ids': tensor([    3,    15,   167, 14047,  1127,    12,   975, 22418,    20,  9886,\n",
       "             19,    12,   143, 17070, 11820,    40,    73,    60, 12640,   179,\n",
       "             21,     3,     9,  7096,     1]),\n",
       "  'input_text': 'e most widespread\\nsolution to constrained decoding is to make invalid\\nsql unrepresentable for a whil',\n",
       "  'expected_output': 'e now it has been\\npossible to restrict autoregress'},\n",
       " {'input_ids': tensor([   19,    12,   143, 17070, 11820,    40,    73,    60, 12640,   179,\n",
       "             21,     3,     9,   298,   230,    34,    65,   118,   487,    12,\n",
       "          18395,  8728,    15, 10292,     1]),\n",
       "  'input_text': ' is to make invalid\\nsql unrepresentable for a while now it has been\\npossible to restrict autoregress',\n",
       "  'expected_output': 'ive decoding to only\\nthose token sequences that co'},\n",
       " {'input_ids': tensor([    3,    15,   230,    34,    65,   118,   487,    12, 18395,  8728,\n",
       "             15, 10292,   757,    20,  9886,    12,   163,   273, 14145,  5932,\n",
       "              7,    24,   576,     1]),\n",
       "  'input_text': 'e now it has been\\npossible to restrict autoregressive decoding to only\\nthose token sequences that co',\n",
       "  'expected_output': 'rrectly parse to sql\\nabstract syntax trees yin and'},\n",
       " {'input_ids': tensor([    3,   757,    20,  9886,    12,   163,   273, 14145,  5932,     7,\n",
       "             24,  6549,   260,     7,    15,    12, 11820,    40,  9838, 28230,\n",
       "           3124,     3,    63,    77,    11,     1]),\n",
       "  'input_text': 'ive decoding to only\\nthose token sequences that correctly parse to sql\\nabstract syntax trees yin and',\n",
       "  'expected_output': ' neubig 2018 lin\\net al 2019 wang et al 2020 more r'},\n",
       " {'input_ids': tensor([    3,    52, 12621,   120,   260,     7,    15,    12, 11820,    40,\n",
       "           9838, 28230,  3124,     3,    63,    77,    11,  5854, 12911,   846,\n",
       "              3,    40,    77,     3,    15,    17,   491,  1360,     3, 17789,\n",
       "              3,    15,    17,   491,  6503,    72,     3,    52,     1]),\n",
       "  'input_text': 'rrectly parse to sql\\nabstract syntax trees yin and neubig 2018 lin\\net al 2019 wang et al 2020 more r',\n",
       "  'expected_output': 'ecently\\nsemiautoregressive improvements to this pa'},\n",
       " {'input_ids': tensor([ 5854, 12911,   846,     3,    40,    77,     3,    15,    17,   491,\n",
       "           1360,     3, 17789,     3,    15,    17,   491,  6503,    72,  1310,\n",
       "           4772,  8010,    60, 10292,   757,  6867,    12,    48,  2576,     1]),\n",
       "  'input_text': ' neubig 2018 lin\\net al 2019 wang et al 2020 more recently\\nsemiautoregressive improvements to this pa',\n",
       "  'expected_output': 'rsing\\n1the picard code is available at httpsgithub'},\n",
       " {'input_ids': tensor([    3,    15,  3728,   120,  4772,  8010,    60, 10292,   757,  6867,\n",
       "             12,    48,   260,     7,    53,   209,   532,  6686,   986,  1081,\n",
       "             19,   347,    44,  4893, 12651, 16420,     1]),\n",
       "  'input_text': 'ecently\\nsemiautoregressive improvements to this parsing\\n1the picard code is available at httpsgithub',\n",
       "  'expected_output': '\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n05005506'},\n",
       " {'input_ids': tensor([    3,    52,     7,    53,   209,   532,  6686,   986,  1081,    19,\n",
       "            347,    44,  4893, 12651, 16420,   369,  3335,     9,    23,  6174,\n",
       "            986,   209,   204,   314,   505,   898, 11638,   812,     3,   632,\n",
       "           2560,  3769,  5176,     1]),\n",
       "  'input_text': 'rsing\\n1the picard code is available at httpsgithub\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n05005506',\n",
       "  'expected_output': '0065070075\\nexact match accuracy\\nt5base\\n t5large\\n t'},\n",
       " {'input_ids': tensor([  369,  3335,     9,    23,  6174,   986,   209,   204,   314,   505,\n",
       "            898, 11638,   812,     3,   632,  2560, 17147,  6007, 15348,  9295,\n",
       "           3072,  2883,  1588,  7452,     3,    17,   755, 10925,     3,    17,\n",
       "            755, 15599,     3,    17,     1]),\n",
       "  'input_text': '\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n050055060065070075\\nexact match accuracy\\nt5base\\n t5large\\n t',\n",
       "  'expected_output': '53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch '},\n",
       " {'input_ids': tensor([    3,  1206, 15348,  9295,  3072,  2883,  1588,  7452,     3,    17,\n",
       "            755, 10925,     3,    17,   755, 15599,     3,    17,  4867,   115,\n",
       "           5839,   420,   357,   420,   591,   420,   927,  2320,   209,  2883,\n",
       "           2244, 19515,     1]),\n",
       "  'input_text': '0065070075\\nexact match accuracy\\nt5base\\n t5large\\n t53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch ',\n",
       "  'expected_output': 'accuracy of the highest\\nscoring prediction as a fu'},\n",
       " {'input_ids': tensor([12210,   115,  5839,   420,   357,   420,   591,   420,   927,  2320,\n",
       "            209,  2883,  2244, 19515,  7452,    13,     8,  2030, 10389, 21332,\n",
       "             38,     3,     9,  7683,     1]),\n",
       "  'input_text': '53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch accuracy of the highest\\nscoring prediction as a fu',\n",
       "  'expected_output': 'nction of beam size on the spi\\nder texttosql devel'},\n",
       " {'input_ids': tensor([ 7452,    13,     8,  2030, 10389, 21332,    38,     3,     9,  1681,\n",
       "             13, 11638,   812,    30,     8,     3,  7675,    74,  1499,   235,\n",
       "              7,  1824,    40,    20,  4911,     1]),\n",
       "  'input_text': 'accuracy of the highest\\nscoring prediction as a function of beam size on the spi\\nder texttosql devel',\n",
       "  'expected_output': 'opment set with p icard turned\\non token prediction'},\n",
       " {'input_ids': tensor([    3,    29,  4985,    13, 11638,   812,    30,     8,     3,  7675,\n",
       "             74,  1499,   235,     7,  1824,    40,   606,   356,    28,     3,\n",
       "            102,     3,    23,  6043,  2120,    30, 14145, 21332,     1]),\n",
       "  'input_text': 'nction of beam size on the spi\\nder texttosql development set with p icard turned\\non token prediction',\n",
       "  'expected_output': 's had to pass p icard checking at\\nevery decoding s'},\n",
       " {'input_ids': tensor([    3,    32,   102,   297,   356,    28,     3,   102,     3,    23,\n",
       "           6043,  2120,    30, 14145, 20099,   141,    12,  1903,     3,   102,\n",
       "              3,    23,  6043,  6450,    44,   334,    20,  9886,     3,     7,\n",
       "              1]),\n",
       "  'input_text': 'opment set with p icard turned\\non token predictions had to pass p icard checking at\\nevery decoding s',\n",
       "  'expected_output': 'tep only the top2 4 and 8 token\\npredictions of eac'},\n",
       " {'input_ids': tensor([    3,     7,   141,    12,  1903,     3,   102,     3,    23,  6043,\n",
       "           6450,    44,   334,    20,  9886,  1147,   163,     8,   420,   357,\n",
       "            314,    11,   505, 14145, 20099,    13,     3,    15,     9,    75,\n",
       "              1]),\n",
       "  'input_text': 's had to pass p icard checking at\\nevery decoding step only the top2 4 and 8 token\\npredictions of eac',\n",
       "  'expected_output': 'h hypothesis were considered in the\\nbeam search wi'},\n",
       " {'input_ids': tensor([    3,    17,    15,   102,   163,     8,   420,   357,   314,    11,\n",
       "            505, 14145, 20099,    13,   284, 22455,   130,  1702,    16,     8,\n",
       "          11638,   960, 11064,     1]),\n",
       "  'input_text': 'tep only the top2 4 and 8 token\\npredictions of each hypothesis were considered in the\\nbeam search wi',\n",
       "  'expected_output': 'th p icard turned off none all token\\npredictions w'},\n",
       " {'input_ids': tensor([    3,   107, 22455,   130,  1702,    16,     8, 11638,   960,    28,\n",
       "              3,   102,     3,    23,  6043,  2120,   326,  5839,    66, 14145,\n",
       "          20099,     3,   210,     1]),\n",
       "  'input_text': 'h hypothesis were considered in the\\nbeam search with p icard turned off none all token\\npredictions w',\n",
       "  'expected_output': 'ere considered and none were checked\\nthe models t5'},\n",
       " {'input_ids': tensor([    3,   189,     3,   102,     3,    23,  6043,  2120,   326,  5839,\n",
       "             66, 14145, 20099,   130,  1702,    11,  5839,   130,  7122,     8,\n",
       "           2250,     3,    17,   755,     1]),\n",
       "  'input_text': 'th p icard turned off none all token\\npredictions were considered and none were checked\\nthe models t5',\n",
       "  'expected_output': 'base large and 3b did not have\\naccess to any datab'},\n",
       " {'input_ids': tensor([    3,    49,    15,  1702,    11,  5839,   130,  7122,     8,  2250,\n",
       "              3,    17,   755, 10925,   508,    11,   220,   115,   410,    59,\n",
       "             43,   592,    12,   136,   331,   115,     1]),\n",
       "  'input_text': 'ere considered and none were checked\\nthe models t5base large and 3b did not have\\naccess to any datab',\n",
       "  'expected_output': 'ase content only to the database\\nschemas\\nparadigm '},\n",
       " {'input_ids': tensor([ 1247,   508,    11,   220,   115,   410,    59,    43,   592,    12,\n",
       "            136,  3501,   738,   163,    12,     8,  3501, 26622,     7, 20491,\n",
       "              1]),\n",
       "  'input_text': 'base large and 3b did not have\\naccess to any database content only to the database\\nschemas\\nparadigm ',\n",
       "  'expected_output': 'have been proposed rubin and berant\\n2021 however w'},\n",
       " {'input_ids': tensor([   38,    15,   738,   163,    12,     8,  3501, 26622,     7, 20491,\n",
       "             43,   118,  4382,  9641,    77,    11,    36,  3569,   460,  2658,\n",
       "            983,     3,   210,     1]),\n",
       "  'input_text': 'ase content only to the database\\nschemas\\nparadigm have been proposed rubin and berant\\n2021 however w',\n",
       "  'expected_output': 'hile effective these approaches\\nhave in common tha'},\n",
       " {'input_ids': tensor([  43,  118, 4382, 9641,   77,   11,   36, 3569,  460, 2658,  983,  298,\n",
       "          1231,  175, 6315,   43,   16, 1017,    3,  189,    9,    1]),\n",
       "  'input_text': 'have been proposed rubin and berant\\n2021 however while effective these approaches\\nhave in common tha',\n",
       "  'expected_output': 't they are achieved at the ex\\npense of using a cus'},\n",
       " {'input_ids': tensor([   3,  107,  699, 1231,  175, 6315,   43,   16, 1017,   24,   79,   33,\n",
       "          5153,   44,    8, 1215, 9790,   13,  338,    3,    9,  123,    7,    1]),\n",
       "  'input_text': 'hile effective these approaches\\nhave in common that they are achieved at the ex\\npense of using a cus',\n",
       "  'expected_output': 'tom vocabulary of special con\\ntrol tokens or a cus'},\n",
       " {'input_ids': tensor([    3,    17,    79,    33,  5153,    44,     8,  1215,  9790,    13,\n",
       "            338,     3,     9,  1653, 19067,    13,   534,   975,     3,    17,\n",
       "           3491, 14145,     7,    42,     3,     9,   123,     7,     1]),\n",
       "  'input_text': 't they are achieved at the ex\\npense of using a custom vocabulary of special con\\ntrol tokens or a cus',\n",
       "  'expected_output': 'tom model architecture or both\\nunfortunately this '},\n",
       " {'input_ids': tensor([   12,    51, 19067,    13,   534,   975,     3,    17,  3491, 14145,\n",
       "              7,    42,     3,     9,  1653,   825,  4648,    42,   321, 12050,\n",
       "             48,     1]),\n",
       "  'input_text': 'tom vocabulary of special con\\ntrol tokens or a custom model architecture or both\\nunfortunately this ',\n",
       "  'expected_output': 'makes them incompatible with\\ngeneric pretrained la'},\n",
       " {'input_ids': tensor([   12,    51,   825,  4648,    42,   321, 12050,    48,   656,   135,\n",
       "             16, 25383,    28,  8165,  7140, 10761,    50,     1]),\n",
       "  'input_text': 'tom model architecture or both\\nunfortunately this makes them incompatible with\\ngeneric pretrained la',\n",
       "  'expected_output': 'nguage model decoders a\\nless invasive and more com'},\n",
       " {'input_ids': tensor([  656,   135,    16, 25383,    28,  8165,  7140, 10761,  1612,   825,\n",
       "             20,  4978,    52,     7,     3,     9,   705,     3, 15267,    11,\n",
       "             72,     3,   287,     1]),\n",
       "  'input_text': 'makes them incompatible with\\ngeneric pretrained language model decoders a\\nless invasive and more com',\n",
       "  'expected_output': 'patible approach is to\\nnot constrain the generatio'},\n",
       " {'input_ids': tensor([    3,    29,  1744,   545,   825,    20,  4978,    52,     7,     3,\n",
       "              9,   705,     3, 15267,    11,    72,  7441,  1295,    19,    12,\n",
       "             59, 27354,     8,  6510,  6850,    32,     1]),\n",
       "  'input_text': 'nguage model decoders a\\nless invasive and more compatible approach is to\\nnot constrain the generatio',\n",
       "  'expected_output': 'n process but instead to\\nﬁlter ﬁnalized beam hypot'},\n",
       " {'input_ids': tensor([ 6234,  2317,  1295,    19,    12,    59, 27354,     8,  3381,   433,\n",
       "             68,  1446,    12,  4191,   804,  1601, 11638, 10950,    17,     1]),\n",
       "  'input_text': 'patible approach is to\\nnot constrain the generation process but instead to\\nﬁlter ﬁnalized beam hypot',\n",
       "  'expected_output': 'heses by validity suhr\\net al 2020 lin et al 2020 y'},\n",
       " {'input_ids': tensor([    3,    29,   433,    68,  1446,    12,  4191,   804,  1601, 11638,\n",
       "          10950, 19712,     7,    57, 21264,     3,     7, 19290,     3,    15,\n",
       "             17,   491,  6503,     3,    40,    77,     3,    15,    17,   491,\n",
       "           6503,     3,    63,     1]),\n",
       "  'input_text': 'n process but instead to\\nﬁlter ﬁnalized beam hypotheses by validity suhr\\net al 2020 lin et al 2020 y',\n",
       "  'expected_output': 'et such ﬁltering is\\nat the expense of a very large'},\n",
       " {'input_ids': tensor([    3,    88,  2260,    57, 21264,     3,     7, 19290,     3,    15,\n",
       "             17,   491,  6503,     3,    40,    77,     3,    15,    17,   491,\n",
       "           6503,   780,   224,  4191,    53,    19,    44,     8,  8225,    13,\n",
       "              3,     9,   182,   508,     1]),\n",
       "  'input_text': 'heses by validity suhr\\net al 2020 lin et al 2020 yet such ﬁltering is\\nat the expense of a very large',\n",
       "  'expected_output': ' beam size\\nwe address the expenses of these approa'},\n",
       " {'input_ids': tensor([    3,    15,    17,   224,  4191,    53,    19,    44,     8,  8225,\n",
       "             13,     3,     9,   182,   508, 11638,   812,    62,  1115,     8,\n",
       "           5159,    13,   175,     3, 12497,     9,     1]),\n",
       "  'input_text': 'et such ﬁltering is\\nat the expense of a very large beam size\\nwe address the expenses of these approa',\n",
       "  'expected_output': 'ches\\nwith a novel incremental parsing method for c'},\n",
       " {'input_ids': tensor([11638,   812,    62,  1115,     8,  5159,    13,   175,  6315,    28,\n",
       "              3,     9,  3714, 28351,   260,     7,    53,  1573,    21,     3,\n",
       "             75,     1]),\n",
       "  'input_text': ' beam size\\nwe address the expenses of these approaches\\nwith a novel incremental parsing method for c',\n",
       "  'expected_output': 'on\\nstrained decoding called picard  which stands\\nf'},\n",
       " {'input_ids': tensor([    3,  2951,    28,     3,     9,  3714, 28351,   260,     7,    53,\n",
       "           1573,    21,   975,     3, 22418,    20,  9886,   718,  6686,   986,\n",
       "             84,  5024,     3,    89,     1]),\n",
       "  'input_text': 'ches\\nwith a novel incremental parsing method for con\\nstrained decoding called picard  which stands\\nf',\n",
       "  'expected_output': 'or parsing incrementally for constrained auto9896\\n'},\n",
       " {'input_ids': tensor([   30,     3, 22418,    20,  9886,   718,  6686,   986,    84,  5024,\n",
       "             21,   260,     7,    53, 28351,   120,    21,   975, 22418,  1510,\n",
       "           3916,  4314,     1]),\n",
       "  'input_text': 'on\\nstrained decoding called picard  which stands\\nfor parsing incrementally for constrained auto9896\\n',\n",
       "  'expected_output': 'figure 2 illustration of constrained beam search w'},\n",
       " {'input_ids': tensor([   42,   260,     7,    53, 28351,   120,    21,   975, 22418,  1510,\n",
       "           3916,  4314,  2320,   204, 16779,    13,   975, 22418, 11638,   960,\n",
       "              3,   210,     1]),\n",
       "  'input_text': 'or parsing incrementally for constrained auto9896\\nfigure 2 illustration of constrained beam search w',\n",
       "  'expected_output': 'ith\\nbeam size 2and p icard  each vertical column r'},\n",
       " {'input_ids': tensor([ 2320,   204, 16779,    13,   975, 22418, 11638,   960,    28, 11638,\n",
       "            812,   204,   232,     3,   102,     3,    23,  6043,   284,  5857,\n",
       "           6710,     3,    52,     1]),\n",
       "  'input_text': 'figure 2 illustration of constrained beam search with\\nbeam size 2and p icard  each vertical column r',\n",
       "  'expected_output': 'epre\\nsents three token predictions for a hypothesi'},\n",
       " {'input_ids': tensor([   34,   107, 11638,   812,   204,   232,     3,   102,     3,    23,\n",
       "           6043,   284,  5857,  6710,  3852,    60,  1622,     7,   386, 14145,\n",
       "          20099,    21,     3,     9, 10950,   532,     7,    23,     1]),\n",
       "  'input_text': 'ith\\nbeam size 2and p icard  each vertical column repre\\nsents three token predictions for a hypothesi',\n",
       "  'expected_output': 's from top\\nto bottom in descending order by probab'},\n",
       " {'input_ids': tensor([    3,    15,  2026,  1622,     7,   386, 14145, 20099,    21,     3,\n",
       "              9, 22455,    45,   420,    12,  2007,    16,     3, 30960,   455,\n",
       "             57,   813, 12534,     1]),\n",
       "  'input_text': 'epre\\nsents three token predictions for a hypothesis from top\\nto bottom in descending order by probab',\n",
       "  'expected_output': 'ility in this\\nexample p icard is conﬁgured to only'},\n",
       " {'input_ids': tensor([    3,     7,    45,   420,    12,  2007,    16,     3, 30960,   455,\n",
       "             57, 15834,    16,    48,   677,     3,   102,     3,    23,  6043,\n",
       "             19, 15786,    12,   163,     1]),\n",
       "  'input_text': 's from top\\nto bottom in descending order by probability in this\\nexample p icard is conﬁgured to only',\n",
       "  'expected_output': ' check the top\\n2 highest ones the rest is automati'},\n",
       " {'input_ids': tensor([    3, 14277,    16,    48,   677,     3,   102,     3,    23,  6043,\n",
       "             19, 15786,    12,   163,   691,     8,   420,   204,  2030,  2102,\n",
       "              8,   880,    19,  7791,    23,     1]),\n",
       "  'input_text': 'ility in this\\nexample p icard is conﬁgured to only check the top\\n2 highest ones the rest is automati',\n",
       "  'expected_output': 'cally dismissed by\\nsetting their score to 1 tokens'},\n",
       " {'input_ids': tensor([  691,     8,   420,   204,  2030,  2102,     8,   880,    19,  3269,\n",
       "          19664,    57,  1898,    70,  2604,    12,   209, 14145,     7,     1]),\n",
       "  'input_text': ' check the top\\n2 highest ones the rest is automatically dismissed by\\nsetting their score to 1 tokens',\n",
       "  'expected_output': ' rejected by p icard\\nred are also assigned a score'},\n",
       " {'input_ids': tensor([  580,    63, 19664,    57,  1898,    70,  2604,    12,   209, 14145,\n",
       "              7, 12967,    57,     3,   102,     3,    23,  6043,  1131,    33,\n",
       "             92,  7604,     3,     9,  2604,     1]),\n",
       "  'input_text': 'cally dismissed by\\nsetting their score to 1 tokens rejected by p icard\\nred are also assigned a score',\n",
       "  'expected_output': ' of 1 accepted\\ntokens green  keep their original s'},\n",
       " {'input_ids': tensor([12967,    57,     3,   102,     3,    23,  6043,  1131,    33,    92,\n",
       "           7604,     3,     9,  2604,    13,   209,  4307, 14145,     7,  1442,\n",
       "            453,    70,   926,     3,     7,     1]),\n",
       "  'input_text': ' rejected by p icard\\nred are also assigned a score of 1 accepted\\ntokens green  keep their original s',\n",
       "  'expected_output': 'core\\nregressive decoding picard is compatible with'},\n",
       " {'input_ids': tensor([   13,   209,  4307, 14145,     7,  1442,   453,    70,   926,  2604,\n",
       "              3,    60, 10292,   757,    20,  9886,  6686,   986,    19,  7441,\n",
       "             28,     1]),\n",
       "  'input_text': ' of 1 accepted\\ntokens green  keep their original score\\nregressive decoding picard is compatible with',\n",
       "  'expected_output': '\\nany existing autoregressive language model de\\ncod'},\n",
       " {'input_ids': tensor([ 2583,     3,    60, 10292,   757,    20,  9886,  6686,   986,    19,\n",
       "           7441,    28,   136,  1895,  8728,    15, 10292,   757,  1612,   825,\n",
       "             20, 10763,     1]),\n",
       "  'input_text': 'core\\nregressive decoding picard is compatible with\\nany existing autoregressive language model de\\ncod',\n",
       "  'expected_output': 'er and vocabularyincluding but not limited\\nto thos'},\n",
       " {'input_ids': tensor([  136,  1895,  8728,    15, 10292,   757,  1612,   825,    20,  1081,\n",
       "             52,    11, 19067,  5751,    68,    59,  1643,    12,     3,    17,\n",
       "          11982,     1]),\n",
       "  'input_text': '\\nany existing autoregressive language model de\\ncoder and vocabularyincluding but not limited\\nto thos',\n",
       "  'expected_output': 'e of large pretrained transformersand it\\ndoes not '},\n",
       " {'input_ids': tensor([    3,    49,    11, 19067,  5751,    68,    59,  1643,    12,   273,\n",
       "             13,   508,  7140, 10761, 19903,     7,   232,    34,   405,    59,\n",
       "              1]),\n",
       "  'input_text': 'er and vocabularyincluding but not limited\\nto those of large pretrained transformersand it\\ndoes not ',\n",
       "  'expected_output': 'require very large beam sizes picard is\\nentirely a'},\n",
       " {'input_ids': tensor([    3,    15,    13,   508,  7140, 10761, 19903,     7,   232,    34,\n",
       "            405,    59,  1457,   182,   508, 11638,  4342,  6686,   986,    19,\n",
       "           4585,     3,     9,     1]),\n",
       "  'input_text': 'e of large pretrained transformersand it\\ndoes not require very large beam sizes picard is\\nentirely a',\n",
       "  'expected_output': 'bsent from pretraining or ﬁnetuning of\\nthe model a'},\n",
       " {'input_ids': tensor([ 1457,   182,   508, 11638,  4342,  6686,   986,    19,  4585, 14101,\n",
       "             45,   554, 13023,    42,  1399,    17,   202,    53,    13,     8,\n",
       "            825,     3,     9,     1]),\n",
       "  'input_text': 'require very large beam sizes picard is\\nentirely absent from pretraining or ﬁnetuning of\\nthe model a',\n",
       "  'expected_output': 'nd can be easily and optionally enabled\\nat inferen'},\n",
       " {'input_ids': tensor([    3,   115,  5277,    45,   554, 13023,    42,  1399,    17,   202,\n",
       "             53,    13,     8,   825,    11,    54,    36,  1153,    11,  9042,\n",
       "            120,  9367,    44,    16,  1010,    35,     1]),\n",
       "  'input_text': 'bsent from pretraining or ﬁnetuning of\\nthe model and can be easily and optionally enabled\\nat inferen',\n",
       "  'expected_output': 'ce time picard operates directly on the\\noutput of '},\n",
       " {'input_ids': tensor([    3,   727,    54,    36,  1153,    11,  9042,   120,  9367,    44,\n",
       "             16, 11788,    97,  6686,   986, 10291,  1461,    30,     8,  3911,\n",
       "             13,     1]),\n",
       "  'input_text': 'nd can be easily and optionally enabled\\nat inference time picard operates directly on the\\noutput of ',\n",
       "  'expected_output': 'the language model which in the case\\nof texttosql '},\n",
       " {'input_ids': tensor([  197,    97,  6686,   986, 10291,  1461,    30,     8,  3911,    13,\n",
       "              8,  1612,   825,    84,    16,     8,   495,    13,  1499,   235,\n",
       "              7,  1824,    40,     1]),\n",
       "  'input_text': 'ce time picard operates directly on the\\noutput of the language model which in the case\\nof texttosql ',\n",
       "  'expected_output': 'translation is the readable surface\\nform of the sq'},\n",
       " {'input_ids': tensor([    8,  1612,   825,    84,    16,     8,   495,    13,  1499,   235,\n",
       "              7,  1824,    40,  7314,    19,     8,     3, 27785,  1774,   607,\n",
       "             13,     8, 11820,     1]),\n",
       "  'input_text': 'the language model which in the case\\nof texttosql translation is the readable surface\\nform of the sq',\n",
       "  'expected_output': 'l code\\nin our experiments we ﬁnd that picard can\\ns'},\n",
       " {'input_ids': tensor([ 7314,    19,     8,     3, 27785,  1774,   607,    13,     8, 11820,\n",
       "             40,  1081,    16,    69, 12341,    62,   253,    24,  6686,   986,\n",
       "             54,     3,     7,     1]),\n",
       "  'input_text': 'translation is the readable surface\\nform of the sql code\\nin our experiments we ﬁnd that picard can\\ns',\n",
       "  'expected_output': 'igniﬁcantly improve the performance of a large\\npre'},\n",
       " {'input_ids': tensor([    3,    40,  1081,    16,    69, 12341,    62,   253,    24,  6686,\n",
       "            986,    54,  4019,  1172,     8,   821,    13,     3,     9,   508,\n",
       "            554,     1]),\n",
       "  'input_text': 'l code\\nin our experiments we ﬁnd that picard can\\nsigniﬁcantly improve the performance of a large\\npre',\n",
       "  'expected_output': 'trained language model raffel et al 2020\\nafter it '},\n",
       " {'input_ids': tensor([    3,  3191,  3286,   288,   120,  1172,     8,   821,    13,     3,\n",
       "              9,   508,  7140, 10761,  1612,   825,     3,    52,     9, 16387,\n",
       "              3,    15,    17,   491,  6503,   227,    34,     1]),\n",
       "  'input_text': 'igniﬁcantly improve the performance of a large\\npretrained language model raffel et al 2020\\nafter it ',\n",
       "  'expected_output': 'is ﬁnetuned on the texttosql task on\\nthe spider te'},\n",
       " {'input_ids': tensor([ 4252,  1612,   825,     3,    52,     9, 16387,     3,    15,    17,\n",
       "            491,  6503,   227,    34,    19,  1399,    17,   444,    26,    30,\n",
       "              8,  1499,   235,     7,  1824,    40,  2491,    30,     8, 18612,\n",
       "              3,    17,    15,     1]),\n",
       "  'input_text': 'trained language model raffel et al 2020\\nafter it is ﬁnetuned on the texttosql task on\\nthe spider te',\n",
       "  'expected_output': 'xttosql dataset yu et al 2018 we\\nﬁnd that a t5base'},\n",
       " {'input_ids': tensor([   19,  1399,    17,   444,    26,    30,     8,  1499,   235,     7,\n",
       "           1824,    40,  2491,    30,     8, 18612,  1499,   235,     7,  1824,\n",
       "             40, 17953,     3,    63,    76,     3,    15,    17,   491,   846,\n",
       "             62,   253,    24,     3,     9,     3,    17,   755, 10925,     1]),\n",
       "  'input_text': 'is ﬁnetuned on the texttosql task on\\nthe spider texttosql dataset yu et al 2018 we\\nﬁnd that a t5base',\n",
       "  'expected_output': ' model with picard can out\\nperform a t5large model'},\n",
       " {'input_ids': tensor([    3,   226,    17,   235,     7,  1824,    40, 17953,     3,    63,\n",
       "             76,     3,    15,    17,   491,   846,    62,   253,    24,     3,\n",
       "              9,     3,    17,   755, 10925,   825,    28,  6686,   986,    54,\n",
       "             91,  1912,     3,     9,     3,    17,   755, 15599,   825,     1]),\n",
       "  'input_text': 'xttosql dataset yu et al 2018 we\\nﬁnd that a t5base model with picard can out\\nperform a t5large model',\n",
       "  'expected_output': ' without it and likewise\\nfor a t5large and a t53b '},\n",
       " {'input_ids': tensor([  825,    28,  6686,   986,    54,    91,  1912,     3,     9,     3,\n",
       "             17,   755, 15599,   825,   406,    34,    11,     3, 15340,    21,\n",
       "              3,     9,     3,    17,   755, 15599,    11,     3,     9,     3,\n",
       "             17,  4867,   115,     1]),\n",
       "  'input_text': ' model with picard can out\\nperform a t5large model without it and likewise\\nfor a t5large and a t53b ',\n",
       "  'expected_output': 'model signiﬁcantly\\nwith the help of picard  a t53b'},\n",
       " {'input_ids': tensor([  406,    34,    11,     3, 15340,    21,     3,     9,     3,    17,\n",
       "            755, 15599,    11,     3,     9,     3,    17,  4867,   115,   825,\n",
       "           4019,    28,     8,   199,    13,  6686,   986,     3,     9,     3,\n",
       "             17,  4867,   115,     1]),\n",
       "  'input_text': ' without it and likewise\\nfor a t5large and a t53b model signiﬁcantly\\nwith the help of picard  a t53b',\n",
       "  'expected_output': ' model can be\\nraised to stateoftheart performance '},\n",
       " {'input_ids': tensor([ 825, 4019,   28,    8,  199,   13, 6686,  986,    3,    9,    3,   17,\n",
       "          4867,  115,  825,   54,   36, 3279,   12,  538,  858,  532, 1408,  821,\n",
       "             1]),\n",
       "  'input_text': 'model signiﬁcantly\\nwith the help of picard  a t53b model can be\\nraised to stateoftheart performance ',\n",
       "  'expected_output': 'on the spider\\nand cosql datasets yu et al 2019\\n2 t'},\n",
       " {'input_ids': tensor([  825,    54,    36,  3279,    12,   538,   858,   532,  1408,   821,\n",
       "             30,     8, 18612,    11,   576,     7,  1824,    40, 17953,     7,\n",
       "              3,    63,    76,     3,    15,    17,   491,  1360,   204,     3,\n",
       "             17,     1]),\n",
       "  'input_text': ' model can be\\nraised to stateoftheart performance on the spider\\nand cosql datasets yu et al 2019\\n2 t',\n",
       "  'expected_output': 'he p icard method\\npicard warps model prediction sc'},\n",
       " {'input_ids': tensor([   30,     8, 18612,    11,   576,     7,  1824,    40, 17953,     7,\n",
       "              3,    63,    76,     3,    15,    17,   491,  1360,   204,     8,\n",
       "              3,   102,     3,    23,  6043,  1573,  6686,   986,   615,   102,\n",
       "              7,   825, 21332,     3,     7,    75,     1]),\n",
       "  'input_text': 'on the spider\\nand cosql datasets yu et al 2019\\n2 the p icard method\\npicard warps model prediction sc',\n",
       "  'expected_output': 'ores and inte\\ngrates trivially with existing algor'},\n",
       " {'input_ids': tensor([    3,    88,     3,   102,     3,    23,  6043,  1573,  6686,   986,\n",
       "            615,   102,     7,   825, 21332,  7586,    11,     3,  2429,  3542,\n",
       "           6203, 22377,  6073,    28,  1895,   491,   122,   127,     1]),\n",
       "  'input_text': 'he p icard method\\npicard warps model prediction scores and inte\\ngrates trivially with existing algor',\n",
       "  'expected_output': 'ithms for greedy\\nand beam search used in autoregre'},\n",
       " {'input_ids': tensor([    3, 14846,    11,     3,  2429,  3542,  6203, 22377,  6073,    28,\n",
       "           1895, 16783,    21, 30337,    63,    11, 11638,   960,   261,    16,\n",
       "           8728,    15,  3584,     1]),\n",
       "  'input_text': 'ores and inte\\ngrates trivially with existing algorithms for greedy\\nand beam search used in autoregre',\n",
       "  'expected_output': 'ssive decoding\\nfrom language models its arguments '},\n",
       " {'input_ids': tensor([   34,   107,    51,     7,    21, 30337,    63,    11, 11638,   960,\n",
       "            261,    16,  8728,    15, 10292,   757,    20,  9886,    45,  1612,\n",
       "           2250,   165, 12874,     1]),\n",
       "  'input_text': 'ithms for greedy\\nand beam search used in autoregressive decoding\\nfrom language models its arguments ',\n",
       "  'expected_output': 'are the token\\nids of the current hypothesis and fo'},\n",
       " {'input_ids': tensor([    3,     7,     7,   757,    20,  9886,    45,  1612,  2250,   165,\n",
       "          12874,    33,     8, 14145,     3,    23,    26,     7,    13,     8,\n",
       "            750, 22455,    11,  5575,     1]),\n",
       "  'input_text': 'ssive decoding\\nfrom language models its arguments are the token\\nids of the current hypothesis and fo',\n",
       "  'expected_output': 'r each vocabu\\nlary token the logsoftmax scores pre'},\n",
       " {'input_ids': tensor([   33,     8, 14145,     3,    23,    26,     7,    13,     8,   750,\n",
       "          22455,    11,    21,   284,     3,  6117,     9,  3007,    50,   651,\n",
       "          14145,     8,  4303, 12369,  9128,  7586,   554,     1]),\n",
       "  'input_text': 'are the token\\nids of the current hypothesis and for each vocabu\\nlary token the logsoftmax scores pre',\n",
       "  'expected_output': 'dicted by the\\nmodels language modeling head picard'},\n",
       " {'input_ids': tensor([    3,    52,   284,     3,  6117,     9,  3007,    50,   651, 14145,\n",
       "              8,  4303, 12369,  9128,  7586, 15439,    57,     8,  2250,  1612,\n",
       "          15309,   819,  6686,   986,     1]),\n",
       "  'input_text': 'r each vocabu\\nlary token the logsoftmax scores predicted by the\\nmodels language modeling head picard',\n",
       "  'expected_output': ' also hasaccess to sql schema information in parti'},\n",
       " {'input_ids': tensor([    3,  4370,  1054,    57,     8,  2250,  1612, 15309,   819,  6686,\n",
       "            986,    92,    65, 20393,    12, 11820,    40, 26622,   251,    16,\n",
       "           5909,     1]),\n",
       "  'input_text': 'dicted by the\\nmodels language modeling head picard also hasaccess to sql schema information in parti',\n",
       "  'expected_output': 'cular\\ninformation about the names of tables and co'},\n",
       " {'input_ids': tensor([   92,    65, 20393,    12, 11820,    40, 26622,   251,    16,  1090,\n",
       "            251,    81,     8,  3056,    13,  5056,    11,   576,     1]),\n",
       "  'input_text': ' also hasaccess to sql schema information in particular\\ninformation about the names of tables and co',\n",
       "  'expected_output': 'lumns\\nand about which column resides in which tabl'},\n",
       " {'input_ids': tensor([    3,  4866,   251,    81,     8,  3056,    13,  5056,    11, 15752,\n",
       "             11,    81,    84,  6710,     3, 28799,    16,    84,  3808,    40,\n",
       "              1]),\n",
       "  'input_text': 'cular\\ninformation about the names of tables and columns\\nand about which column resides in which tabl',\n",
       "  'expected_output': 'e\\nat each generation step picard ﬁrst restricts\\npr'},\n",
       " {'input_ids': tensor([    3,  5171,    29,     7,    11,    81,    84,  6710,     3, 28799,\n",
       "             16,    84,   953,    44,   284,  3381,  1147,  6686,   986,   166,\n",
       "          18395,     7,  4880,     1]),\n",
       "  'input_text': 'lumns\\nand about which column resides in which table\\nat each generation step picard ﬁrst restricts\\npr',\n",
       "  'expected_output': 'ediction to the top khighest probability tokens\\nan'},\n",
       " {'input_ids': tensor([    3,    15,    44,   284,  3381,  1147,  6686,   986,   166, 18395,\n",
       "              7, 21332,    12,     8,   420,     3,   157,  6739,   222, 15834,\n",
       "          14145,     7,    46,     1]),\n",
       "  'input_text': 'e\\nat each generation step picard ﬁrst restricts\\nprediction to the top khighest probability tokens\\nan',\n",
       "  'expected_output': 'd then assigns a score of 1 to those that fail\\npic'},\n",
       " {'input_ids': tensor([    3,    15, 12472,    12,     8,   420,     3,   157,  6739,   222,\n",
       "          15834, 14145,     7,    11,   258, 12317,     7,     3,     9,  2604,\n",
       "             13,   209,    12,   273,    24,  5124,  6686,     1]),\n",
       "  'input_text': 'ediction to the top khighest probability tokens\\nand then assigns a score of 1 to those that fail\\npic',\n",
       "  'expected_output': 'ard s numerous checks see figure 2 these\\nchecks ar'},\n",
       " {'input_ids': tensor([    3,    26,   258, 12317,     7,     3,     9,  2604,    13,   209,\n",
       "             12,   273,    24,  5124,  6686,   986,     3,     7,  2724, 11642,\n",
       "            217,  2320,   204,   175, 11642,  1584,     1]),\n",
       "  'input_text': 'd then assigns a score of 1 to those that fail\\npicard s numerous checks see figure 2 these\\nchecks ar',\n",
       "  'expected_output': 'e enabled by fast incremental parsing\\nosullivan an'},\n",
       " {'input_ids': tensor([    3,   986,     3,     7,  2724, 11642,   217,  2320,   204,   175,\n",
       "          11642,    33,  9367,    57,  1006, 28351,   260,     7,    53,     3,\n",
       "             32,     7,    83, 20580,   152,    46,     1]),\n",
       "  'input_text': 'ard s numerous checks see figure 2 these\\nchecks are enabled by fast incremental parsing\\nosullivan an',\n",
       "  'expected_output': 'd gamari 2021 based on monadic\\ncombinators leijen '},\n",
       " {'input_ids': tensor([    3,    15,  9367,    57,  1006, 28351,   260,     7,    53,     3,\n",
       "             32,     7,    83, 20580,   152,    11,     3,  8758,  1665,   460,\n",
       "           2658,     3,   390,    30,  1911,     9,  4370, 10374,  6230,  4628,\n",
       "            354,    35,     1]),\n",
       "  'input_text': 'e enabled by fast incremental parsing\\nosullivan and gamari 2021 based on monadic\\ncombinators leijen ',\n",
       "  'expected_output': 'and meijer 2001 there are\\nfour picard mode setting'},\n",
       " {'input_ids': tensor([    3,    26,     3,  8758,  1665,   460,  2658,     3,   390,    30,\n",
       "           1911,     9,  4370, 10374,  6230,  4628,   354,    35,    11,   140,\n",
       "             23, 12488,  4402,   132,    33,   662,  6686,   986,  2175,  1898,\n",
       "              1]),\n",
       "  'input_text': 'd gamari 2021 based on monadic\\ncombinators leijen and meijer 2001 there are\\nfour picard mode setting',\n",
       "  'expected_output': 's that control their com\\nprehensiveness off no che'},\n",
       " {'input_ids': tensor([   11,   140,    23, 12488,  4402,   132,    33,   662,  6686,   986,\n",
       "           2175,  3803,    24,   610,    70,     3,   287,     3, 22459,   162,\n",
       "            655,   326,   150,     3,  1033,     1]),\n",
       "  'input_text': 'and meijer 2001 there are\\nfour picard mode settings that control their com\\nprehensiveness off no che',\n",
       "  'expected_output': 'cking lexing parsing\\nwithout guards and parsing wi'},\n",
       " {'input_ids': tensor([    3,     7,    24,   610,    70,     3,   287,     3, 22459,   162,\n",
       "            655,   326,   150,  6450,    90,   226,    53,   260,     7,    53,\n",
       "            406,  4879,     7,    11,   260,     7,    53, 11064,     1]),\n",
       "  'input_text': 's that control their com\\nprehensiveness off no checking lexing parsing\\nwithout guards and parsing wi',\n",
       "  'expected_output': 'th guardsthe high\\nest mode a prediction that passe'},\n",
       " {'input_ids': tensor([    3,  2406,    53,    90,   226,    53,   260,     7,    53,   406,\n",
       "           4879,     7,    11,   260,     7,    53,    28,  4879,     7,   532,\n",
       "            306,   259,  2175,     3,     9, 21332,    24,  8063,     1]),\n",
       "  'input_text': 'cking lexing parsing\\nwithout guards and parsing with guardsthe high\\nest mode a prediction that passe',\n",
       "  'expected_output': 's a higher mode\\nwill always pass a lower mode but '},\n",
       " {'input_ids': tensor([    3,   189,  4879,     7,   532,   306,   259,  2175,     3,     9,\n",
       "          21332,    24,  9016,     3,     9,  1146,  2175,    56,   373,  1903,\n",
       "              3,     9,  1364,  2175,    68,     1]),\n",
       "  'input_text': 'th guardsthe high\\nest mode a prediction that passes a higher mode\\nwill always pass a lower mode but ',\n",
       "  'expected_output': 'not necessarily\\nvice versa\\n21 lexing\\nin lexing mod'},\n",
       " {'input_ids': tensor([   3,    7,    3,    9, 1146, 2175,   56,  373, 1903,    3,    9, 1364,\n",
       "          2175,   68,   59, 6539, 6444, 2676,    9, 1401,   90,  226,   53,   16,\n",
       "            90,  226,   53, 1794,    1]),\n",
       "  'input_text': 's a higher mode\\nwill always pass a lower mode but not necessarily\\nvice versa\\n21 lexing\\nin lexing mod',\n",
       "  'expected_output': 'e picard checks the output on\\na lexical level only'},\n",
       " {'input_ids': tensor([   59,  6539,  6444,  2676,     9,  1401,    90,   226,    53,    16,\n",
       "             90,   226,    53,  2175,  6686,   986, 11642,     8,  3911,    30,\n",
       "              3,     9,     3, 30949,   138,   593,   163,     1]),\n",
       "  'input_text': 'not necessarily\\nvice versa\\n21 lexing\\nin lexing mode picard checks the output on\\na lexical level only',\n",
       "  'expected_output': ' it attempts to convert the\\npartial detokenized mo'},\n",
       " {'input_ids': tensor([    3,    15,  6686,   986, 11642,     8,  3911,    30,     3,     9,\n",
       "              3, 30949,   138,   593,   163,    34,  9048,    12,  5755,     8,\n",
       "          11807,    20,   235,  2217,  1601,  2288,     1]),\n",
       "  'input_text': 'e picard checks the output on\\na lexical level only it attempts to convert the\\npartial detokenized mo',\n",
       "  'expected_output': 'del output to a whitespace\\ndelimited sequence of i'},\n",
       " {'input_ids': tensor([   34,  9048,    12,  5755,     8, 11807,    20,   235,  2217,  1601,\n",
       "            825,  3911,    12,     3,     9,   872,  6633,    20, 29901,  5932,\n",
       "             13,     3,    23,     1]),\n",
       "  'input_text': ' it attempts to convert the\\npartial detokenized model output to a whitespace\\ndelimited sequence of i',\n",
       "  'expected_output': 'ndividual sql keywords\\nlikeselect  punctuation lik'},\n",
       " {'input_ids': tensor([   20,    40,  3911,    12,     3,     9,   872,  6633,    20, 29901,\n",
       "           5932,    13,   928, 11820,    40, 12545,   114,     7,    15,  3437,\n",
       "           5427,    76,   257,     3,  8654,     1]),\n",
       "  'input_text': 'del output to a whitespace\\ndelimited sequence of individual sql keywords\\nlikeselect  punctuation lik',\n",
       "  'expected_output': 'e  operators like\\nand literals like string and num'},\n",
       " {'input_ids': tensor([    3,   727,    23,  6961,  3471, 11820,    40, 12545,   114,     7,\n",
       "             15,  3437,  5427,    76,   257,   114,  9490,   114,    11, 26998,\n",
       "              7,   114,  6108,    11,     3,  5525,     1]),\n",
       "  'input_text': 'ndividual sql keywords\\nlikeselect  punctuation like  operators like\\nand literals like string and num',\n",
       "  'expected_output': 'ber values in\\nsql conditions and identiﬁers like a'},\n",
       " {'input_ids': tensor([    3,    15,  9490,   114,    11, 26998,     7,   114,  6108,    11,\n",
       "            381,  2620,    16, 11820,    40,  1124,    11,     3,  8826,    52,\n",
       "              7,   114,     3,     9,     1]),\n",
       "  'input_text': 'e  operators like\\nand literals like string and number values in\\nsql conditions and identiﬁers like a',\n",
       "  'expected_output': 'liases tables\\nand columnswithout being sensitive t'},\n",
       " {'input_ids': tensor([    3,  1152,  2620,    16, 11820,    40,  1124,    11,     3,  8826,\n",
       "             52,     7,   114,     3,  5434,  2260,  5056,    11, 15752, 23016,\n",
       "            271,  6280,     3,    17,     1]),\n",
       "  'input_text': 'ber values in\\nsql conditions and identiﬁers like aliases tables\\nand columnswithout being sensitive t',\n",
       "  'expected_output': 'o the order\\nin which these lexical items appear by'},\n",
       " {'input_ids': tensor([    3,    40,    23,     9,  2260,  5056,    11, 15752, 23016,   271,\n",
       "           6280,    12,     8,   455,    16,    84,   175,     3, 30949,   138,\n",
       "           1173,  2385,    57,     1]),\n",
       "  'input_text': 'liases tables\\nand columnswithout being sensitive to the order\\nin which these lexical items appear by',\n",
       "  'expected_output': ' making it\\nsopicard can detect spelling errors in '},\n",
       " {'input_ids': tensor([    3,    32,     8,   455,    16,    84,   175,     3, 30949,   138,\n",
       "           1173,  2385,    57,   492,    34,    78,  6174,   986,    54,  8432,\n",
       "          19590,  6854,    16,     1]),\n",
       "  'input_text': 'o the order\\nin which these lexical items appear by making it\\nsopicard can detect spelling errors in ',\n",
       "  'expected_output': 'keywords\\nor reject table and column names that are'},\n",
       " {'input_ids': tensor([  492,    34,    78,  6174,   986,    54,  8432, 19590,  6854,    16,\n",
       "          12545,    42, 15092,   953,    11,  6710,  3056,    24,    33,     1]),\n",
       "  'input_text': ' making it\\nsopicard can detect spelling errors in keywords\\nor reject table and column names that are',\n",
       "  'expected_output': ' invalid\\nfor the given sql schema for instance con'},\n",
       " {'input_ids': tensor([12545,    42, 15092,   953,    11,  6710,  3056,    24,    33, 17070,\n",
       "             21,     8,   787, 11820,    40, 26622,    21,  3421,   975,     1]),\n",
       "  'input_text': 'keywords\\nor reject table and column names that are invalid\\nfor the given sql schema for instance con',\n",
       "  'expected_output': 'sider\\nthe question what are the email cell phone\\na'},\n",
       " {'input_ids': tensor([17070,    21,     8,   787, 11820,    40, 26622,    21,  3421,  1099,\n",
       "              8,   822,   125,    33,     8,   791,  2358,   951,     3,     9,\n",
       "              1]),\n",
       "  'input_text': ' invalid\\nfor the given sql schema for instance consider\\nthe question what are the email cell phone\\na',\n",
       "  'expected_output': 'nd home phone of each professional from\\nspiders de'},\n",
       " {'input_ids': tensor([  596,    52,     8,   822,   125,    33,     8,   791,  2358,   951,\n",
       "             11,   234,   951,    13,   284,   771,    45, 18612,     7,    20,\n",
       "              1]),\n",
       "  'input_text': 'sider\\nthe question what are the email cell phone\\nand home phone of each professional from\\nspiders de',\n",
       "  'expected_output': 'velopment set on the dogkennels\\ndatabase our ﬁnetu'},\n",
       " {'input_ids': tensor([    3,   727,   234,   951,    13,   284,   771,    45, 18612,     7,\n",
       "            606,   356,    30,     8,  1782,  9376,  3573,  3501,    69,  1399,\n",
       "             17,    76,     1]),\n",
       "  'input_text': 'nd home phone of each professional from\\nspiders development set on the dogkennels\\ndatabase our ﬁnetu',\n",
       "  'expected_output': 'ned t5large model predicts\\nselect emailaddress cel'},\n",
       " {'input_ids': tensor([    3,   162,  8745,   297,   356,    30,     8,  1782,  9376,  3573,\n",
       "           3501,    69,  1399,    17,   444,    26,     3,    17,   755, 15599,\n",
       "            825,  9689,     7,  1738,   791,     9,    26, 12039,  1503,     1]),\n",
       "  'input_text': 'velopment set on the dogkennels\\ndatabase our ﬁnetuned t5large model predicts\\nselect emailaddress cel',\n",
       "  'expected_output': 'lphone\\nhomephone from professionals while\\nthe grou'},\n",
       " {'input_ids': tensor([    3,    29,    15,    26,     3,    17,   755, 15599,   825,  9689,\n",
       "              7,  1738,   791,     9,    26, 12039, 29417,   234,  6399,    45,\n",
       "           2481,   298,     8,     3,  3844,    76,     1]),\n",
       "  'input_text': 'ned t5large model predicts\\nselect emailaddress cellphone\\nhomephone from professionals while\\nthe grou',\n",
       "  'expected_output': 'nd truth selects cellnumber instead of\\nthe invalid'},\n",
       " {'input_ids': tensor([    3,    40,  6399,   234,  6399,    45,  2481,   298,     8,  1591,\n",
       "           2827,  1738,     7,  2358,  5525,  1152,  1446,    13,     8, 17070,\n",
       "              1]),\n",
       "  'input_text': 'lphone\\nhomephone from professionals while\\nthe ground truth selects cellnumber instead of\\nthe invalid',\n",
       "  'expected_output': ' cellphone column this mistake is\\ncaught and avoid'},\n",
       " {'input_ids': tensor([    3,   727,  2827,  1738,     7,  2358,  5525,  1152,  1446,    13,\n",
       "              8, 17070, 29417,  6710,    48,  6202,    19,  4682,    11,  1792,\n",
       "              1]),\n",
       "  'input_text': 'nd truth selects cellnumber instead of\\nthe invalid cellphone column this mistake is\\ncaught and avoid',\n",
       "  'expected_output': 'ed by p icard in lexing mode\\n22 parsing without gu'},\n",
       " {'input_ids': tensor([29417,  6710,    48,  6202,    19,  4682,    11, 16652,    57,     3,\n",
       "            102,     3,    23,  6043,    16,    90,   226,    53,  2175,  1630,\n",
       "            260,     7,    53,   406,     3,  1744,     1]),\n",
       "  'input_text': ' cellphone column this mistake is\\ncaught and avoided by p icard in lexing mode\\n22 parsing without gu',\n",
       "  'expected_output': 'ards\\nin the lowest parsing mode above lexingreferr'},\n",
       " {'input_ids': tensor([   3,   15,   26,   57,    3,  102,    3,   23, 6043,   16,   90,  226,\n",
       "            53, 2175, 1630,  260,    7,   53,  406, 4879,    7,   16,    8, 7402,\n",
       "           260,    7,   53, 2175,  756,   90,  226,   53,   60, 1010,   52,    1]),\n",
       "  'input_text': 'ed by p icard in lexing mode\\n22 parsing without guards\\nin the lowest parsing mode above lexingreferr',\n",
       "  'expected_output': 'ed\\nto as parsing without guards picard checks the\\n'},\n",
       " {'input_ids': tensor([    3,   986,     7,    16,     8,  7402,   260,     7,    53,  2175,\n",
       "            756,    90,   226,    53,  4822,    12,    38,   260,     7,    53,\n",
       "            406,  4879,     7,  6686,   986, 11642,     8,     1]),\n",
       "  'input_text': 'ards\\nin the lowest parsing mode above lexingreferred\\nto as parsing without guards picard checks the\\n',\n",
       "  'expected_output': 'output on a grammatical level picard attempts to\\np'},\n",
       " {'input_ids': tensor([    3,    15,    26,    12,    38,   260,     7,    53,   406,  4879,\n",
       "              7,  6686,   986, 11642,     8,  3911,    30,     3,     9,     3,\n",
       "           5096,  4992,   138,   593,  6686,   986,  9048,    12,     3,   102,\n",
       "              1]),\n",
       "  'input_text': 'ed\\nto as parsing without guards picard checks the\\noutput on a grammatical level picard attempts to\\np',\n",
       "  'expected_output': 'arse the detokenized model output to a data struc\\n'},\n",
       " {'input_ids': tensor([3911,   30,    3,    9,    3, 5096, 4992,  138,  593, 6686,  986, 9048,\n",
       "            12,  260,    7,   15,    8,   20,  235, 2217, 1601,  825, 3911,   12,\n",
       "             3,    9,  331,    3, 6159,   75,    1]),\n",
       "  'input_text': 'output on a grammatical level picard attempts to\\nparse the detokenized model output to a data struc\\n',\n",
       "  'expected_output': 'ture that represents the abstract syntax tree ast\\n'},\n",
       " {'input_ids': tensor([ 1584,     7,    15,     8,    20,   235,  2217,  1601,   825,  3911,\n",
       "             12,     3,     9,   331,     3,  6159,    75,     3,  2693,    24,\n",
       "           5475,     8,  9838, 28230,  2195,    38,    17,     1]),\n",
       "  'input_text': 'arse the detokenized model output to a data struc\\nture that represents the abstract syntax tree ast\\n',\n",
       "  'expected_output': 'of the predicted sql query contrary to lexing\\nmode'},\n",
       " {'input_ids': tensor([    3,  2693,    24,  5475,     8,  9838, 28230,  2195,    38,    17,\n",
       "             13,     8, 15439, 11820,    40, 11417, 15943,    12,    90,   226,\n",
       "             53,  2175,     1]),\n",
       "  'input_text': 'ture that represents the abstract syntax tree ast\\nof the predicted sql query contrary to lexing\\nmode',\n",
       "  'expected_output': ' the order in which keywords and clauses ap\\npear n'},\n",
       " {'input_ids': tensor([   13,     8, 15439, 11820,    40, 11417, 15943,    12,    90,   226,\n",
       "             53,  2175,     8,   455,    16,    84, 12545,    11, 14442,     7,\n",
       "              3,     9,   102,   158,   291,     3,    29,     1]),\n",
       "  'input_text': 'of the predicted sql query contrary to lexing\\nmode the order in which keywords and clauses ap\\npear n',\n",
       "  'expected_output': 'ow matters picard can reject invalid query\\nstructu'},\n",
       " {'input_ids': tensor([    8,   455,    16,    84, 12545,    11, 14442,     7,     3,     9,\n",
       "            102,   158,   291,   230,  4573,  6686,   986,    54, 15092, 17070,\n",
       "          11417,     3,  7593,    76,     1]),\n",
       "  'input_text': ' the order in which keywords and clauses ap\\npear now matters picard can reject invalid query\\nstructu',\n",
       "  'expected_output': 'res eg ﬁnd missing from clauses or incor\\nrect orde'},\n",
       " {'input_ids': tensor([    3,  2381,  4573,  6686,   986,    54, 15092, 17070, 11417,  5278,\n",
       "              3,    15,   122,   253,  3586,    45, 14442,     7,    42,    16,\n",
       "           5715,     3, 12621,    42,   221,     1]),\n",
       "  'input_text': 'ow matters picard can reject invalid query\\nstructures eg ﬁnd missing from clauses or incor\\nrect orde',\n",
       "  'expected_output': 'rs of clauses and keywords it can also\\ndetect a ra'},\n",
       " {'input_ids': tensor([    3,    60,     7,     3,    15,   122,   253,  3586,    45, 14442,\n",
       "              7,    42,    16,  5715,     3, 12621,  5022,    13, 14442,     7,\n",
       "             11, 12545,    34,    54,    92,  8432,     3,     9,     3,    52,\n",
       "              9,     1]),\n",
       "  'input_text': 'res eg ﬁnd missing from clauses or incor\\nrect orders of clauses and keywords it can also\\ndetect a ra',\n",
       "  'expected_output': 'nge of issues with compositions of sql9897expressi'},\n",
       " {'input_ids': tensor([    3,    52,     7,    13, 14442,     7,    11, 12545,    34,    54,\n",
       "             92,  8432,     3,     9,   620,    13,   807,    28,  5761,     7,\n",
       "             13, 11820,    40,  3916,  4327,   994,  4715,    23,     1]),\n",
       "  'input_text': 'rs of clauses and keywords it can also\\ndetect a range of issues with compositions of sql9897expressi',\n",
       "  'expected_output': 'ons number one if picard matches on\\natidcid patter'},\n",
       " {'input_ids': tensor([    3,    29,   397,    13,   807,    28,  5761,     7,    13, 11820,\n",
       "             40,  3916,  4327, 20940,     7,   381,    80,     3,    99,  6686,\n",
       "            986,  6407,    30,    44,    23,    26, 10812,  6234,   449,     1]),\n",
       "  'input_text': 'nge of issues with compositions of sql9897expressions number one if picard matches on\\natidcid patter',\n",
       "  'expected_output': 'n but the table with the id tid\\ndoes not contain a'},\n",
       " {'input_ids': tensor([   30,     7,   381,    80,     3,    99,  6686,   986,  6407,    30,\n",
       "             44,    23,    26, 10812,  3275,    68,     8,   953,    28,     8,\n",
       "              3,    23,    26,     3,    17,    23,    26,   405,    59,  3480,\n",
       "              3,     9,     1]),\n",
       "  'input_text': 'ons number one if picard matches on\\natidcid pattern but the table with the id tid\\ndoes not contain a',\n",
       "  'expected_output': ' column with id cid then that\\nparse is rejected se'},\n",
       " {'input_ids': tensor([    3,    29,    68,     8,   953,    28,     8,     3,    23,    26,\n",
       "              3,    17,    23,    26,   405,    59,  3480,     3,     9,  6710,\n",
       "             28,     3,    23,    26,     3, 10812,   258,    24,   260,     7,\n",
       "             15,    19, 12967,   142,     1]),\n",
       "  'input_text': 'n but the table with the id tid\\ndoes not contain a column with id cid then that\\nparse is rejected se',\n",
       "  'expected_output': 'condly if picard ﬁrst matches\\non an aliascid patte'},\n",
       " {'input_ids': tensor([ 6710,    28,     3,    23,    26,     3, 10812,   258,    24,   260,\n",
       "              7,    15,    19, 12967,   511,   120,     3,    99,  6686,   986,\n",
       "            166,  6407,    30,    46,     3,  5434,     7, 10812,     3,   102,\n",
       "          10206,     1]),\n",
       "  'input_text': ' column with id cid then that\\nparse is rejected secondly if picard ﬁrst matches\\non an aliascid patte',\n",
       "  'expected_output': 'rn and then later matches\\non the tid as alias patt'},\n",
       " {'input_ids': tensor([  975,    26,   120,     3,    99,  6686,   986,   166,  6407,    30,\n",
       "             46,     3,  5434,     7, 10812,  3275,    11,   258,   865,  6407,\n",
       "             30,     8,     3,    17,    23,    26,    38,     3,  5434,     7,\n",
       "           6234,    17,     1]),\n",
       "  'input_text': 'condly if picard ﬁrst matches\\non an aliascid pattern and then later matches\\non the tid as alias patt',\n",
       "  'expected_output': 'ern but tid does not\\ncontain cid then that parse i'},\n",
       " {'input_ids': tensor([    3,    52,    29,    11,   258,   865,  6407,    30,     8,     3,\n",
       "             17,    23,    26,    38,     3,  5434,     7,  3275,    68,     3,\n",
       "             17,    23,    26,   405,    59,  3480,     3, 10812,   258,    24,\n",
       "            260,     7,    15,     3,    23,     1]),\n",
       "  'input_text': 'rn and then later matches\\non the tid as alias pattern but tid does not\\ncontain cid then that parse i',\n",
       "  'expected_output': 's also rejected an\\nequivalent rule also exists for'},\n",
       " {'input_ids': tensor([    3,    49,    29,    68,     3,    17,    23,    26,   405,    59,\n",
       "           3480,     3, 10812,   258,    24,   260,     7,    15,    19,    92,\n",
       "          12967,    46,  7072,  3356,    92,  8085,    21,     1]),\n",
       "  'input_text': 'ern but tid does not\\ncontain cid then that parse is also rejected an\\nequivalent rule also exists for',\n",
       "  'expected_output': ' subqueries bound to\\ntable aliases lastly picard p'},\n",
       " {'input_ids': tensor([    3,     7,    92, 12967,    46,  7072,  3356,    92,  8085,    21,\n",
       "            769,   835,  2593,  8120,    12,   953,     3,  5434,  2260,   336,\n",
       "            120,  6686,   986,     3,   102,     1]),\n",
       "  'input_text': 's also rejected an\\nequivalent rule also exists for subqueries bound to\\ntable aliases lastly picard p',\n",
       "  'expected_output': 'rohibits duplicate\\nbinding of a table alias in the'},\n",
       " {'input_ids': tensor([  769,   835,  2593,  8120,    12,   953,     3,  5434,  2260,   336,\n",
       "            120,  6686,   986, 19551,     7, 19197, 11293,    13,     3,     9,\n",
       "            953,     3,  5434,     7,    16,     8,     1]),\n",
       "  'input_text': ' subqueries bound to\\ntable aliases lastly picard prohibits duplicate\\nbinding of a table alias in the',\n",
       "  'expected_output': ' same select scope\\nbut permits shadowing of aliase'},\n",
       " {'input_ids': tensor([    3,    52,    32, 13506,    17,     7, 19197, 11293,    13,     3,\n",
       "              9,   953,     3,  5434,     7,    16,     8,   337,  1738,  7401,\n",
       "             68, 14079,  8552,    53,    13,     3,  5434,     7,    15,     1]),\n",
       "  'input_text': 'rohibits duplicate\\nbinding of a table alias in the same select scope\\nbut permits shadowing of aliase',\n",
       "  'expected_output': 's deﬁned in a sur\\nrounding scope this can happen i'},\n",
       " {'input_ids': tensor([  337,  1738,  7401,    68, 14079,  8552,    53,    13,     3,  5434,\n",
       "           2260,  4802,    16,     3,     9,   244,  1751,    53,  7401,    48,\n",
       "             54,  1837,     3,    23,     1]),\n",
       "  'input_text': ' same select scope\\nbut permits shadowing of aliases deﬁned in a sur\\nrounding scope this can happen i',\n",
       "  'expected_output': 'n nested sql\\nqueries\\n23 parsing with guards\\nin its'},\n",
       " {'input_ids': tensor([    3,     7,  4802,    16,     3,     9,   244,  1751,    53,  7401,\n",
       "             48,    54,  1837,    16,     3,  1496,  1054, 11820,    40, 13154,\n",
       "           1902,   260,     7,    53,    28,  4879,     7,    16,   165,     1]),\n",
       "  'input_text': 's deﬁned in a sur\\nrounding scope this can happen in nested sql\\nqueries\\n23 parsing with guards\\nin its',\n",
       "  'expected_output': ' highest parsing mode picard engages\\nin additional'},\n",
       " {'input_ids': tensor([    3,    29,     3,  1496,  1054, 11820,    40, 13154,  1902,   260,\n",
       "              7,    53,    28,  4879,     7,    16,   165,  2030,   260,     7,\n",
       "             53,  2175,  6686,   986,  4082,     7,    16,  1151,     1]),\n",
       "  'input_text': 'n nested sql\\nqueries\\n23 parsing with guards\\nin its highest parsing mode picard engages\\nin additional',\n",
       "  'expected_output': ' analysescalled guardswhile as\\nsembling the sql as'},\n",
       " {'input_ids': tensor([ 2030,   260,     7,    53,  2175,  6686,   986,  4082,     7,    16,\n",
       "           1151, 15282,  9341,  4879,     7, 12124,   109,    38,     3,     7,\n",
       "           8312,    53,     8, 11820,    40,    38,     1]),\n",
       "  'input_text': ' highest parsing mode picard engages\\nin additional analysescalled guardswhile as\\nsembling the sql as',\n",
       "  'expected_output': 't if picard matches on\\ntidcid oraliascid  then gua'},\n",
       " {'input_ids': tensor([15282,  9341,  4879,     7, 12124,   109,    38,     3,     7,  8312,\n",
       "             53,     8, 11820,    40,    38,    17,     3,    99,  6686,   986,\n",
       "           6407,    30,     3,    17,    23,    26, 10812,    42,  5434,     7,\n",
       "          10812,   258,     3,  1744,     9,     1]),\n",
       "  'input_text': ' analysescalled guardswhile as\\nsembling the sql ast if picard matches on\\ntidcid oraliascid  then gua',\n",
       "  'expected_output': 'rds require\\nthat the table tid or the alias alias '},\n",
       " {'input_ids': tensor([    3,    17,     3,    99,  6686,   986,  6407,    30,     3,    17,\n",
       "             23,    26, 10812,    42,  5434,     7, 10812,   258,  4879,     7,\n",
       "           1457,    24,     8,   953,     3,    17,    23,    26,    42,     8,\n",
       "              3,  5434,     7,     3,  5434,     7,     1]),\n",
       "  'input_text': 't if picard matches on\\ntidcid oraliascid  then guards require\\nthat the table tid or the alias alias ',\n",
       "  'expected_output': ' respectively\\nis eventually brought into scope by '},\n",
       " {'input_ids': tensor([   3,   52,   26,    7, 1457,   24,    8,  953,    3,   17,   23,   26,\n",
       "            42,    8,    3, 5434,    7,    3, 5434,    7, 6898,   19, 3725, 1940,\n",
       "           139, 7401,   57,    1]),\n",
       "  'input_text': 'rds require\\nthat the table tid or the alias alias  respectively\\nis eventually brought into scope by ',\n",
       "  'expected_output': 'adding it to the\\nfrom clause moreover the alias al'},\n",
       " {'input_ids': tensor([ 6898,    19,  3725,  1940,   139,  7401,    57,  2651,    34,    12,\n",
       "              8,    45, 14442,    72,  1890,     8,     3,  5434,     7,   491,\n",
       "              1]),\n",
       "  'input_text': ' respectively\\nis eventually brought into scope by adding it to the\\nfrom clause moreover the alias al',\n",
       "  'expected_output': 'ias is con\\nstrained to resolve to a table or a sub'},\n",
       " {'input_ids': tensor([ 2651,    34,    12,     8,    45, 14442,    72,  1890,     8,     3,\n",
       "           5434,     7,     3,  5434,     7,    19,   975,     3, 22418,    12,\n",
       "           7785,    12,     3,     9,   953,    42,     3,     9,   769,     1]),\n",
       "  'input_text': 'adding it to the\\nfrom clause moreover the alias alias is con\\nstrained to resolve to a table or a sub',\n",
       "  'expected_output': 'query that has\\nthe column cid in it if picard matc'},\n",
       " {'input_ids': tensor([    3,    23,     9,     7,    19,   975,     3, 22418,    12,  7785,\n",
       "             12,     3,     9,   953,    42,     3,     9,   769,   835,   651,\n",
       "             24,    65,     8,  6710,     3, 10812,    16,    34,     3,    99,\n",
       "           6686,   986,  6928,    75,     1]),\n",
       "  'input_text': 'ias is con\\nstrained to resolve to a table or a subquery that has\\nthe column cid in it if picard matc',\n",
       "  'expected_output': 'hes on the\\npattern cid then another guard requires'},\n",
       " {'input_ids': tensor([11417,    24,    65,     8,  6710,     3, 10812,    16,    34,     3,\n",
       "             99,  6686,   986,  6407,    30,     8,  3275,     3, 10812,   258,\n",
       "            430,  4879,  2311,     1]),\n",
       "  'input_text': 'query that has\\nthe column cid in it if picard matches on the\\npattern cid then another guard requires',\n",
       "  'expected_output': ' that ex\\nactly one table is eventually brought int'},\n",
       " {'input_ids': tensor([    3,    88,     7,    30,     8,  3275,     3, 10812,   258,   430,\n",
       "           4879,  2311,    24,  1215,  1810,   120,    80,   953,    19,  3725,\n",
       "           1940,    16,    17,     1]),\n",
       "  'input_text': 'hes on the\\npattern cid then another guard requires that ex\\nactly one table is eventually brought int',\n",
       "  'expected_output': 'o scope that\\ncontains a column with that id these '},\n",
       " {'input_ids': tensor([  24, 1215, 1810,  120,   80,  953,   19, 3725, 1940,  139, 7401,   24,\n",
       "          2579,    3,    9, 6710,   28,   24,    3,   23,   26,  175,    1]),\n",
       "  'input_text': ' that ex\\nactly one table is eventually brought into scope that\\ncontains a column with that id these ',\n",
       "  'expected_output': 'guards are\\nenforced eagerly in order to fail fast '},\n",
       " {'input_ids': tensor([    3,    32,  7401,    24,  2579,     3,     9,  6710,    28,    24,\n",
       "              3,    23,    26,   175,  4879,     7,    33, 13321,    26, 10876,\n",
       "            120,    16,   455,    12,  5124,  1006,     1]),\n",
       "  'input_text': 'o scope that\\ncontains a column with that id these guards are\\nenforced eagerly in order to fail fast ',\n",
       "  'expected_output': 'and to eject\\ninvalid hypotheses from the beam at t'},\n",
       " {'input_ids': tensor([ 4879,     7,    33, 13321,    26, 10876,   120,    16,   455,    12,\n",
       "           5124,  1006,    11,    12,     3,    15, 11827, 17070, 10950, 19712,\n",
       "              7,    45,     8, 11638,    44,     3,    17,     1]),\n",
       "  'input_text': 'guards are\\nenforced eagerly in order to fail fast and to eject\\ninvalid hypotheses from the beam at t',\n",
       "  'expected_output': 'he earliest\\npossible time the ﬁrst time this is ha'},\n",
       " {'input_ids': tensor([   11,    12,     3,    15, 11827, 17070, 10950, 19712,     7,    45,\n",
       "              8, 11638,    44,     8,     3, 16454,   487,    97,     8,   166,\n",
       "             97,    48,    19,  4244,     1]),\n",
       "  'input_text': 'and to eject\\ninvalid hypotheses from the beam at the earliest\\npossible time the ﬁrst time this is ha',\n",
       "  'expected_output': 'ppening is\\nafter parsing the from clause\\nonly with'},\n",
       " {'input_ids': tensor([    3,    88,     3, 16454,   487,    97,     8,   166,    97,    48,\n",
       "             19,  4626,    19,   227,   260,     7,    53,     8,    45, 14442,\n",
       "            163,    28,     1]),\n",
       "  'input_text': 'he earliest\\npossible time the ﬁrst time this is happening is\\nafter parsing the from clause\\nonly with',\n",
       "  'expected_output': ' these guards picard is able to\\nreject a wrong pre'},\n",
       " {'input_ids': tensor([    3,  1572,    35,    53,    19,   227,   260,     7,    53,     8,\n",
       "             45, 14442,   163,    28,   175,  4879,     7,  6686,   986,    19,\n",
       "              3,   179,    12, 15092,     3,     9,  1786,   554,     1]),\n",
       "  'input_text': 'ppening is\\nafter parsing the from clause\\nonly with these guards picard is able to\\nreject a wrong pre',\n",
       "  'expected_output': 'diction from our ﬁnetuned\\nt5large model like selec'},\n",
       " {'input_ids': tensor([  175,  4879,     7,  6686,   986,    19,     3,   179,    12, 15092,\n",
       "              3,     9,  1786, 21332,    45,    69,  1399,    17,   444,    26,\n",
       "              3,    17,   755, 15599,   825,   114,     3,     7,   400,    75,\n",
       "              1]),\n",
       "  'input_text': ' these guards picard is able to\\nreject a wrong prediction from our ﬁnetuned\\nt5large model like selec',\n",
       "  'expected_output': 't maker model\\nfrom carmakers for the question what'},\n",
       " {'input_ids': tensor([    3, 12472,    45,    69,  1399,    17,   444,    26,     3,    17,\n",
       "            755, 15599,   825,   114,  1738, 13762,   825,    45,   443,  8910,\n",
       "             21,     8,   822,   125,     1]),\n",
       "  'input_text': 'diction from our ﬁnetuned\\nt5large model like select maker model\\nfrom carmakers for the question what',\n",
       "  'expected_output': ' are\\nthe makers and models here the correct table '},\n",
       " {'input_ids': tensor([    3,    17, 13762,   825,    45,   443,  8910,    21,     8,   822,\n",
       "            125,    33,     8, 13730,    11,  2250,   270,     8,  2024,   953,\n",
       "              1]),\n",
       "  'input_text': 't maker model\\nfrom carmakers for the question what are\\nthe makers and models here the correct table ',\n",
       "  'expected_output': 'to\\nuse would have been modellist  since it is the\\n'},\n",
       " {'input_ids': tensor([   33,     8, 13730,    11,  2250,   270,     8,  2024,   953,    12,\n",
       "            169,   133,    43,   118,   825,  3350,   437,    34,    19,     8,\n",
       "              1]),\n",
       "  'input_text': ' are\\nthe makers and models here the correct table to\\nuse would have been modellist  since it is the\\n',\n",
       "  'expected_output': 'only one in spiders car1 schema that contains\\nboth'},\n",
       " {'input_ids': tensor([   12,   169,   133,    43,   118,   825,  3350,   437,    34,    19,\n",
       "              8,   163,    80,    16, 18612,     7,   443,   536, 26622,    24,\n",
       "           2579,   321,     1]),\n",
       "  'input_text': 'to\\nuse would have been modellist  since it is the\\nonly one in spiders car1 schema that contains\\nboth',\n",
       "  'expected_output': ' a maker and a model column\\nadditional checks and '},\n",
       " {'input_ids': tensor([  163,    80,    16, 18612,     7,   443,   536, 26622,    24,  2579,\n",
       "            321,     3,     9, 13762,    11,     3,     9,   825,  6710,  1151,\n",
       "          11642,    11,     1]),\n",
       "  'input_text': 'only one in spiders car1 schema that contains\\nboth a maker and a model column\\nadditional checks and ',\n",
       "  'expected_output': 'guards are conceivable\\nfor instance checking that '},\n",
       " {'input_ids': tensor([    3,     9, 13762,    11,     3,     9,   825,  6710,  1151, 11642,\n",
       "             11,  4879,     7,    33,     3, 30932,    15,    21,  3421,  6450,\n",
       "             24,     1]),\n",
       "  'input_text': ' a maker and a model column\\nadditional checks and guards are conceivable\\nfor instance checking that ',\n",
       "  'expected_output': 'only expressions of\\nthe same type are compared or '},\n",
       " {'input_ids': tensor([ 4879,     7,    33,     3, 30932,    15,    21,  3421,  6450,    24,\n",
       "            163,  3893,     7,    13,     8,   337,   686,    33,     3,  2172,\n",
       "             42,     1]),\n",
       "  'input_text': 'guards are conceivable\\nfor instance checking that only expressions of\\nthe same type are compared or ',\n",
       "  'expected_output': 'that column types\\nselected by union except  orinte'},\n",
       " {'input_ids': tensor([ 163, 3893,    7,   13,    8,  337,  686,   33,    3, 2172,   42,   24,\n",
       "          6710, 1308, 2639,   57, 7021, 3578,   42, 2429,    1]),\n",
       "  'input_text': 'only expressions of\\nthe same type are compared or that column types\\nselected by union except  orinte',\n",
       "  'expected_output': 'rsect\\nqueries match we leave these additional chec'},\n",
       " {'input_ids': tensor([   24,  6710,  1308,  2639,    57,  7021,  3578,    42,  3870,  7549,\n",
       "             17, 13154,  1588,    62,  1175,   175,  1151,     3,  1033,    75,\n",
       "              1]),\n",
       "  'input_text': 'that column types\\nselected by union except  orintersect\\nqueries match we leave these additional chec',\n",
       "  'expected_output': 'ks to\\nfuture work\\n3 experiments\\nour experiments ar'},\n",
       " {'input_ids': tensor([    3,    52,  7549,    17, 13154,  1588,    62,  1175,   175,  1151,\n",
       "          11642,    12,   647,   161,   220, 12341,    69, 12341,  1584,     1]),\n",
       "  'input_text': 'rsect\\nqueries match we leave these additional checks to\\nfuture work\\n3 experiments\\nour experiments ar',\n",
       "  'expected_output': 'e mainly focused on spider\\nyu et al 2018 a large m'},\n",
       " {'input_ids': tensor([    3,   157,     7,    12,   647,   161,   220, 12341,    69, 12341,\n",
       "             33,     3,  4894,  2937,    30, 18612,     3,    63,    76,     3,\n",
       "             15,    17,   491,   846,     3,     9,   508,     3,    51,     1]),\n",
       "  'input_text': 'ks to\\nfuture work\\n3 experiments\\nour experiments are mainly focused on spider\\nyu et al 2018 a large m',\n",
       "  'expected_output': 'ultidomain and cross\\ndatabase dataset for texttosq'},\n",
       " {'input_ids': tensor([    3,    15,     3,  4894,  2937,    30, 18612,     3,    63,    76,\n",
       "              3,    15,    17,   491,   846,     3,     9,   508,  1249, 22999,\n",
       "             11,  2269,  3501, 17953,    21,  1499,   235,     7,  1824,     1]),\n",
       "  'input_text': 'e mainly focused on spider\\nyu et al 2018 a large multidomain and cross\\ndatabase dataset for texttosq',\n",
       "  'expected_output': 'l parsing we train\\non the 7000 examples in the spi'},\n",
       " {'input_ids': tensor([    3,    83,    17,    23, 22999,    11,  2269,  3501, 17953,    21,\n",
       "           1499,   235,     7,  1824,    40,   260,     7,    53,    62,  2412,\n",
       "             30,     8,     3, 27133,  4062,    16,     8,     3,  7675,     1]),\n",
       "  'input_text': 'ultidomain and cross\\ndatabase dataset for texttosql parsing we train\\non the 7000 examples in the spi',\n",
       "  'expected_output': 'der training set andevaluate on spiders developmen'},\n",
       " {'input_ids': tensor([    3,    40,   260,     7,    53,    62,  2412,    30,     8,     3,\n",
       "          27133,  4062,    16,     8, 18612,   761,   356,    11,  4721, 22156,\n",
       "             30, 18612,     7,  1344,   904,     1]),\n",
       "  'input_text': 'l parsing we train\\non the 7000 examples in the spider training set andevaluate on spiders developmen',\n",
       "  'expected_output': 't set and its hid\\nden test set we also report resu'},\n",
       " {'input_ids': tensor([   74,   761,   356,    11,  4721, 22156,    30, 18612,     7,   606,\n",
       "            356,    11,   165,     3, 11740,   177,   794,   356,    62,    92,\n",
       "            934,     3,    60,     7,    76,     1]),\n",
       "  'input_text': 'der training set andevaluate on spiders development set and its hid\\nden test set we also report resu',\n",
       "  'expected_output': 'lts on the cosql\\nsqlgrounded dialog state tracking'},\n",
       " {'input_ids': tensor([    3,    17,   356,    11,   165,     3, 11740,   177,   794,   356,\n",
       "             62,    92,   934,   772,    30,     8,   576,     7,  1824,    40,\n",
       "          11820,    40,   122, 12279, 13463,   538,  6418,     1]),\n",
       "  'input_text': 't set and its hid\\nden test set we also report results on the cosql\\nsqlgrounded dialog state tracking',\n",
       "  'expected_output': ' task yu et al\\n2019 where we predict a sql query f'},\n",
       " {'input_ids': tensor([    3,    40,    17,     7,    30,     8,   576,     7,  1824,    40,\n",
       "          11820,    40,   122, 12279, 13463,   538,  6418,  2491,     3,    63,\n",
       "             76,     3,    15,    17,   491,  1360,   213,    62,  9689,     3,\n",
       "              9, 11820,    40, 11417,     3,    89,     1]),\n",
       "  'input_text': 'lts on the cosql\\nsqlgrounded dialog state tracking task yu et al\\n2019 where we predict a sql query f',\n",
       "  'expected_output': 'or each\\nquestion given previous questions in an in'},\n",
       " {'input_ids': tensor([ 2491,     3,    63,    76,     3,    15,    17,   491,  1360,   213,\n",
       "             62,  9689,     3,     9, 11820,    40, 11417,    21,   284,   822,\n",
       "            787,  1767,   746,    16,    46,    16,     1]),\n",
       "  'input_text': ' task yu et al\\n2019 where we predict a sql query for each\\nquestion given previous questions in an in',\n",
       "  'expected_output': 'teraction\\ncontext for this task we train on both t'},\n",
       " {'input_ids': tensor([  42,  284,  822,  787, 1767,  746,   16,   46, 6565, 2625,   21,   48,\n",
       "          2491,   62, 2412,   30,  321,    3,   17,    1]),\n",
       "  'input_text': 'or each\\nquestion given previous questions in an interaction\\ncontext for this task we train on both t',\n",
       "  'expected_output': 'he spider\\ntexttosql training data and the cosql di'},\n",
       " {'input_ids': tensor([    3,   449,  4787,  2625,    21,    48,  2491,    62,  2412,    30,\n",
       "            321,     8, 18612,  1499,   235,     7,  1824,    40,   761,   331,\n",
       "             11,     8,   576,     7,  1824,    40,  1227,     1]),\n",
       "  'input_text': 'teraction\\ncontext for this task we train on both the spider\\ntexttosql training data and the cosql di',\n",
       "  'expected_output': 'alog\\nstate tracking training data and evaluate on '},\n",
       " {'input_ids': tensor([    3,    88, 18612,  1499,   235,     7,  1824,    40,   761,   331,\n",
       "             11,     8,   576,     7,  1824,    40, 13463,   538,  6418,   761,\n",
       "            331,    11,  6825,    30,     1]),\n",
       "  'input_text': 'he spider\\ntexttosql training data and the cosql dialog\\nstate tracking training data and evaluate on ',\n",
       "  'expected_output': 'the\\ncosql development and test sets\\nspider and cos'},\n",
       " {'input_ids': tensor([    3,     9,  2152,   538,  6418,   761,   331,    11,  6825,    30,\n",
       "              8,   576,     7,  1824,    40,   606,    11,   794,  3369, 18612,\n",
       "             11,   576,     7,     1]),\n",
       "  'input_text': 'alog\\nstate tracking training data and evaluate on the\\ncosql development and test sets\\nspider and cos',\n",
       "  'expected_output': 'ql are both zeroshot settings\\nthere is no overlap '},\n",
       " {'input_ids': tensor([    8,   576,     7,  1824,    40,   606,    11,   794,  3369, 18612,\n",
       "             11,   576,     7,  1824,    40,    33,   321,  5733, 11159,  3803,\n",
       "            132,    19,   150, 21655,     1]),\n",
       "  'input_text': 'the\\ncosql development and test sets\\nspider and cosql are both zeroshot settings\\nthere is no overlap ',\n",
       "  'expected_output': 'between questions or databases\\nbetween the respect'},\n",
       " {'input_ids': tensor([    3,  1824,    40,    33,   321,  5733, 11159,  3803,   132,    19,\n",
       "            150, 21655,   344,   746,    42, 16961,   344,     8,  1445,     1]),\n",
       "  'input_text': 'ql are both zeroshot settings\\nthere is no overlap between questions or databases\\nbetween the respect',\n",
       "  'expected_output': 'ive training development and\\ntest sets\\non spider w'},\n",
       " {'input_ids': tensor([  344,   746,    42, 16961,   344,     8,  6477,   761,   606,    11,\n",
       "            794,  3369,    30, 18612,     3,   210,     1]),\n",
       "  'input_text': 'between questions or databases\\nbetween the respective training development and\\ntest sets\\non spider w',\n",
       "  'expected_output': 'e determine model performance\\nbased on three metri'},\n",
       " {'input_ids': tensor([    3,   757,   761,   606,    11,   794,  3369,    30, 18612,    62,\n",
       "           2082,   825,   821,     3,   390,    30,   386,     3,  8180,     1]),\n",
       "  'input_text': 'ive training development and\\ntest sets\\non spider we determine model performance\\nbased on three metri',\n",
       "  'expected_output': 'cs exactsetmatch accuracy\\nexecution accuracy and t'},\n",
       " {'input_ids': tensor([    3,    15,  2082,   825,   821,     3,   390,    30,   386, 15905,\n",
       "           2883,  2244, 19515,  7452,  9328,  7452,    11,     3,    17,     1]),\n",
       "  'input_text': 'e determine model performance\\nbased on three metrics exactsetmatch accuracy\\nexecution accuracy and t',\n",
       "  'expected_output': 'estsuite execution accu\\nracy zhong et al 2020 exac'},\n",
       " {'input_ids': tensor([    3,    75,     7,  2883,  2244, 19515,  7452,  9328,  7452,    11,\n",
       "           3830,  8431,  9328,     3,  6004,    76,     3,    52,  4710,     3,\n",
       "            172, 23001,     3,    15,    17,   491,  6503,  1215,     9,    75,\n",
       "              1]),\n",
       "  'input_text': 'cs exactsetmatch accuracy\\nexecution accuracy and testsuite execution accu\\nracy zhong et al 2020 exac',\n",
       "  'expected_output': 'tsetmatch accu\\nracy compares the predicted and the'},\n",
       " {'input_ids': tensor([  259,     7,  8431,  9328,     3,  6004,    76,     3,    52,  4710,\n",
       "              3,   172, 23001,     3,    15,    17,   491,  6503,  2883,  2244,\n",
       "          19515,     3,  6004,    76,     3,    52,  4710,  4048,     7,     8,\n",
       "          15439,    11,     8,     1]),\n",
       "  'input_text': 'estsuite execution accu\\nracy zhong et al 2020 exactsetmatch accu\\nracy compares the predicted and the',\n",
       "  'expected_output': ' groundtruth\\nsql query by parsing both into a norm'},\n",
       " {'input_ids': tensor([    3,    17,  2244, 19515,     3,  6004,    76,     3,    52,  4710,\n",
       "           4048,     7,     8, 15439,    11,     8,  1591,  2666,   189, 11820,\n",
       "             40, 11417,    57,   260,     7,    53,   321,   139,     3,     9,\n",
       "           7982,     1]),\n",
       "  'input_text': 'tsetmatch accu\\nracy compares the predicted and the groundtruth\\nsql query by parsing both into a norm',\n",
       "  'expected_output': 'alized data\\nstructure this comparison is not sensi'},\n",
       " {'input_ids': tensor([ 1591,  2666,   189, 11820,    40, 11417,    57,   260,     7,    53,\n",
       "            321,   139,     3,     9,  1389,  1601,   331,  1809,    48,  4993,\n",
       "             19,    59,  3952,    23,     1]),\n",
       "  'input_text': ' groundtruth\\nsql query by parsing both into a normalized data\\nstructure this comparison is not sensi',\n",
       "  'expected_output': 'tive to lit\\neral query values and can decrease und'},\n",
       " {'input_ids': tensor([  491,  1601,   331,  1809,    48,  4993,    19,    59,  6280,    12,\n",
       "           4996,     3,    49,   138, 11417,  2620,    11,    54,  6313,    64,\n",
       "              1]),\n",
       "  'input_text': 'alized data\\nstructure this comparison is not sensitive to lit\\neral query values and can decrease und',\n",
       "  'expected_output': 'er semantic\\npreserving sql query rewriting executi'},\n",
       " {'input_ids': tensor([    3,  3268,    12,  4996,     3,    49,   138, 11417,  2620,    11,\n",
       "             54,  6313,   365, 27632,     3, 22140, 11820,    40, 11417,     3,\n",
       "             60,  9933,  9362,    23,     1]),\n",
       "  'input_text': 'tive to lit\\neral query values and can decrease under semantic\\npreserving sql query rewriting executi',\n",
       "  'expected_output': 'on accu\\nracy compares the results of executing the'},\n",
       " {'input_ids': tensor([    3,    49, 27632,     3, 22140, 11820,    40, 11417,     3,    60,\n",
       "           9933,  9328,     3,  6004,    76,     3,    52,  4710,  4048,     7,\n",
       "              8,   772,    13,     3, 26685,     8,     1]),\n",
       "  'input_text': 'er semantic\\npreserving sql query rewriting execution accu\\nracy compares the results of executing the',\n",
       "  'expected_output': ' predicted\\nand groundtruth sql queries on the data'},\n",
       " {'input_ids': tensor([   30,     3,  6004,    76,     3,    52,  4710,  4048,     7,     8,\n",
       "            772,    13,     3, 26685,     8, 15439,    11,  1591,  2666,   189,\n",
       "          11820,    40, 13154,    30,     8,   331,     1]),\n",
       "  'input_text': 'on accu\\nracy compares the results of executing the predicted\\nand groundtruth sql queries on the data',\n",
       "  'expected_output': 'base con\\ntents shipped with the spider dataset thi'},\n",
       " {'input_ids': tensor([15439,    11,  1591,  2666,   189, 11820,    40, 13154,    30,     8,\n",
       "           3501,   975,  5748,     7, 10737,    28,     8, 18612, 17953,     3,\n",
       "           7436,     1]),\n",
       "  'input_text': ' predicted\\nand groundtruth sql queries on the database con\\ntents shipped with the spider dataset thi',\n",
       "  'expected_output': 's metric is\\nsensitive to literal query values but '},\n",
       " {'input_ids': tensor([ 1247,   975,  5748,     7, 10737,    28,     8, 18612, 17953,    48,\n",
       "              3,  7959,    19,  6280,    12, 26998, 11417,  2620,    68,     1]),\n",
       "  'input_text': 'base con\\ntents shipped with the spider dataset this metric is\\nsensitive to literal query values but ',\n",
       "  'expected_output': 'suffers from a\\nhigh false positive rate zhong et a'},\n",
       " {'input_ids': tensor([    3,     7,     3,  7959,    19,  6280,    12, 26998, 11417,  2620,\n",
       "             68,  5696,     7,    45,     3,     9,   306,  6136,  1465,  1080,\n",
       "              3,   172, 23001,     3,    15,    17,     3,     9,     1]),\n",
       "  'input_text': 's metric is\\nsensitive to literal query values but suffers from a\\nhigh false positive rate zhong et a',\n",
       "  'expected_output': 'l 2020 lastly\\ntestsuite execution accuracy extends'},\n",
       " {'input_ids': tensor([ 5696,     7,    45,     3,     9,   306,  6136,  1465,  1080,     3,\n",
       "            172, 23001,     3,    15,    17,   491,  6503,   336,   120,  3830,\n",
       "           8431,  9328,  7452,  4285,     7,     1]),\n",
       "  'input_text': 'suffers from a\\nhigh false positive rate zhong et al 2020 lastly\\ntestsuite execution accuracy extends',\n",
       "  'expected_output': ' execution to\\nmultiple database instances per sql '},\n",
       " {'input_ids': tensor([    3,    40,  6503,   336,   120,  3830,  8431,  9328,  7452,  4285,\n",
       "              7,  9328,    12,  1317,  3501, 10316,   399, 11820,    40,     1]),\n",
       "  'input_text': 'l 2020 lastly\\ntestsuite execution accuracy extends execution to\\nmultiple database instances per sql ',\n",
       "  'expected_output': 'schema the\\ncontents of these instances are optimiz'},\n",
       " {'input_ids': tensor([ 9328,    12,  1317,  3501, 10316,   399, 11820,    40, 26622,     8,\n",
       "          10223,    13,   175, 10316,    33, 19769,     1]),\n",
       "  'input_text': ' execution to\\nmultiple database instances per sql schema the\\ncontents of these instances are optimiz',\n",
       "  'expected_output': 'ed to lower\\nthe number of false positives and to p'},\n",
       " {'input_ids': tensor([26622,     8, 10223,    13,   175, 10316,    33, 18149,    12,  1364,\n",
       "              8,   381,    13,  6136,  1465,     7,    11,    12,     3,   102,\n",
       "              1]),\n",
       "  'input_text': 'schema the\\ncontents of these instances are optimized to lower\\nthe number of false positives and to p',\n",
       "  'expected_output': 'rovide the\\nbest approximation of semantic accuracy'},\n",
       " {'input_ids': tensor([    3,    15,    26,    12,  1364,     8,   381,    13,  6136,  1465,\n",
       "              7,    11,    12,   370,     8,   200,  1120, 12907,   603,   257,\n",
       "             13, 27632,  7452,     1]),\n",
       "  'input_text': 'ed to lower\\nthe number of false positives and to provide the\\nbest approximation of semantic accuracy',\n",
       "  'expected_output': '\\non cosql we measure model performance in\\nterms of'},\n",
       " {'input_ids': tensor([    3,  8843,  1599,     8,   200,  1120, 12907,   603,   257,    13,\n",
       "          27632,  7452,    30,   576,     7,  1824,    40,    62,  3613,   825,\n",
       "            821,    16,  1353,    13,     1]),\n",
       "  'input_text': 'rovide the\\nbest approximation of semantic accuracy\\non cosql we measure model performance in\\nterms of',\n",
       "  'expected_output': ' the question match accuracy and the inter\\naction '},\n",
       " {'input_ids': tensor([  30,  576,    7, 1824,   40,   62, 3613,  825,  821,   16, 1353,   13,\n",
       "             8,  822, 1588, 7452,   11,    8, 1413, 1041,    1]),\n",
       "  'input_text': '\\non cosql we measure model performance in\\nterms of the question match accuracy and the inter\\naction ',\n",
       "  'expected_output': 'match accuracy both metrics are based on\\nexactsetm'},\n",
       " {'input_ids': tensor([    8,   822,  1588,  7452,    11,     8,  1413,  1041,  1588,  7452,\n",
       "            321, 15905,    33,     3,   390,    30,  2883,  2244,    51,     1]),\n",
       "  'input_text': ' the question match accuracy and the inter\\naction match accuracy both metrics are based on\\nexactsetm',\n",
       "  'expected_output': 'atch accuracy interaction match accu\\nracy is the j'},\n",
       " {'input_ids': tensor([ 1588,  7452,   321, 15905,    33,     3,   390,    30,  2883,  2244,\n",
       "          19515,  7452,  6565,  1588,     3,  6004,    76,     3,    52,  4710,\n",
       "             19,     8,     3,   354,     1]),\n",
       "  'input_text': 'match accuracy both metrics are based on\\nexactsetmatch accuracy interaction match accu\\nracy is the j',\n",
       "  'expected_output': 'oint accuracy over all questions in an\\ninteraction'},\n",
       " {'input_ids': tensor([    3, 14547,  7452,  6565,  1588,     3,  6004,    76,     3,    52,\n",
       "           4710,    19,     8,  4494,  7452,   147,    66,   746,    16,    46,\n",
       "           6565,     1]),\n",
       "  'input_text': 'atch accuracy interaction match accu\\nracy is the joint accuracy over all questions in an\\ninteraction',\n",
       "  'expected_output': '\\nwe are encouraged by results by shaw et al\\n2021 w'},\n",
       " {'input_ids': tensor([    3,    32,    77,    17,  7452,   147,    66,   746,    16,    46,\n",
       "           6565,    62,    33,  6470,    57,   772,    57,     3, 15622,     3,\n",
       "             15,    17,   491,   460,  2658,     3,   210,     1]),\n",
       "  'input_text': 'oint accuracy over all questions in an\\ninteraction\\nwe are encouraged by results by shaw et al\\n2021 w',\n",
       "  'expected_output': 'ho showed that a pretrained t5base\\nor t53b model c'},\n",
       " {'input_ids': tensor([   62,    33,  6470,    57,   772,    57,     3, 15622,     3,    15,\n",
       "             17,   491,   460,  2658,   113,  3217,    24,     3,     9,  7140,\n",
       "          10761,     3,    17,   755, 10925,    42,     3,    17,  4867,   115,\n",
       "            825,     3,    75,     1]),\n",
       "  'input_text': '\\nwe are encouraged by results by shaw et al\\n2021 who showed that a pretrained t5base\\nor t53b model c',\n",
       "  'expected_output': 'an not only learn the textto\\nsql task but also gen'},\n",
       " {'input_ids': tensor([ 3534,  3217,    24,     3,     9,  7140, 10761,     3,    17,   755,\n",
       "          10925,    42,     3,    17,  4867,   115,   825,    54,    59,   163,\n",
       "            669,     8,  1499,   235, 11820,    40,  2491,    68,    92,     3,\n",
       "            729,     1]),\n",
       "  'input_text': 'ho showed that a pretrained t5base\\nor t53b model can not only learn the textto\\nsql task but also gen',\n",
       "  'expected_output': 'eralize to unseen databases\\nand even that t53b can'},\n",
       " {'input_ids': tensor([   46,    59,   163,   669,     8,  1499,   235, 11820,    40,  2491,\n",
       "             68,    92,   879,  1737,    12,  1149,    15,    35, 16961,    11,\n",
       "            237,    24,     3,    17,  4867,   115,    54,     1]),\n",
       "  'input_text': 'an not only learn the textto\\nsql task but also generalize to unseen databases\\nand even that t53b can',\n",
       "  'expected_output': ' be competitive with the\\nthenstateoftheart choi et'},\n",
       " {'input_ids': tensor([    3,    49,   138,  1737,    12,  1149,    15,    35, 16961,    11,\n",
       "            237,    24,     3,    17,  4867,   115,    54,    36,  3265,    28,\n",
       "              8,   258,  5540,   858,   532,  1408,     3,  3995,    23,     3,\n",
       "             15,    17,     1]),\n",
       "  'input_text': 'eralize to unseen databases\\nand even that t53b can be competitive with the\\nthenstateoftheart choi et',\n",
       "  'expected_output': ' al 2021 wang et al\\n2020all without modiﬁcations t'},\n",
       " {'input_ids': tensor([   36,  3265,    28,     8,   258,  5540,   858,   532,  1408,     3,\n",
       "           3995,    23,     3,    15,    17,   491,   460,  2658,     3, 17789,\n",
       "              3,    15,    17,   491,  6503,  1748,   406, 14172,     3,    17,\n",
       "              1]),\n",
       "  'input_text': ' be competitive with the\\nthenstateoftheart choi et al 2021 wang et al\\n2020all without modiﬁcations t',\n",
       "  'expected_output': 'o the model we\\ntherefore use t5 as the baseline fo'},\n",
       " {'input_ids': tensor([  491,   460,  2658,     3, 17789,     3,    15,    17,   491,  6503,\n",
       "           1748,   406, 14172,    12,     8,   825,    62,  2459,   169,     3,\n",
       "             17,   755,    38,     8, 20726,  5575,     1]),\n",
       "  'input_text': ' al 2021 wang et al\\n2020all without modiﬁcations to the model we\\ntherefore use t5 as the baseline fo',\n",
       "  'expected_output': 'r all our experi\\nments\\nin order to allow for gener'},\n",
       " {'input_ids': tensor([    3,    32,     8,   825,    62,  2459,   169,     3,    17,   755,\n",
       "             38,     8, 20726,    21,    66,    69,  1215,  4267,     3,  4128,\n",
       "             16,   455,    12,   995,    21, 11467,     1]),\n",
       "  'input_text': 'o the model we\\ntherefore use t5 as the baseline for all our experi\\nments\\nin order to allow for gener',\n",
       "  'expected_output': 'alization to unseen\\ndatabases we encode the schema'},\n",
       " {'input_ids': tensor([    3,    52,    66,    69,  1215,  4267,     3,  4128,    16,   455,\n",
       "             12,   995,    21,   879,  1707,    12,  1149,    15,    35, 16961,\n",
       "             62, 23734,     8, 26622,     1]),\n",
       "  'input_text': 'r all our experi\\nments\\nin order to allow for generalization to unseen\\ndatabases we encode the schema',\n",
       "  'expected_output': ' together with the\\nquestions we use the same seria'},\n",
       " {'input_ids': tensor([  491,  1707,    12,  1149,    15,    35, 16961,    62, 23734,     8,\n",
       "          26622,   544,    28,     8,   746,    62,   169,     8,   337,     3,\n",
       "              7,  4476,     1]),\n",
       "  'input_text': 'alization to unseen\\ndatabases we encode the schema together with the\\nquestions we use the same seria',\n",
       "  'expected_output': 'lization scheme\\nused by shaw et al 2021 in experim'},\n",
       " {'input_ids': tensor([  544,    28,     8,   746,    62,   169,     8,   337, 10501,  1707,\n",
       "           5336,   261,    57,     3, 15622,     3,    15,    17,   491,   460,\n",
       "           2658,    16,  1215,  4267,    51,     1]),\n",
       "  'input_text': ' together with the\\nquestions we use the same serialization scheme\\nused by shaw et al 2021 in experim',\n",
       "  'expected_output': 'ents using9898database content we detect and attac'},\n",
       " {'input_ids': tensor([    3,    40,  1707,  5336,   261,    57,     3, 15622,     3,    15,\n",
       "             17,   491,   460,  2658,    16, 12341,   338,  3916,  3916,  6757,\n",
       "          10925,   738,    62,  8432,    11,     3, 14748,    75,     1]),\n",
       "  'input_text': 'lization scheme\\nused by shaw et al 2021 in experiments using9898database content we detect and attac',\n",
       "  'expected_output': 'h the database\\nvalues to the column names in a fas'},\n",
       " {'input_ids': tensor([    3,   295,     7,   338,  3916,  3916,  6757, 10925,   738,    62,\n",
       "           8432,    11,  9129,     8,  3501,  2620,    12,     8,  6710,  3056,\n",
       "             16,     3,     9,     3,    89,     9,     7,     1]),\n",
       "  'input_text': 'ents using9898database content we detect and attach the database\\nvalues to the column names in a fas',\n",
       "  'expected_output': 'hion similar to\\nthe bridge model by lin et al 2020'},\n",
       " {'input_ids': tensor([   3,  107,    8, 3501, 2620,   12,    8, 6710, 3056,   16,    3,    9,\n",
       "          2934, 1126,   12,    8, 4716,  825,   57,    3,   40,   77,    3,   15,\n",
       "            17,  491, 6503,    1]),\n",
       "  'input_text': 'h the database\\nvalues to the column names in a fashion similar to\\nthe bridge model by lin et al 2020',\n",
       "  'expected_output': ' when\\nﬁnetuning for the cosql dialog state trackin'},\n",
       " {'input_ids': tensor([ 7102,   106,  1126,    12,     8,  4716,   825,    57,     3,    40,\n",
       "             77,     3,    15,    17,   491,  6503,   116,  1399,    17,   202,\n",
       "             53,    21,     8,   576,     7,  1824,    40, 13463,   538,  1463,\n",
       "             77,     1]),\n",
       "  'input_text': 'hion similar to\\nthe bridge model by lin et al 2020 when\\nﬁnetuning for the cosql dialog state trackin',\n",
       "  'expected_output': 'g\\ntask we append the previous questions in the in\\n'},\n",
       " {'input_ids': tensor([  116,  1399,    17,   202,    53,    21,     8,   576,     7,  1824,\n",
       "             40, 13463,   538,  6418,  2491,    62,  1120,   989,     8,  1767,\n",
       "            746,    16,     8,    16,     1]),\n",
       "  'input_text': ' when\\nﬁnetuning for the cosql dialog state tracking\\ntask we append the previous questions in the in\\n',\n",
       "  'expected_output': 'teraction in reverse chronological order to the in'},\n",
       " {'input_ids': tensor([    3,   122,  2491,    62,  1120,   989,     8,  1767,   746,    16,\n",
       "              8,    16,     3,   449,  4787,    16,  7211, 31803,   455,    12,\n",
       "              8,    16,     1]),\n",
       "  'input_text': 'g\\ntask we append the previous questions in the in\\nteraction in reverse chronological order to the in',\n",
       "  'expected_output': '\\nput inputs exceeding the 512token limit of t5 are'},\n",
       " {'input_ids': tensor([    3,   449,  4787,    16,  7211, 31803,   455,    12,     8,    16,\n",
       "            474,  3785,     7, 19829,     8,     3, 24163,   235,  2217,  2006,\n",
       "             13,     3,    17,   755,    33,     1]),\n",
       "  'input_text': 'teraction in reverse chronological order to the in\\nput inputs exceeding the 512token limit of t5 are',\n",
       "  'expected_output': '\\ntruncated the target is the sql from the spider\\na'},\n",
       " {'input_ids': tensor([  474,  3785,     7, 19829,     8,     3, 24163,   235,  2217,  2006,\n",
       "             13,     3,    17,   755,    33,     3,    17,  4312,    75,   920,\n",
       "              8,  2387,    19,     8, 11820,    40,    45,     8, 18612,     3,\n",
       "              9,     1]),\n",
       "  'input_text': '\\nput inputs exceeding the 512token limit of t5 are\\ntruncated the target is the sql from the spider\\na',\n",
       "  'expected_output': 'ndor cosql training sets unmodiﬁed except for\\na co'},\n",
       " {'input_ids': tensor([    3,    17,  4312,    75,   920,     8,  2387,    19,     8, 11820,\n",
       "             40,    45,     8, 18612,    11,   127,   576,     7,  1824,    40,\n",
       "            761,  3369,    73,  7360,  3676,  3578,    21,     3,     9,   576,\n",
       "              1]),\n",
       "  'input_text': '\\ntruncated the target is the sql from the spider\\nandor cosql training sets unmodiﬁed except for\\na co',\n",
       "  'expected_output': 'nversion of keywords and identiﬁers to lower\\ncase '},\n",
       " {'input_ids': tensor([    3,   727,   127,   576,     7,  1824,    40,   761,  3369,    73,\n",
       "           7360,  3676,  3578,    21,     3,     9,  6113,    13, 12545,    11,\n",
       "              3,  8826,    52,     7,    12,  1364,   495,     1]),\n",
       "  'input_text': 'ndor cosql training sets unmodiﬁed except for\\na conversion of keywords and identiﬁers to lower\\ncase ',\n",
       "  'expected_output': 'we ﬁnetune t5 for up to 3072 epochs using\\nadafacto'},\n",
       " {'input_ids': tensor([    3,    29,  8674,    13, 12545,    11,     3,  8826,    52,     7,\n",
       "             12,  1364,   495,    62,  1399,    17,   444,     3,    17,   755,\n",
       "             21,    95,    12,   604,  5865,     3,    15,   102,  6322,     7,\n",
       "            338,     3,     9,    26,     9,  8717,    32,     1]),\n",
       "  'input_text': 'nversion of keywords and identiﬁers to lower\\ncase we ﬁnetune t5 for up to 3072 epochs using\\nadafacto',\n",
       "  'expected_output': 'r shazeer and stern 2018 a batch size\\nof2048  and '},\n",
       " {'input_ids': tensor([   62,  1399,    17,   444,     3,    17,   755,    21,    95,    12,\n",
       "            604,  5865,     3,    15,   102,  6322,     7,   338,     3,     9,\n",
       "             26,     9, 17899,     3,     7, 10557,    15,    49,    11,     3,\n",
       "          13072,   846,     3,     9, 11587,   812,    13,  1755,  3707,    11,\n",
       "              1]),\n",
       "  'input_text': 'we ﬁnetune t5 for up to 3072 epochs using\\nadafactor shazeer and stern 2018 a batch size\\nof2048  and ',\n",
       "  'expected_output': 'a learning rate of 104\\nresults our ﬁndings on the '},\n",
       " {'input_ids': tensor([    3,    52,     3,     7, 10557,    15,    49,    11,     3, 13072,\n",
       "            846,     3,     9, 11587,   812,    13,  1755,  3707,    11,     3,\n",
       "              9,  1036,  1080,    13,     3, 15442,   772,    69,  7469,    30,\n",
       "              8,     1]),\n",
       "  'input_text': 'r shazeer and stern 2018 a batch size\\nof2048  and a learning rate of 104\\nresults our ﬁndings on the ',\n",
       "  'expected_output': 'spider dataset are\\nsummarized in table 1 and figur'},\n",
       " {'input_ids': tensor([    3,     9,  1036,  1080,    13,     3, 15442,   772,    69,  7469,\n",
       "             30,     8, 18612, 17953,    33, 21603,    26,    16,   953,   209,\n",
       "             11,     3,  9178,     1]),\n",
       "  'input_text': 'a learning rate of 104\\nresults our ﬁndings on the spider dataset are\\nsummarized in table 1 and figur',\n",
       "  'expected_output': 'e 1 our repro\\nductions of shaw et al 2021s results'},\n",
       " {'input_ids': tensor([18612, 17953,    33, 21603,    26,    16,   953,   209,    11,  2320,\n",
       "            209,    69, 19482,     3,  8291,     7,    13,     3, 15622,     3,\n",
       "             15,    17,   491,   460,  2658,     7,   772,     1]),\n",
       "  'input_text': 'spider dataset are\\nsummarized in table 1 and figure 1 our repro\\nductions of shaw et al 2021s results',\n",
       "  'expected_output': ' with t5\\ncannot compete with the current state of '},\n",
       " {'input_ids': tensor([    3,    15,   209,    69, 19482,     3,  8291,     7,    13,     3,\n",
       "          15622,     3,    15,    17,   491,   460,  2658,     7,   772,    28,\n",
       "              3,    17,   755,  1178,  5978,    28,     8,   750,   538,    13,\n",
       "              1]),\n",
       "  'input_text': 'e 1 our repro\\nductions of shaw et al 2021s results with t5\\ncannot compete with the current state of ',\n",
       "  'expected_output': 'the art on\\nspider the issue is that these models p'},\n",
       " {'input_ids': tensor([   28,     3,    17,   755,  1178,  5978,    28,     8,   750,   538,\n",
       "             13,     8,   768,    30, 18612,     8,   962,    19,    24,   175,\n",
       "           2250,     3,   102,     1]),\n",
       "  'input_text': ' with t5\\ncannot compete with the current state of the art on\\nspider the issue is that these models p',\n",
       "  'expected_output': 'redict a\\nlot of invalid sql for instance 12 of the'},\n",
       " {'input_ids': tensor([    8,   768,    30, 18612,     8,   962,    19,    24,   175,  2250,\n",
       "           9689,     3,     9,   418,    13, 17070, 11820,    40,    21,  3421,\n",
       "            586,    13,     8,     1]),\n",
       "  'input_text': 'the art on\\nspider the issue is that these models predict a\\nlot of invalid sql for instance 12 of the',\n",
       "  'expected_output': ' sql\\nqueries generated by the t53b model on spider'},\n",
       " {'input_ids': tensor([    3,    60, 12194,     3,     9,   418,    13, 17070, 11820,    40,\n",
       "             21,  3421,   586,    13,     8, 11820,    40, 13154,  6126,    57,\n",
       "              8,     3,    17,  4867,   115,   825,    30, 18612,     1]),\n",
       "  'input_text': 'redict a\\nlot of invalid sql for instance 12 of the sql\\nqueries generated by the t53b model on spider',\n",
       "  'expected_output': 's\\ndevelopment set result in an execution error how'},\n",
       " {'input_ids': tensor([11820,    40, 13154,  6126,    57,     8,     3,    17,  4867,   115,\n",
       "            825,    30, 18612,     7,   606,   356,   741,    16,    46,  9328,\n",
       "           3505,   149,     1]),\n",
       "  'input_text': ' sql\\nqueries generated by the t53b model on spiders\\ndevelopment set result in an execution error how',\n",
       "  'expected_output': '\\never when these same models are augmented with\\npi'},\n",
       " {'input_ids': tensor([    3,     7,   606,   356,   741,    16,    46,  9328,  3505,   149,\n",
       "            664,   116,   175,   337,  2250,    33,     3, 28984,    28,  2816,\n",
       "              1]),\n",
       "  'input_text': 's\\ndevelopment set result in an execution error how\\never when these same models are augmented with\\npi',\n",
       "  'expected_output': 'card  we ﬁnd substantial improvements first\\ninvali'},\n",
       " {'input_ids': tensor([  664,   116,   175,   337,  2250,    33,     3, 28984,    28,  6686,\n",
       "            986,    62,   253,  7354,  6867,   166,    16,  2165,    23,     1]),\n",
       "  'input_text': '\\never when these same models are augmented with\\npicard  we ﬁnd substantial improvements first\\ninvali',\n",
       "  'expected_output': 'd sql predictions become rare for t53b\\nwith picard'},\n",
       " {'input_ids': tensor([  895,    62,   253,  7354,  6867,   166, 17070, 11820,    40, 20099,\n",
       "            582,  3400,    21,     3,    17,  4867,   115,    28,  6686,   986,\n",
       "              1]),\n",
       "  'input_text': 'card  we ﬁnd substantial improvements first\\ninvalid sql predictions become rare for t53b\\nwith picard',\n",
       "  'expected_output': '  only 2 of the predictions are un\\nusable in these'},\n",
       " {'input_ids': tensor([    3,    26, 11820,    40, 20099,   582,  3400,    21,     3,    17,\n",
       "           4867,   115,    28,  6686,   986,   163,   204,    13,     8, 20099,\n",
       "             33,    73,   178,   179,    16,   175,     1]),\n",
       "  'input_text': 'd sql predictions become rare for t53b\\nwith picard  only 2 of the predictions are un\\nusable in these',\n",
       "  'expected_output': ' cases beam search exited without\\nﬁnding a valid s'},\n",
       " {'input_ids': tensor([  163,   204,    13,     8, 20099,    33,    73,   178,   179,    16,\n",
       "            175,  1488, 11638,   960,  7189,    15,    26,   406,  2342,     3,\n",
       "              9,  3982,     3,     7,     1]),\n",
       "  'input_text': '  only 2 of the predictions are un\\nusable in these cases beam search exited without\\nﬁnding a valid s',\n",
       "  'expected_output': 'ql prediction second and most\\nsigniﬁcantly by usin'},\n",
       " {'input_ids': tensor([ 1488, 11638,   960,  7189,    15,    26,   406,  2342,     3,     9,\n",
       "           3982, 11820,    40, 21332,   511,    11,   167,  4019,    57,   178,\n",
       "             77,     1]),\n",
       "  'input_text': ' cases beam search exited without\\nﬁnding a valid sql prediction second and most\\nsigniﬁcantly by usin',\n",
       "  'expected_output': 'g picard  the t53b model is\\nlifted to stateofthear'},\n",
       " {'input_ids': tensor([    3,  1824,    40, 21332,   511,    11,   167,  4019,    57,   338,\n",
       "           6686,   986,     8,     3,    17,  4867,   115,   825,    19, 19464,\n",
       "             12,   538,   858,   532,   291,     1]),\n",
       "  'input_text': 'ql prediction second and most\\nsigniﬁcantly by using picard  the t53b model is\\nlifted to stateofthear',\n",
       "  'expected_output': 't performance we measure\\nan exactsetmatch accuracy'},\n",
       " {'input_ids': tensor([    3,   122,  6686,   986,     8,     3,    17,  4867,   115,   825,\n",
       "             19, 19464,    12,   538,   858,   532,  1408,   821,    62,  3613,\n",
       "             46,  2883,  2244, 19515,  7452,     1]),\n",
       "  'input_text': 'g picard  the t53b model is\\nlifted to stateoftheart performance we measure\\nan exactsetmatch accuracy',\n",
       "  'expected_output': ' of 755 on the devel\\nopment set and 719 on the tes'},\n",
       " {'input_ids': tensor([    3,    17,   821,    62,  3613,    46,  2883,  2244, 19515,  7452,\n",
       "             13,   489,  3769,    30,     8,    20,  4911,     3,    32,   102,\n",
       "            297,   356,    11,   489,  2294,    30,     8,     3,  1422,     1]),\n",
       "  'input_text': 't performance we measure\\nan exactsetmatch accuracy of 755 on the devel\\nopment set and 719 on the tes',\n",
       "  'expected_output': 't set the execution\\naccuracy results are 793 and 7'},\n",
       " {'input_ids': tensor([  13,  489, 3769,   30,    8,   20, 4911,    3,   32,  102,  297,  356,\n",
       "            11,  489, 2294,   30,    8,  794,  356,    8, 9328, 7452,  772,   33,\n",
       "           489, 4271,   11,  489,    1]),\n",
       "  'input_text': ' of 755 on the devel\\nopment set and 719 on the test set the execution\\naccuracy results are 793 and 7',\n",
       "  'expected_output': '51 respectively\\nthese numbers are on par or higher'},\n",
       " {'input_ids': tensor([   3,   17,  356,    8, 9328, 7452,  772,   33,  489, 4271,   11,  489,\n",
       "          5553, 6898,  175, 2302,   33,   30,  260,   42, 1146,    1]),\n",
       "  'input_text': 't set the execution\\naccuracy results are 793 and 751 respectively\\nthese numbers are on par or higher',\n",
       "  'expected_output': ' than those of\\nthe closest competitor lgesql  elec'},\n",
       " {'input_ids': tensor([11696,  6898,   175,  2302,    33,    30,   260,    42,  1146,   145,\n",
       "            273,    13,     8, 12257, 18766,     3,    40,  2897,  1824,    40,\n",
       "              3,   400,    75,     1]),\n",
       "  'input_text': '51 respectively\\nthese numbers are on par or higher than those of\\nthe closest competitor lgesql  elec',\n",
       "  'expected_output': 'tra\\ncao et al 2021 see table 1 furthermore we\\nachi'},\n",
       " {'input_ids': tensor([  145,   273,    13,     8, 12257, 18766,     3,    40,  2897,  1824,\n",
       "             40, 11924,    52,     9,   212,    32,     3,    15,    17,   491,\n",
       "            460,  2658,   217,   953,   209,   856,  3706,    62,     3, 11015,\n",
       "              1]),\n",
       "  'input_text': ' than those of\\nthe closest competitor lgesql  electra\\ncao et al 2021 see table 1 furthermore we\\nachi',\n",
       "  'expected_output': 'eve a testsuite execution accuracy of 719\\non spide'},\n",
       " {'input_ids': tensor([   3, 1313,  212,   32,    3,   15,   17,  491,  460, 2658,  217,  953,\n",
       "           209,  856, 3706,   62, 1984,    3,    9, 3830, 8431, 9328, 7452,   13,\n",
       "           489, 2294,   30,    3, 7675,  221,    1]),\n",
       "  'input_text': 'tra\\ncao et al 2021 see table 1 furthermore we\\nachieve a testsuite execution accuracy of 719\\non spide',\n",
       "  'expected_output': 'rs development set\\nour ﬁndings on the cosql dialog'},\n",
       " {'input_ids': tensor([    3,    15,   162,     3,     9,  3830,  8431,  9328,  7452,    13,\n",
       "            489,  2294,    30, 18612,     7,   606,   356,    69,  7469,    30,\n",
       "              8,   576,     7,  1824,    40, 13463,     1]),\n",
       "  'input_text': 'eve a testsuite execution accuracy of 719\\non spiders development set\\nour ﬁndings on the cosql dialog',\n",
       "  'expected_output': ' state tracking\\ndataset see table 2 are similar to'},\n",
       " {'input_ids': tensor([    3,    52,     7,   606,   356,    69,  7469,    30,     8,   576,\n",
       "              7,  1824,    40, 13463,   538,  6418, 17953,   217,   953,   204,\n",
       "             33,  1126,    12,     1]),\n",
       "  'input_text': 'rs development set\\nour ﬁndings on the cosql dialog state tracking\\ndataset see table 2 are similar to',\n",
       "  'expected_output': ' those for spider\\npicard signiﬁcantly improves the'},\n",
       " {'input_ids': tensor([  538,  6418, 17953,   217,   953,   204,    33,  1126,    12,   273,\n",
       "             21, 18612,  6686,   986,  4019,  1172,     7,     8,     1]),\n",
       "  'input_text': ' state tracking\\ndataset see table 2 are similar to those for spider\\npicard signiﬁcantly improves the',\n",
       "  'expected_output': ' performance\\nand our ﬁnetuned t53b model achieves '},\n",
       " {'input_ids': tensor([  273,    21, 18612,  6686,   986,  4019,  1172,     7,     8,   821,\n",
       "             11,    69,  1399,    17,   444,    26,     3,    17,  4867,   115,\n",
       "            825,  1984,     7,     1]),\n",
       "  'input_text': ' those for spider\\npicard signiﬁcantly improves the performance\\nand our ﬁnetuned t53b model achieves ',\n",
       "  'expected_output': 'stateof\\ntheart performance\\npicard is not only impr'},\n",
       " {'input_ids': tensor([ 821,   11,   69, 1399,   17,  444,   26,    3,   17, 4867,  115,  825,\n",
       "          1984,    7,  538,  858,    8, 1408,  821, 6686,  986,   19,   59,  163,\n",
       "          4840,   52,    1]),\n",
       "  'input_text': ' performance\\nand our ﬁnetuned t53b model achieves stateof\\ntheart performance\\npicard is not only impr',\n",
       "  'expected_output': 'oving performance it\\nis also fast during evaluatio'},\n",
       " {'input_ids': tensor([ 538,  858,    8, 1408,  821, 6686,  986,   19,   59,  163, 4863,  821,\n",
       "            34,   19,   92, 1006,  383,    3,   15, 7480,  144,   23,   32,    1]),\n",
       "  'input_text': 'stateof\\ntheart performance\\npicard is not only improving performance it\\nis also fast during evaluatio',\n",
       "  'expected_output': 'n of the t53b model\\non spider the decoding speed w'},\n",
       " {'input_ids': tensor([    3,    32,  3745,   821,    34,    19,    92,  1006,   383,  5002,\n",
       "             13,     8,     3,    17,  4867,   115,   825,    30, 18612,     8,\n",
       "             20,  9886,  1634,     3,   210,     1]),\n",
       "  'input_text': 'oving performance it\\nis also fast during evaluation of the t53b model\\non spider the decoding speed w',\n",
       "  'expected_output': 'ith beam size 4\\non an nvidia a100sxm440gb gpu was '},\n",
       " {'input_ids': tensor([    3,    29,    13,     8,     3,    17,  4867,   115,   825,    30,\n",
       "          18612,     8,    20,  9886,  1634,    28, 11638,   812,   314,    30,\n",
       "             46,     3,    29,   208, 24594,     3,     9,  2915,     7,   226,\n",
       "             51, 22335,   122,   115,     3,   122,  4987,    47,     1]),\n",
       "  'input_text': 'n of the t53b model\\non spider the decoding speed with beam size 4\\non an nvidia a100sxm440gb gpu was ',\n",
       "  'expected_output': 'on\\naverage 25 seconds per sample without picard\\nan'},\n",
       " {'input_ids': tensor([   34,   107, 11638,   812,   314,    30,    46,     3,    29,   208,\n",
       "          24594,     3,     9,  2915,     7,   226,    51, 22335,   122,   115,\n",
       "              3,   122,  4987,    47,    30,  1348,   944,  3978,   399,  3106,\n",
       "            406,  6686,   986,    46,     1]),\n",
       "  'input_text': 'ith beam size 4\\non an nvidia a100sxm440gb gpu was on\\naverage 25 seconds per sample without picard\\nan',\n",
       "  'expected_output': 'd 31 seconds per sample with p icard \\nbeam size fi'},\n",
       " {'input_ids': tensor([   30,  1348,   944,  3978,   399,  3106,   406,  6686,   986,    11,\n",
       "           2664,  3978,   399,  3106,    28,     3,   102,     3,    23,  6043,\n",
       "          11638,   812,   361,     1]),\n",
       "  'input_text': 'on\\naverage 25 seconds per sample without picard\\nand 31 seconds per sample with p icard \\nbeam size fi',\n",
       "  'expected_output': 'gure 1 shows results on spider with\\nout and with p'},\n",
       " {'input_ids': tensor([    3,    26,  2664,  3978,   399,  3106,    28,     3,   102,     3,\n",
       "             23,  6043, 11638,   812,  2320,   209,  1267,   772,    30, 18612,\n",
       "             28,    91,    11,    28,     3,   102,     1]),\n",
       "  'input_text': 'd 31 seconds per sample with p icard \\nbeam size figure 1 shows results on spider with\\nout and with p',\n",
       "  'expected_output': 'icard when parsing with guards\\n1 2 4 8\\nbeam size\\n0'},\n",
       " {'input_ids': tensor([    3,  7840,    15,   209,  1267,   772,    30, 18612,    28,    91,\n",
       "             11,    28,  6686,   986,   116,   260,     7,    53,    28,  4879,\n",
       "              7,   209,   204,   314,   505, 11638,   812,     3,   632,     1]),\n",
       "  'input_text': 'gure 1 shows results on spider with\\nout and with picard when parsing with guards\\n1 2 4 8\\nbeam size\\n0',\n",
       "  'expected_output': '60062064066068070\\nexact match accuracy\\nturned off\\n'},\n",
       " {'input_ids': tensor([    3,    23,  6043,   116,   260,     7,    53,    28,  4879,     7,\n",
       "            209,   204,   314,   505, 11638,   812, 13574,  1206, 26898, 23714,\n",
       "           3539,  5176,  2079,  2518,  2883,  1588,  7452,  2120,   326,     1]),\n",
       "  'input_text': 'icard when parsing with guards\\n1 2 4 8\\nbeam size\\n060062064066068070\\nexact match accuracy\\nturned off\\n',\n",
       "  'expected_output': 'lexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁna'},\n",
       " {'input_ids': tensor([ 7366, 26898, 23714,  3539,  5176,  2079,  2518,  2883,  1588,  7452,\n",
       "           2120,   326,    90,   226,    53,   260,     7,    53,  2275,  4879,\n",
       "              7,   260,     7,    53,     3,   210,  4879,     7,  5839,  2202,\n",
       "              9,     1]),\n",
       "  'input_text': '60062064066068070\\nexact match accuracy\\nturned off\\nlexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁna',\n",
       "  'expected_output': 'lizing\\nincremental\\nfigure 3 exactsetmatch accuracy'},\n",
       " {'input_ids': tensor([   90,   226,    53,   260,     7,    53,  2275,  4879,     7,   260,\n",
       "              7,    53,     3,   210,  4879,     7,  5839,   804,  2610, 28351,\n",
       "           2320,   220,  2883,  2244, 19515,  7452,     1]),\n",
       "  'input_text': 'lexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁnalizing\\nincremental\\nfigure 3 exactsetmatch accuracy',\n",
       "  'expected_output': ' on the spider de\\nvelopment set as a function of b'},\n",
       " {'input_ids': tensor([    3,    40,  2610, 28351,  2320,   220,  2883,  2244, 19515,  7452,\n",
       "             30,     8, 18612,    20,     3,   162,  8745,   297,   356,    38,\n",
       "              3,     9,  1681,    13,     3,   115,     1]),\n",
       "  'input_text': 'lizing\\nincremental\\nfigure 3 exactsetmatch accuracy on the spider de\\nvelopment set as a function of b',\n",
       "  'expected_output': 'eam size for top 4\\npicard on t5large schema only a'},\n",
       " {'input_ids': tensor([   30,     8, 18612,    20,     3,   162,  8745,   297,   356,    38,\n",
       "              3,     9,  1681,    13, 11638,   812,    21,   420,   314,  6686,\n",
       "            986,    30,     3,    17,   755, 15599, 26622,   163,     3,     9,\n",
       "              1]),\n",
       "  'input_text': ' on the spider de\\nvelopment set as a function of beam size for top 4\\npicard on t5large schema only a',\n",
       "  'expected_output': 'nd for different\\noperation modes turned off lexing'},\n",
       " {'input_ids': tensor([    3,    15,   265,   812,    21,   420,   314,  6686,   986,    30,\n",
       "              3,    17,   755, 15599, 26622,   163,    11,    21,   315,  2986,\n",
       "          12632,  2120,   326,    90,   226,    53,     1]),\n",
       "  'input_text': 'eam size for top 4\\npicard on t5large schema only and for different\\noperation modes turned off lexing',\n",
       "  'expected_output': ' parsing without\\nguards and parsing with guards in'},\n",
       " {'input_ids': tensor([    3,   727,    21,   315,  2986, 12632,  2120,   326,    90,   226,\n",
       "             53,   260,     7,    53,   406,  4879,     7,    11,   260,     7,\n",
       "             53,    28,  4879,     7,    16,     1]),\n",
       "  'input_text': 'nd for different\\noperation modes turned off lexing parsing without\\nguards and parsing with guards in',\n",
       "  'expected_output': ' each mode p i\\ncard is either used incrementally a'},\n",
       " {'input_ids': tensor([  260,     7,    53,   406,  4879,     7,    11,   260,     7,    53,\n",
       "             28,  4879,     7,    16,   284,  2175,     3,   102,     3,    23,\n",
       "            895,    19,   893,   261, 28351,   120,     3,     9,     1]),\n",
       "  'input_text': ' parsing without\\nguards and parsing with guards in each mode p i\\ncard is either used incrementally a',\n",
       "  'expected_output': 't each step or only\\nwhen ﬁnalizing a hypothesis\\nfo'},\n",
       " {'input_ids': tensor([  284,  2175,     3,   102,     3,    23,   895,    19,   893,   261,\n",
       "          28351,   120,    44,   284,  1147,    42,   163,   116,   804,  2610,\n",
       "              3,     9, 22455,  5575,     1]),\n",
       "  'input_text': ' each mode p i\\ncard is either used incrementally at each step or only\\nwhen ﬁnalizing a hypothesis\\nfo',\n",
       "  'expected_output': 'r different beam sizes and sizes of t5 for\\neach mo'},\n",
       " {'input_ids': tensor([    3,    17,   284,  1147,    42,   163,   116,   804,  2610,     3,\n",
       "              9, 22455,    21,   315, 11638,  4342,    11,  4342,    13,     3,\n",
       "             17,   755,    21,   284,  2288,     1]),\n",
       "  'input_text': 't each step or only\\nwhen ﬁnalizing a hypothesis\\nfor different beam sizes and sizes of t5 for\\neach mo',\n",
       "  'expected_output': 'del size picard increases performance\\nwith increas'},\n",
       " {'input_ids': tensor([    3,    52,   315, 11638,  4342,    11,  4342,    13,     3,    17,\n",
       "            755,    21,   284,   825,   812,  6686,   986,  5386,   821,    28,\n",
       "             16,  5045,     9,     7,     1]),\n",
       "  'input_text': 'r different beam sizes and sizes of t5 for\\neach model size picard increases performance\\nwith increas',\n",
       "  'expected_output': 'ing beam size these increases are\\nthe strongest fo'},\n",
       " {'input_ids': tensor([   20,    40,   812,  6686,   986,  5386,   821,    28,  3094, 11638,\n",
       "            812,   175,  5386,    33,     8, 19095,  5575,     1]),\n",
       "  'input_text': 'del size picard increases performance\\nwith increasing beam size these increases are\\nthe strongest fo',\n",
       "  'expected_output': 'r the step from beam size 1to2\\nless pronounced fro'},\n",
       " {'input_ids': tensor([    3,    53, 11638,   812,   175,  5386,    33,     8, 19095,    21,\n",
       "              8,  1147,    45, 11638,   812,   209,   235,   357,   705,     3,\n",
       "          20721,     3,  6155,     1]),\n",
       "  'input_text': 'ing beam size these increases are\\nthe strongest for the step from beam size 1to2\\nless pronounced fro',\n",
       "  'expected_output': 'm 2to4 and then saturating\\nfor beam sizes above 4 '},\n",
       " {'input_ids': tensor([    3,    52,     8,  1147,    45, 11638,   812,   209,   235,   357,\n",
       "            705,     3, 20721,    45,   204,   235,   591,    11,   258,     3,\n",
       "              7,  6010,  1014,    21, 11638,  4342,   756,   314,     1]),\n",
       "  'input_text': 'r the step from beam size 1to2\\nless pronounced from 2to4 and then saturating\\nfor beam sizes above 4 ',\n",
       "  'expected_output': 'even with greedy search\\nbeam size 1picard allows f'},\n",
       " {'input_ids': tensor([    3,    51,   204,   235,   591,    11,   258,     3,     7,  6010,\n",
       "           1014,    21, 11638,  4342,   756,   314,   237,    28, 30337,    63,\n",
       "            960, 11638,   812,   209,  6174,   986,  1250,     3,    89,     1]),\n",
       "  'input_text': 'm 2to4 and then saturating\\nfor beam sizes above 4 even with greedy search\\nbeam size 1picard allows f',\n",
       "  'expected_output': 'or some modest\\nimprovements note that without pica'},\n",
       " {'input_ids': tensor([  237,    28, 30337,    63,   960, 11638,   812,   209,  6174,   986,\n",
       "           1250,    21,   128, 11306,  6867,  2232,    24,   406,  6686,     9,\n",
       "              1]),\n",
       "  'input_text': 'even with greedy search\\nbeam size 1picard allows for some modest\\nimprovements note that without pica',\n",
       "  'expected_output': 'rd  these\\nmodels do not beneﬁt from beam search th'},\n",
       " {'input_ids': tensor([   42,   128, 11306,  6867,  2232,    24,   406,  6686,   986,   175,\n",
       "           2250,   103,    59,  1656,    45, 11638,   960,     3,   189,     1]),\n",
       "  'input_text': 'or some modest\\nimprovements note that without picard  these\\nmodels do not beneﬁt from beam search th',\n",
       "  'expected_output': 'e num\\nberk of highestprobability tokens that are p'},\n",
       " {'input_ids': tensor([    3,    52,    26,   175,  2250,   103,    59,  1656,    45, 11638,\n",
       "            960,     8,     3,  5525,     3,  1152,   157,    13,  2030,  1409,\n",
       "            115,  2020, 14145,     7,    24,    33,     3,   102,     1]),\n",
       "  'input_text': 'rd  these\\nmodels do not beneﬁt from beam search the num\\nberk of highestprobability tokens that are p',\n",
       "  'expected_output': 'ro\\ncessed by picard at each decoding step has a\\nmo'},\n",
       " {'input_ids': tensor([    3,    15,     3,  5525,     3,  1152,   157,    13,  2030,  1409,\n",
       "            115,  2020, 14145,     7,    24,    33,   813, 27251,    26,    57,\n",
       "           6686,   986,    44,   284,    20,  9886,  1147,    65,     3,     9,\n",
       "           2288,     1]),\n",
       "  'input_text': 'e num\\nberk of highestprobability tokens that are pro\\ncessed by picard at each decoding step has a\\nmo',\n",
       "  'expected_output': 'dest to negligible impact on performance it is\\nthe'},\n",
       " {'input_ids': tensor([    3,    52,    32, 27251,    26,    57,  6686,   986,    44,   284,\n",
       "             20,  9886,  1147,    65,     3,     9, 11306,    12, 14261,  2825,\n",
       "           2317,  1113,    30,   821,    34,    19,     8,     1]),\n",
       "  'input_text': 'ro\\ncessed by picard at each decoding step has a\\nmodest to negligible impact on performance it is\\nthe',\n",
       "  'expected_output': ' largest for t5base smaller for t5large and\\nalmost'},\n",
       " {'input_ids': tensor([   93,    17,    12, 14261,  2825,  2317,  1113,    30,   821,    34,\n",
       "             19,     8,  2015,    21,     3,    17,   755, 10925,  2755,    21,\n",
       "              3,    17,   755, 15599,    11,   966,     1]),\n",
       "  'input_text': 'dest to negligible impact on performance it is\\nthe largest for t5base smaller for t5large and\\nalmost',\n",
       "  'expected_output': ' undetectable for t53b we do not study\\nthe case k '},\n",
       " {'input_ids': tensor([ 2015,    21,     3,    17,   755, 10925,  2755,    21,     3,    17,\n",
       "            755, 15599,    11,   966,  3550,  5822,  3869,    21,     3,    17,\n",
       "           4867,   115,    62,   103,    59,   810,     8,   495,     3,   157,\n",
       "              1]),\n",
       "  'input_text': ' largest for t5base smaller for t5large and\\nalmost undetectable for t53b we do not study\\nthe case k ',\n",
       "  'expected_output': '1 because it reduces the beam search\\nto constraine'},\n",
       " {'input_ids': tensor([ 3550,  5822,  3869,    21,     3,    17,  4867,   115,    62,   103,\n",
       "             59,   810,     8,   495,     3,   157,   209,   250,    34,  1428,\n",
       "              7,     8, 11638,   960,    12, 27354,    15,     1]),\n",
       "  'input_text': ' undetectable for t53b we do not study\\nthe case k 1 because it reduces the beam search\\nto constraine',\n",
       "  'expected_output': 'd greedy search\\nablations in figure 3 we have cond'},\n",
       " {'input_ids': tensor([  209,   250,    34,  1428,     7,     8, 11638,   960,    12,   975,\n",
       "          22418, 30337,    63,   960,     3, 15403,  1628,    16,  2320,   220,\n",
       "             62,    43,   975,    26,     1]),\n",
       "  'input_text': '1 because it reduces the beam search\\nto constrained greedy search\\nablations in figure 3 we have cond',\n",
       "  'expected_output': 'ensed our\\nablation analysis for picard  we show re'},\n",
       " {'input_ids': tensor([    3,    26, 30337,    63,   960,     3, 15403,  1628,    16,  2320,\n",
       "            220,    62,    43,   975,   537,  3843,    69,   703,  6105,  1693,\n",
       "             21,  6686,   986,    62,   504,     3,    60,     1]),\n",
       "  'input_text': 'd greedy search\\nablations in figure 3 we have condensed our\\nablation analysis for picard  we show re',\n",
       "  'expected_output': 'sults for\\nour t5large model in all four picard che'},\n",
       " {'input_ids': tensor([    3,  5167,    26,    69,   703,  6105,  1693,    21,  6686,   986,\n",
       "             62,   504,   772,    21,    69,     3,    17,   755, 15599,   825,\n",
       "             16,    66,   662,  6686,   986,     3,  1033,     1]),\n",
       "  'input_text': 'ensed our\\nablation analysis for picard  we show results for\\nour t5large model in all four picard che',\n",
       "  'expected_output': 'cking\\nmodes and for four different beam sizes on t'},\n",
       " {'input_ids': tensor([    3,     7,    83,    17,     7,    21,    69,     3,    17,   755,\n",
       "          15599,   825,    16,    66,   662,  6686,   986,  6450, 12632,    11,\n",
       "             21,   662,   315, 11638,  4342,    30,     3,    17,     1]),\n",
       "  'input_text': 'sults for\\nour t5large model in all four picard checking\\nmodes and for four different beam sizes on t',\n",
       "  'expected_output': 'he spi\\nder development set when checking increment'},\n",
       " {'input_ids': tensor([    3,  2406,    53, 12632,    11,    21,   662,   315, 11638,  4342,\n",
       "             30,     8,     3,  7675,    74,   606,   356,   116,  6450, 26006,\n",
       "              1]),\n",
       "  'input_text': 'cking\\nmodes and for four different beam sizes on the spi\\nder development set when checking increment',\n",
       "  'expected_output': 'ally\\nat each decoding step lexing shows a small im'},\n",
       " {'input_ids': tensor([    3,    88,     3,  7675,    74,   606,   356,   116,  6450, 28351,\n",
       "            120,    44,   284,    20,  9886,  1147,    90,   226,    53,  1267,\n",
       "              3,     9,   422,   256,     1]),\n",
       "  'input_text': 'he spi\\nder development set when checking incrementally\\nat each decoding step lexing shows a small im',\n",
       "  'expected_output': '\\nprovement over the unconstrained t5 model the\\nres'},\n",
       " {'input_ids': tensor([    3,  1427,    44,   284,    20,  9886,  1147,    90,   226,    53,\n",
       "           1267,     3,     9,   422,   256,  4410,   297,   147,     8,    73,\n",
       "           1018, 22418,     3,    17,   755,   825,     8,     3,    60,     7,\n",
       "              1]),\n",
       "  'input_text': 'ally\\nat each decoding step lexing shows a small im\\nprovement over the unconstrained t5 model the\\nres',\n",
       "  'expected_output': 'ults without picard and with picard in lex\\ning mod'},\n",
       " {'input_ids': tensor([ 4410,   297,   147,     8,    73,  1018, 22418,     3,    17,   755,\n",
       "            825,     8,   772,   406,  6686,   986,    11,    28,  6686,   986,\n",
       "             16,    90,   226,     3,    53,  1794,     1]),\n",
       "  'input_text': '\\nprovement over the unconstrained t5 model the\\nresults without picard and with picard in lex\\ning mod',\n",
       "  'expected_output': 'e are largely independent of the beam size\\nthis is'},\n",
       " {'input_ids': tensor([    3,    83,    17,     7,   406,  6686,   986,    11,    28,  6686,\n",
       "            986,    16,    90,   226,     3,    53,  2175,    33,     3,  6974,\n",
       "           2547,    13,     8, 11638,   812,    48,    19,     1]),\n",
       "  'input_text': 'ults without picard and with picard in lex\\ning mode are largely independent of the beam size\\nthis is',\n",
       "  'expected_output': ' different when picard is switched into\\nthe more s'},\n",
       " {'input_ids': tensor([    3,    15,    33,     3,  6974,  2547,    13,     8, 11638,   812,\n",
       "             48,    19,   315,   116,  6686,   986,    19, 17785,   139,     8,\n",
       "             72,     3,     7,     1]),\n",
       "  'input_text': 'e are largely independent of the beam size\\nthis is different when picard is switched into\\nthe more s',\n",
       "  'expected_output': 'ophisticated parsing modes both with\\nand without g'},\n",
       " {'input_ids': tensor([  315,   116,  6686,   986,    19, 17785,   139,     8,    72,  8732,\n",
       "            260,     7,    53, 12632,   321,    28,    11,   406,     3,   122,\n",
       "              1]),\n",
       "  'input_text': ' different when picard is switched into\\nthe more sophisticated parsing modes both with\\nand without g',\n",
       "  'expected_output': 'uards improvements from picard\\nincrease rapidly fo'},\n",
       " {'input_ids': tensor([    3, 10775,  3040,   920,   260,     7,    53, 12632,   321,    28,\n",
       "             11,   406,  4879,     7,  6867,    45,  6686,   986,   993,  7313,\n",
       "           5575,     1]),\n",
       "  'input_text': 'ophisticated parsing modes both with\\nand without guards improvements from picard\\nincrease rapidly fo',\n",
       "  'expected_output': 'r increasing beam sizes where\\nparsing with guards '},\n",
       " {'input_ids': tensor([    3,    76,   986,     7,  6867,    45,  6686,   986,   993,  7313,\n",
       "             21,  3094, 11638,  4342,   213,   260,     7,    53,    28,  4879,\n",
       "              7,     1]),\n",
       "  'input_text': 'uards improvements from picard\\nincrease rapidly for increasing beam sizes where\\nparsing with guards ',\n",
       "  'expected_output': 'clearly has a strong lead over9899development test'},\n",
       " {'input_ids': tensor([    3,    52,  3094, 11638,  4342,   213,   260,     7,    53,    28,\n",
       "           4879,     7,  3133,    65,     3,     9,  1101,   991,   147,  3916,\n",
       "           3264, 19677,   794,     1]),\n",
       "  'input_text': 'r increasing beam sizes where\\nparsing with guards clearly has a strong lead over9899development test',\n",
       "  'expected_output': '\\nsystem em ex em ex\\nbridge v2  bert ensembleylin e'},\n",
       " {'input_ids': tensor([ 3133,    65,     3,     9,  1101,   991,   147,  3916,  3264, 19677,\n",
       "            794,   358,     3,    15,    51,  1215,     3,    15,    51,  1215,\n",
       "           4716,     3,   208,   357,     3,  7041,  8784,    63,    40,    77,\n",
       "              3,    15,     1]),\n",
       "  'input_text': 'clearly has a strong lead over9899development test\\nsystem em ex em ex\\nbridge v2  bert ensembleylin e',\n",
       "  'expected_output': 't al 2020 711 703 675 683\\nsmbop  g rappayrubin and'},\n",
       " {'input_ids': tensor([  358,     3,    15,    51,  1215,     3,    15,    51,  1215,  4716,\n",
       "              3,   208,   357,     3,  7041,  8784,    63,    40,    77,     3,\n",
       "             15,    17,   491,  6503,   489,  2596,  2861,   519,   431,  3072,\n",
       "            431,  4591,     3,     7,  6310,   102,     3,   122,     3,  5846,\n",
       "           8832, 14446,    77,    11,     1]),\n",
       "  'input_text': '\\nsystem em ex em ex\\nbridge v2  bert ensembleylin et al 2020 711 703 675 683\\nsmbop  g rappayrubin and',\n",
       "  'expected_output': ' berant 2021 747 750 695 711\\nratsql  gapyshi et al'},\n",
       " {'input_ids': tensor([    3,    17,   491,  6503,   489,  2596,  2861,   519,   431,  3072,\n",
       "            431,  4591,     3,     7,  6310,   102,     3,   122,     3,  5846,\n",
       "           8832, 14446,    77,    11,    36,  3569,   460,  2658,   489,  4177,\n",
       "              3,  9979,   431,  3301,   489,  2596, 20063,  1824,    40,  6813,\n",
       "             63,  5605,     3,    15,    17,   491,     1]),\n",
       "  'input_text': 't al 2020 711 703 675 683\\nsmbop  g rappayrubin and berant 2021 747 750 695 711\\nratsql  gapyshi et al',\n",
       "  'expected_output': ' 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al '},\n",
       " {'input_ids': tensor([   36,  3569,   460,  2658,   489,  4177,     3,  9979,   431,  3301,\n",
       "            489,  2596, 20063,  1824,    40,  6813,    63,  5605,     3,    15,\n",
       "             17,   491,   460,  2658,   489,  2606,   431,  4327,     3,    26,\n",
       "             17, 12304,   413, 11820,    40,     7,   102,     3,    52, 18299,\n",
       "             17,     3,     9,    63,   226,    76,     3,    15,    17,   491,\n",
       "              1]),\n",
       "  'input_text': ' berant 2021 747 750 695 711\\nratsql  gapyshi et al 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al ',\n",
       "  'expected_output': '2021 750  709 \\nlgesql  electraycao et al 2021 751 '},\n",
       " {'input_ids': tensor([  460,  2658,   489,  2606,   431,  4327,     3,    26,    17, 12304,\n",
       "            413, 11820,    40,     7,   102,     3,    52, 18299,    17,     3,\n",
       "              9,    63,   226,    76,     3,    15,    17,   491,   460,  2658,\n",
       "              3,  9979,   489,  4198,     3,    40,  2897,  1824,    40, 11924,\n",
       "           2866,   658,    32,     3,    15,    17,   491,   460,  2658,   489,\n",
       "           5553,     1]),\n",
       "  'input_text': ' 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al 2021 750  709 \\nlgesql  electraycao et al 2021 751 ',\n",
       "  'expected_output': ' 720 \\nt5base shaw et al 2021 571   \\nt53b shaw et a'},\n",
       " {'input_ids': tensor([  460,  2658,     3,  9979,   489,  4198,     3,    40,  2897,  1824,\n",
       "             40, 11924,  2866,   658,    32,     3,    15,    17,   491,   460,\n",
       "           2658,   489,  5553,     3, 18517,     3,    17,   755, 10925,     3,\n",
       "          15622,     3,    15,    17,   491,   460,  2658,   305,  4450,     3,\n",
       "             17,  4867,   115,     3, 15622,     3,    15,    17,     3,     9,\n",
       "              1]),\n",
       "  'input_text': '2021 750  709 \\nlgesql  electraycao et al 2021 751  720 \\nt5base shaw et al 2021 571   \\nt53b shaw et a',\n",
       "  'expected_output': 'l 2021 700   \\nt5base ours 572 579  \\nt5basep icard '},\n",
       " {'input_ids': tensor([    3, 18517,     3,    17,   755, 10925,     3, 15622,     3,    15,\n",
       "             17,   491,   460,  2658,   305,  4450,     3,    17,  4867,   115,\n",
       "              3, 15622,     3,    15,    17,   491,   460,  2658, 12283,     3,\n",
       "             17,   755, 10925,    69,     7,   305,  5865,   305,  4440,     3,\n",
       "             17,   755, 10925,   102,     3,    23,  6043,     1]),\n",
       "  'input_text': ' 720 \\nt5base shaw et al 2021 571   \\nt53b shaw et al 2021 700   \\nt5base ours 572 579  \\nt5basep icard ',\n",
       "  'expected_output': '658 684  \\nt5large 653 672  \\nt5largep icard 691 729'},\n",
       " {'input_ids': tensor([    3,    40,   460,  2658, 12283,     3,    17,   755, 10925,    69,\n",
       "              7,   305,  5865,   305,  4440,     3,    17,   755, 10925,   102,\n",
       "              3,    23,  6043,   431,  3449,   431,  4608,     3,    17,   755,\n",
       "          15599,   431,  4867,   431,  5865,     3,    17,   755, 15599,   102,\n",
       "              3,    23,  6043,   431,  4729,   489,  3166,     1]),\n",
       "  'input_text': 'l 2021 700   \\nt5base ours 572 579  \\nt5basep icard 658 684  \\nt5large 653 672  \\nt5largep icard 691 729',\n",
       "  'expected_output': '  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by'},\n",
       " {'input_ids': tensor([  431,  3449,   431,  4608,     3,    17,   755, 15599,   431,  4867,\n",
       "            431,  5865,     3,    17,   755, 15599,   102,     3,    23,  6043,\n",
       "            431,  4729,   489,  3166,     3,    17,  4867,   115,    69,     7,\n",
       "            431,  3264,   489,  2534,     3,    17,  4867,   115,   102,     3,\n",
       "             23,  6043,   489,  4853,   489,  3891,     3,    17,  4867,   969,\n",
       "              1]),\n",
       "  'input_text': '658 684  \\nt5large 653 672  \\nt5largep icard 691 729  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by',\n",
       "  'expected_output': '715 744 680 701\\nt53bp icardy755 793 719 751\\ntable '},\n",
       " {'input_ids': tensor([   3,   17, 4867,  115,   69,    7,  431, 3264,  489, 2534,    3,   17,\n",
       "          4867,  115,  102,    3,   23, 6043,  489, 4853,  489, 3891,    3,   17,\n",
       "          4867,  969,  940, 1808,  489, 3628,  431, 2079, 2861,  536,    3,   17,\n",
       "          4867,  115,  102,    3,   23, 6043,   63, 3072,  755,  489, 4271,  489,\n",
       "          2294,  489, 5553,  953,    1]),\n",
       "  'input_text': '  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by715 744 680 701\\nt53bp icardy755 793 719 751\\ntable ',\n",
       "  'expected_output': '1 our results bottom and relevant prior art top on'},\n",
       " {'input_ids': tensor([ 489, 1808,  489, 3628,  431, 2079, 2861,  536,    3,   17, 4867,  115,\n",
       "           102,    3,   23, 6043,   63, 3072,  755,  489, 4271,  489, 2294,  489,\n",
       "          5553,  953,  209,   69,  772, 2007,   11, 2193, 1884,  768,  420,   30,\n",
       "             1]),\n",
       "  'input_text': '715 744 680 701\\nt53bp icardy755 793 719 751\\ntable 1 our results bottom and relevant prior art top on',\n",
       "  'expected_output': ' the spider texttosql task shown are the exactset\\n'},\n",
       " {'input_ids': tensor([  209,    69,   772,  2007,    11,  2193,  1884,   768,   420,    30,\n",
       "              8, 18612,  1499,   235,     7,  1824,    40,  2491,  2008,    33,\n",
       "              8,  2883,  2244,     1]),\n",
       "  'input_text': '1 our results bottom and relevant prior art top on the spider texttosql task shown are the exactset\\n',\n",
       "  'expected_output': 'match accuracy em and execution accuracy ex percen'},\n",
       " {'input_ids': tensor([    8, 18612,  1499,   235,     7,  1824,    40,  2491,  2008,    33,\n",
       "              8,  2883,  2244,  1588,  7452,     3,    15,    51,    11,  9328,\n",
       "           7452,  1215,   399,    75,    35,     1]),\n",
       "  'input_text': ' the spider texttosql task shown are the exactset\\nmatch accuracy em and execution accuracy ex percen',\n",
       "  'expected_output': 'tages on spiders development and test sets our res'},\n",
       " {'input_ids': tensor([ 1588,  7452,     3,    15,    51,    11,  9328,  7452,  1215,  5294,\n",
       "              7,    30, 18612,     7,   606,    11,   794,  3369,    69,     3,\n",
       "             60,     7,     1]),\n",
       "  'input_text': 'match accuracy em and execution accuracy ex percentages on spiders development and test sets our res',\n",
       "  'expected_output': 'ults\\nare for a beam of size 4 and p icard is parsi'},\n",
       " {'input_ids': tensor([    3,  6505,     7,    30, 18612,     7,   606,    11,   794,  3369,\n",
       "             69,   772,    33,    21,     3,     9, 11638,    13,   812,   314,\n",
       "             11,     3,   102,     3,    23,  6043,    19,   260,     7,    23,\n",
       "              1]),\n",
       "  'input_text': 'tages on spiders development and test sets our results\\nare for a beam of size 4 and p icard is parsi',\n",
       "  'expected_output': 'ng with guards for the top2 token predictions a da'},\n",
       " {'input_ids': tensor([    3,    83,    17,     7,    33,    21,     3,     9, 11638,    13,\n",
       "            812,   314,    11,     3,   102,     3,    23,  6043,    19,   260,\n",
       "              7,    53,    28,  4879,     7,    21,     8,   420,   357, 14145,\n",
       "          20099,     3,     9,   836,     1]),\n",
       "  'input_text': 'ults\\nare for a beam of size 4 and p icard is parsing with guards for the top2 token predictions a da',\n",
       "  'expected_output': 'gger  indicates\\nuse of database content otherwise '},\n",
       " {'input_ids': tensor([    3,  1725,    28,  4879,     7,    21,     8,   420,   357, 14145,\n",
       "          20099,     3,     9,   836,  6938,  9379,   169,    13,  3501,   738,\n",
       "           2904,     1]),\n",
       "  'input_text': 'ng with guards for the top2 token predictions a dagger  indicates\\nuse of database content otherwise ',\n",
       "  'expected_output': 'schema only\\ndevelopment test\\nsystem qm im qm im\\nra'},\n",
       " {'input_ids': tensor([    3,  6938,  9379,   169,    13,  3501,   738,  2904, 26622,   163,\n",
       "            606,   794,   358,     3,  1824,    51,   256,     3,  1824,    51,\n",
       "            256,     3,    52,     9,     1]),\n",
       "  'input_text': 'gger  indicates\\nuse of database content otherwise schema only\\ndevelopment test\\nsystem qm im qm im\\nra',\n",
       "  'expected_output': 'tsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538'},\n",
       " {'input_ids': tensor([26622,   163,   606,   794,   358,     3,  1824,    51,   256,     3,\n",
       "           1824,    51,   256, 20063,  1824,    40,     3,     7,    75,    42,\n",
       "             15,    63,    76,     3,    15,    17,   491,   460,  2658,   305,\n",
       "           2658,   204,  1755,   305,  2938,     3, 24837,     3,    17,  4867,\n",
       "            115,   305,  3747,     1]),\n",
       "  'input_text': 'schema only\\ndevelopment test\\nsystem qm im qm im\\nratsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538',\n",
       "  'expected_output': ' 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 o'},\n",
       " {'input_ids': tensor([    3,    17,     7,  1824,    40,     3,     7,    75,    42,    15,\n",
       "             63,    76,     3,    15,    17,   491,   460,  2658,   305,  2658,\n",
       "            204,  1755,   305,  2938,     3, 24837,     3,    17,  4867,   115,\n",
       "            305,  3747,   204,  2606,   305,  2534,   204,  2517,     3,    17,\n",
       "           4867,   115,   102,     3,    23,  6043,   305,  3951,   997,   357,\n",
       "            305,  4448,   204,  4118,   953,   204,     3,    32,     1]),\n",
       "  'input_text': 'tsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 o',\n",
       "  'expected_output': 'ur results bottom and relevant prior art top on th'},\n",
       " {'input_ids': tensor([ 204, 2606,  305, 2534,  204, 2517,    3,   17, 4867,  115,  102,    3,\n",
       "            23, 6043,  305, 3951,  997,  357,  305, 4448,  204, 4118,  953,  204,\n",
       "            69,  772, 2007,   11, 2193, 1884,  768,  420,   30,    3,  189,    1]),\n",
       "  'input_text': ' 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 our results bottom and relevant prior art top on th',\n",
       "  'expected_output': 'e cosql dialog state tracking task shown are the\\nq'},\n",
       " {'input_ids': tensor([    3,   450,   772,  2007,    11,  2193,  1884,   768,   420,    30,\n",
       "              8,   576,     7,  1824,    40, 13463,   538,  6418,  2491,  2008,\n",
       "             33,     8,     3,  1824,     1]),\n",
       "  'input_text': 'ur results bottom and relevant prior art top on the cosql dialog state tracking task shown are the\\nq',\n",
       "  'expected_output': 'uestion match accuracy qm and interaction match ac'},\n",
       " {'input_ids': tensor([    3,    15,   576,     7,  1824,    40, 13463,   538,  6418,  2491,\n",
       "           2008,    33,     8,   822,  1588,  7452,     3,  1824,    51,    11,\n",
       "           6565,  1588,     3,     9,    75,     1]),\n",
       "  'input_text': 'e cosql dialog state tracking task shown are the\\nquestion match accuracy qm and interaction match ac',\n",
       "  'expected_output': 'curacy im percentages on cosqls development and\\nte'},\n",
       " {'input_ids': tensor([   3,   76, 3340,  106, 1588, 7452,    3, 1824,   51,   11, 6565, 1588,\n",
       "          7452,  256, 5294,    7,   30,  576,    7, 1824,   40,    7,  606,   11,\n",
       "             3,   17,   15,    1]),\n",
       "  'input_text': 'uestion match accuracy qm and interaction match accuracy im percentages on cosqls development and\\nte',\n",
       "  'expected_output': 'st sets our results are for a beam of size 4 and p'},\n",
       " {'input_ids': tensor([ 5495,  4710,   256,  5294,     7,    30,   576,     7,  1824,    40,\n",
       "              7,   606,    11,   794,  3369,    69,   772,    33,    21,     3,\n",
       "              9, 11638,    13,   812,   314,    11,     3,   102,     1]),\n",
       "  'input_text': 'curacy im percentages on cosqls development and\\ntest sets our results are for a beam of size 4 and p',\n",
       "  'expected_output': ' icard is parsing with guards for the top2 token p'},\n",
       " {'input_ids': tensor([    3,     7,    17,  3369,    69,   772,    33,    21,     3,     9,\n",
       "          11638,    13,   812,   314,    11,     3,   102,     3,    23,  6043,\n",
       "             19,   260,     7,    53,    28,  4879,     7,    21,     8,   420,\n",
       "            357, 14145,     3,   102,     1]),\n",
       "  'input_text': 'st sets our results are for a beam of size 4 and p icard is parsing with guards for the top2 token p',\n",
       "  'expected_output': 'redictions\\nparsing without them\\nin order to compar'},\n",
       " {'input_ids': tensor([    3,    23,  6043,    19,   260,     7,    53,    28,  4879,     7,\n",
       "             21,     8,   420,   357, 14145, 20099,   260,     7,    53,   406,\n",
       "            135,    16,   455,    12, 12608,     1]),\n",
       "  'input_text': ' icard is parsing with guards for the top2 token predictions\\nparsing without them\\nin order to compar',\n",
       "  'expected_output': 'e picard with the ﬁltering\\nbyvalidity approach of '},\n",
       " {'input_ids': tensor([    3,    60, 12472,     7,   260,     7,    53,   406,   135,    16,\n",
       "            455,    12,  4048,  6686,   986,    28,     8,  4191,    53,    57,\n",
       "          27769,   485,  1295,    13,     1]),\n",
       "  'input_text': 'redictions\\nparsing without them\\nin order to compare picard with the ﬁltering\\nbyvalidity approach of ',\n",
       "  'expected_output': 'suhr et al 2020 and lin\\net al 2020 we have studied'},\n",
       " {'input_ids': tensor([    3,    15,  6686,   986,    28,     8,  4191,    53,    57, 27769,\n",
       "            485,  1295,    13,     3,     7, 19290,     3,    15,    17,   491,\n",
       "           6503,    11,     3,    40,    77,     3,    15,    17,   491,  6503,\n",
       "             62,    43,  7463,     1]),\n",
       "  'input_text': 'e picard with the ﬁltering\\nbyvalidity approach of suhr et al 2020 and lin\\net al 2020 we have studied',\n",
       "  'expected_output': ' also what happens\\nwhen picard is only checking hy'},\n",
       " {'input_ids': tensor([    3,     7, 19290,     3,    15,    17,   491,  6503,    11,     3,\n",
       "             40,    77,     3,    15,    17,   491,  6503,    62,    43,  7463,\n",
       "             92,   125,  2906,   116,  6686,   986,    19,   163,  6450,     3,\n",
       "            107,    63,     1]),\n",
       "  'input_text': 'suhr et al 2020 and lin\\net al 2020 we have studied also what happens\\nwhen picard is only checking hy',\n",
       "  'expected_output': 'potheses when\\nthe model predicts their ﬁnalization'},\n",
       " {'input_ids': tensor([   92,   125,  2906,   116,  6686,   986,    19,   163,  6450, 10950,\n",
       "          19712,     7,   116,     8,   825,  9689,     7,    70,   804,  1707,\n",
       "              1]),\n",
       "  'input_text': ' also what happens\\nwhen picard is only checking hypotheses when\\nthe model predicts their ﬁnalization',\n",
       "  'expected_output': ' with the endof\\nsequence token2in this restrained '},\n",
       " {'input_ids': tensor([ 1977, 19712,     7,   116,     8,   825,  9689,     7,    70,   804,\n",
       "           1707,    28,     8,   414,   858,  5932, 14145,   357,    77,    48,\n",
       "            880, 10761,     1]),\n",
       "  'input_text': 'potheses when\\nthe model predicts their ﬁnalization with the endof\\nsequence token2in this restrained ',\n",
       "  'expected_output': 'mode picard\\nis still effective but much less so co'},\n",
       " {'input_ids': tensor([   28,     8,   414,   858,  5932, 14145,   357,    77,    48,   880,\n",
       "          10761,  2175,  6686,   986,    19,   341,  1231,    68,   231,   705,\n",
       "             78,   576,     1]),\n",
       "  'input_text': ' with the endof\\nsequence token2in this restrained mode picard\\nis still effective but much less so co',\n",
       "  'expected_output': 'mpared to\\nnormal incremental operation the gap bet'},\n",
       " {'input_ids': tensor([ 2175,  6686,   986,    19,   341,  1231,    68,   231,   705,    78,\n",
       "              3,  2172,    12,  1389, 28351,  2986,     8,  6813,    36,    17,\n",
       "              1]),\n",
       "  'input_text': 'mode picard\\nis still effective but much less so compared to\\nnormal incremental operation the gap bet',\n",
       "  'expected_output': 'ween\\nthese two modes of operation only begins to s'},\n",
       " {'input_ids': tensor([    3,  1167,     9,  1271,    12,  1389, 28351,  2986,     8,  6813,\n",
       "            344,   175,   192, 12632,    13,  2986,   163,  4396,    12,     3,\n",
       "              7,     1]),\n",
       "  'input_text': 'mpared to\\nnormal incremental operation the gap between\\nthese two modes of operation only begins to s',\n",
       "  'expected_output': 'hrink\\nfor large beam sizes this is understandable '},\n",
       " {'input_ids': tensor([   62,    35,   175,   192, 12632,    13,  2986,   163,  4396,    12,\n",
       "          18508,    21,   508, 11638,  4342,    48,    19,   734,   179,     1]),\n",
       "  'input_text': 'ween\\nthese two modes of operation only begins to shrink\\nfor large beam sizes this is understandable ',\n",
       "  'expected_output': 'since\\nlin et al 2020 used beam sizes of at least 1'},\n",
       " {'input_ids': tensor([    3,   107, 13419,    21,   508, 11638,  4342,    48,    19,   734,\n",
       "            179,   437,     3,    40,    77,     3,    15,    17,   491,  6503,\n",
       "            261, 11638,  4342,    13,    44,   709,   209,     1]),\n",
       "  'input_text': 'hrink\\nfor large beam sizes this is understandable since\\nlin et al 2020 used beam sizes of at least 1',\n",
       "  'expected_output': '6\\nand up to 64to reach optimal results with ﬁlteri'},\n",
       " {'input_ids': tensor([  437,     3,    40,    77,     3,    15,    17,   491,  6503,   261,\n",
       "          11638,  4342,    13,    44,   709,   898,    11,    95,    12,  6687,\n",
       "            235,  1535,  6624,   772,    28,  4191,    23,     1]),\n",
       "  'input_text': 'since\\nlin et al 2020 used beam sizes of at least 16\\nand up to 64to reach optimal results with ﬁlteri',\n",
       "  'expected_output': 'ng\\nwhile suhr et al 2020 used a beam of size 100\\n4'},\n",
       " {'input_ids': tensor([  431,    11,    95,    12,  6687,   235,  1535,  6624,   772,    28,\n",
       "           4191,    53,   298,     3,     7, 19290,     3,    15,    17,   491,\n",
       "           6503,   261,     3,     9, 11638,    13,   812,   910,   314,     1]),\n",
       "  'input_text': '6\\nand up to 64to reach optimal results with ﬁltering\\nwhile suhr et al 2020 used a beam of size 100\\n4',\n",
       "  'expected_output': ' conclusion\\nwe propose and evaluate a new method p'},\n",
       " {'input_ids': tensor([    3,  1725,   298,     3,     7, 19290,     3,    15,    17,   491,\n",
       "           6503,   261,     3,     9, 11638,    13,   812,   910,   314,  7489,\n",
       "             62,  4230,    11,  6825,     3,     9,   126,  1573,     3,   102,\n",
       "              1]),\n",
       "  'input_text': 'ng\\nwhile suhr et al 2020 used a beam of size 100\\n4 conclusion\\nwe propose and evaluate a new method p',\n",
       "  'expected_output': 'icard \\nfor simple and effective constrained decodi'},\n",
       " {'input_ids': tensor([ 7489,    62,  4230,    11,  6825,     3,     9,   126,  1573,  6686,\n",
       "            986,    21,   650,    11,  1231,   975, 22418,    20,   509,    26,\n",
       "             23,     1]),\n",
       "  'input_text': ' conclusion\\nwe propose and evaluate a new method picard \\nfor simple and effective constrained decodi',\n",
       "  'expected_output': 'ng with\\n2this is not exactly equivalent to ﬁlterin'},\n",
       " {'input_ids': tensor([    3,    23,  6043,    21,   650,    11,  1231,   975, 22418,    20,\n",
       "           9886,    28,   204,  8048,    19,    59,  1776,  7072,    12,  4191,\n",
       "             77,     1]),\n",
       "  'input_text': 'icard \\nfor simple and effective constrained decoding with\\n2this is not exactly equivalent to ﬁlterin',\n",
       "  'expected_output': 'g a completely\\nﬁnalized beam because the hypothese'},\n",
       " {'input_ids': tensor([    3,  1725,    28,   204,  8048,    19,    59,  1776,  7072,    12,\n",
       "           4191,    53,     3,     9,  1551,   804,  1601, 11638,   250,     8,\n",
       "          10950, 19712,     1]),\n",
       "  'input_text': 'ng with\\n2this is not exactly equivalent to ﬁltering a completely\\nﬁnalized beam because the hypothese',\n",
       "  'expected_output': 's rejected by picard\\nnever enter it and never take'},\n",
       " {'input_ids': tensor([    3,   122,     3,     9,  1551,   804,  1601, 11638,   250,     8,\n",
       "          10950, 19712,     7, 12967,    57,  6686,   986,   470,  2058,    34,\n",
       "             11,   470,   240,     1]),\n",
       "  'input_text': 'g a completely\\nﬁnalized beam because the hypotheses rejected by picard\\nnever enter it and never take',\n",
       "  'expected_output': ' up any spacelarge pretrained language models on b'},\n",
       " {'input_ids': tensor([    3,     7, 12967,    57,  6686,   986,   470,  2058,    34,    11,\n",
       "            470,   240,    95,   136,   628, 15599,  7140, 10761,  1612,  2250,\n",
       "             30,     3,   115,     1]),\n",
       "  'input_text': 's rejected by picard\\nnever enter it and never take up any spacelarge pretrained language models on b',\n",
       "  'expected_output': 'oth the\\nspider crossdomain and crossdatabase textt'},\n",
       " {'input_ids': tensor([   95,   136,   628, 15599,  7140, 10761,  1612,  2250,    30,   321,\n",
       "              8, 18612,  2269, 22999,    11,  2269,  6757, 10925,  1499,    17,\n",
       "              1]),\n",
       "  'input_text': ' up any spacelarge pretrained language models on both the\\nspider crossdomain and crossdatabase textt',\n",
       "  'expected_output': 'o\\nsql dataset and the cosql sqlgrounded dialog\\nsta'},\n",
       " {'input_ids': tensor([    3,    32,   189,     8, 18612,  2269, 22999,    11,  2269,  6757,\n",
       "          10925,  1499,   235, 11820,    40, 17953,    11,     8,   576,     7,\n",
       "           1824,    40, 11820,    40,   122, 12279, 13463,  3342,     1]),\n",
       "  'input_text': 'oth the\\nspider crossdomain and crossdatabase textto\\nsql dataset and the cosql sqlgrounded dialog\\nsta',\n",
       "  'expected_output': 'te tracking dataset we ﬁnd that the picard de\\ncodi'},\n",
       " {'input_ids': tensor([    3,    32, 11820,    40, 17953,    11,     8,   576,     7,  1824,\n",
       "             40, 11820,    40,   122, 12279, 13463,   538,  6418, 17953,    62,\n",
       "            253,    24,     8,  6686,   986,    20, 10763,    23,     1]),\n",
       "  'input_text': 'o\\nsql dataset and the cosql sqlgrounded dialog\\nstate tracking dataset we ﬁnd that the picard de\\ncodi',\n",
       "  'expected_output': 'ng method not only signiﬁcantly improves the\\nperfo'},\n",
       " {'input_ids': tensor([    3,    17,    15,  6418, 17953,    62,   253,    24,     8,  6686,\n",
       "            986,    20,     3,  9886,  1573,    59,   163,  4019,  1172,     7,\n",
       "              8,   399,    89,    32,     1]),\n",
       "  'input_text': 'te tracking dataset we ﬁnd that the picard de\\ncoding method not only signiﬁcantly improves the\\nperfo',\n",
       "  'expected_output': 'rmance of ﬁnetuned but otherwise unmodi\\nﬁed t5 mod'},\n",
       " {'input_ids': tensor([   3, 1725, 1573,   59,  163, 4019, 1172,    7,    8,  821,   13, 1399,\n",
       "            17,  444,   26,   68, 2904,   73, 7360,   23, 1479,   26,    3,   17,\n",
       "           755, 1794,    1]),\n",
       "  'input_text': 'ng method not only signiﬁcantly improves the\\nperformance of ﬁnetuned but otherwise unmodi\\nﬁed t5 mod',\n",
       "  'expected_output': 'els it also lifts a t53b model to state\\noftheart r'},\n",
       " {'input_ids': tensor([   3,   52,   51,  663,   13, 1399,   17,  444,   26,   68, 2904,   73,\n",
       "          7360,   23, 1479,   26,    3,   17,  755, 2250,   34,   92, 5656,    7,\n",
       "             3,    9,    3,   17, 4867,  115,  825,   12,  538,   13,  532, 1408,\n",
       "             3,   52,    1]),\n",
       "  'input_text': 'rmance of ﬁnetuned but otherwise unmodi\\nﬁed t5 models it also lifts a t53b model to state\\noftheart r',\n",
       "  'expected_output': 'esults on the established exactmatch\\nand execution'},\n",
       " {'input_ids': tensor([    3,  3573,    34,    92,  5656,     7,     3,     9,     3,    17,\n",
       "           4867,   115,   825,    12,   538,    13,   532,  1408,   772,    30,\n",
       "              8,  2127,  2883, 19515,    11,  9328,     1]),\n",
       "  'input_text': 'els it also lifts a t53b model to state\\noftheart results on the established exactmatch\\nand execution',\n",
       "  'expected_output': ' accuracy metrics\\nacknowledgements\\nwe thank lee za'},\n",
       " {'input_ids': tensor([    3,    15,     7,    83,    17,     7,    30,     8,  2127,  2883,\n",
       "          19515,    11,  9328,  7452, 15905,  8406,  4128,    62,  2763,    90,\n",
       "             15,     3,  1629,     1]),\n",
       "  'input_text': 'esults on the established exactmatch\\nand execution accuracy metrics\\nacknowledgements\\nwe thank lee za',\n",
       "  'expected_output': 'mparo for his contributions to the\\nexperiments on '},\n",
       " {'input_ids': tensor([ 7452, 15905,  8406,  4128,    62,  2763,    90,    15,     3, 19271,\n",
       "           1893,    32,    21,   112,  7548,    12,     8, 12341,    30,     1]),\n",
       "  'input_text': ' accuracy metrics\\nacknowledgements\\nwe thank lee zamparo for his contributions to the\\nexperiments on ',\n",
       "  'expected_output': 'the cosql dataset further we\\nwould like to thank p'},\n",
       " {'input_ids': tensor([    3,    51,  1893,    32,    21,   112,  7548,    12,     8, 12341,\n",
       "             30,     8,   576,     7,  1824,    40, 17953,   856,    62,   133,\n",
       "            114,    12,  2763,     3,   102,     1]),\n",
       "  'input_text': 'mparo for his contributions to the\\nexperiments on the cosql dataset further we\\nwould like to thank p',\n",
       "  'expected_output': 'ete shaw for his input on\\nthe reproduction of the '},\n",
       " {'input_ids': tensor([    8,   576,     7,  1824,    40, 17953,   856,    62,   133,   114,\n",
       "             12,  2763,  3947,    15,     3, 15622,    21,   112,  3785,    30,\n",
       "              8, 19192,    13,     8,     1]),\n",
       "  'input_text': 'the cosql dataset further we\\nwould like to thank pete shaw for his input on\\nthe reproduction of the ',\n",
       "  'expected_output': 't5 results on spider we\\nwould also like to extend '},\n",
       " {'input_ids': tensor([    3,    15,    17,    15,     3, 15622,    21,   112,  3785,    30,\n",
       "              8, 19192,    13,     8,     3,    17,   755,   772,    30, 18612,\n",
       "             62,   133,    92,   114,    12,  4285,     1]),\n",
       "  'input_text': 'ete shaw for his input on\\nthe reproduction of the t5 results on spider we\\nwould also like to extend ',\n",
       "  'expected_output': 'our gratitude to tao yu\\nand yusen zhang for their '},\n",
       " {'input_ids': tensor([    3,    17,   755,   772,    30, 18612,    62,   133,    92,   114,\n",
       "             12,  4285,    69, 17142,    12,     3,    17,     9,    32,     3,\n",
       "             63,    76,    11,     3,    63,   302,    35,     3,   172,  9270,\n",
       "             21,    70,     1]),\n",
       "  'input_text': 't5 results on spider we\\nwould also like to extend our gratitude to tao yu\\nand yusen zhang for their ',\n",
       "  'expected_output': 'efforts in evaluating our\\nmodel on the test split '},\n",
       " {'input_ids': tensor([   69, 17142,    12,     3,    17,     9,    32,     3,    63,    76,\n",
       "             11,     3,    63,   302,    35,     3,   172,  9270,    21,    70,\n",
       "           2231,    16,     3, 17768,    69,   825,    30,     8,   794,  5679,\n",
       "              1]),\n",
       "  'input_text': 'our gratitude to tao yu\\nand yusen zhang for their efforts in evaluating our\\nmodel on the test split ',\n",
       "  'expected_output': 'of the spider and cosql\\ndatasets finally we thank '},\n",
       " {'input_ids': tensor([ 2231,    16,     3, 17768,    69,   825,    30,     8,   794,  5679,\n",
       "             13,     8, 18612,    11,   576,     7,  1824,    40, 17953,     7,\n",
       "           2031,    62,  2763,     1]),\n",
       "  'input_text': 'efforts in evaluating our\\nmodel on the test split of the spider and cosql\\ndatasets finally we thank ',\n",
       "  'expected_output': 'our anonymous review\\ners for their time and valuab'},\n",
       " {'input_ids': tensor([   13,     8, 18612,    11,   576,     7,  1824,    40, 17953,     7,\n",
       "           2031,    62,  2763,    69, 17896,  1132,     3,   277,    21,    70,\n",
       "             97,    11,     3,  7480,     9,   115,     1]),\n",
       "  'input_text': 'of the spider and cosql\\ndatasets finally we thank our anonymous review\\ners for their time and valuab',\n",
       "  'expected_output': 'le suggestions9900references\\nruisheng cao lu chen '},\n",
       " {'input_ids': tensor([   69, 17896,  1132,     3,   277,    21,    70,    97,    11,  3435,\n",
       "           5782,  3264,  1206,    60, 11788,     7,     3,    52,    76,  1273,\n",
       "           4606,   212,    32,     3,    40,    76,     3,  1559,     1]),\n",
       "  'input_text': 'our anonymous review\\ners for their time and valuable suggestions9900references\\nruisheng cao lu chen ',\n",
       "  'expected_output': 'zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql'},\n",
       " {'input_ids': tensor([   90,  5782,  3264,  1206,    60, 11788,     7,     3,    52,    76,\n",
       "           1273,  4606,   212,    32,     3,    40,    76,     3,  1559,     3,\n",
       "            172,   107,    23,     3,  1559,     3,    63,   152,  4517,     3,\n",
       "            172,  1024,    32,  2629,     3,   172,   107,    76,    11,     3,\n",
       "           1258,    23,     3,    63,    76,   460,  2658,     3,    40,  2897,\n",
       "           1824,    40,     1]),\n",
       "  'input_text': 'le suggestions9900references\\nruisheng cao lu chen zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql',\n",
       "  'expected_output': ' line graph en\\nhanced texttosql model with mixed l'},\n",
       " {'input_ids': tensor([   3,  172,  107,   23,    3, 1559,    3,   63,  152, 4517,    3,  172,\n",
       "          1024,   32, 2629,    3,  172,  107,   76,   11,    3, 1258,   23,    3,\n",
       "            63,   76,  460, 2658,    3,   40, 2897, 1824,   40,  689, 8373,    3,\n",
       "            35,    3,  107,  663,   26, 1499,  235,    7, 1824,   40,  825,   28,\n",
       "          4838,    3,   40,    1]),\n",
       "  'input_text': 'zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql line graph en\\nhanced texttosql model with mixed l',\n",
       "  'expected_output': 'ocal and non\\nlocal relations in proceedings of the'},\n",
       " {'input_ids': tensor([  689,  8373,     3,    35,     3,   107,   663,    26,  1499,   235,\n",
       "              7,  1824,    40,   825,    28,  4838,   415,    11,   529,   415,\n",
       "           5836,    16, 13339,    13,     8,     1]),\n",
       "  'input_text': ' line graph en\\nhanced texttosql model with mixed local and non\\nlocal relations in proceedings of the',\n",
       "  'expected_output': ' 59th annual\\nmeeting of the association for comput'},\n",
       " {'input_ids': tensor([    3,    32,  1489,    11,   529,   415,  5836,    16, 13339,    13,\n",
       "              8,     3,  3390,   189,  2041,  1338,    13,     8,  6028,    21,\n",
       "              3,   287,  2562,     1]),\n",
       "  'input_text': 'ocal and non\\nlocal relations in proceedings of the 59th annual\\nmeeting of the association for comput',\n",
       "  'expected_output': 'ational lin\\nguistics and the 11th international jo'},\n",
       " {'input_ids': tensor([    3,  3390,   189,  2041,  1338,    13,     8,  6028,    21, 25850,\n",
       "              3,    40,    77,     3,  1744,  3040,     7,    11,     8,   850,\n",
       "            189,  1038,     3,  1927,     1]),\n",
       "  'input_text': ' 59th annual\\nmeeting of the association for computational lin\\nguistics and the 11th international jo',\n",
       "  'expected_output': 'int conference\\non natural language processing volu'},\n",
       " {'input_ids': tensor([   44,  6318,     3,    40,    77,     3,  1744,  3040,     7,    11,\n",
       "              8,   850,   189,  1038,  4494,  2542,    30,   793,  1612,  3026,\n",
       "              3, 12740,     1]),\n",
       "  'input_text': 'ational lin\\nguistics and the 11th international joint conference\\non natural language processing volu',\n",
       "  'expected_output': 'me 1 long\\npapers  pages 25412555 online associatio'},\n",
       " {'input_ids': tensor([   16,    17,  2542,    30,   793,  1612,  3026,  2908,   209,   307,\n",
       "           5778,  1688,   944,   591, 10124,  3769,   367,    38,  5444,   144,\n",
       "             23,    32,     1]),\n",
       "  'input_text': 'int conference\\non natural language processing volume 1 long\\npapers  pages 25412555 online associatio',\n",
       "  'expected_output': 'n for\\ncomputational linguistics\\ndonghyun choi myeo'},\n",
       " {'input_ids': tensor([  140,   209,   307,  5778,  1688,   944,   591, 10124,  3769,   367,\n",
       "           6028,    21, 25850,     3, 24703,     7,   278,   122,   107,    63,\n",
       "            202,     3,  3995,    23,    82,    15,    32,     1]),\n",
       "  'input_text': 'me 1 long\\npapers  pages 25412555 online association for\\ncomputational linguistics\\ndonghyun choi myeo',\n",
       "  'expected_output': 'ng cheol shin eunggyun kim\\nand dong ryeol shin 202'},\n",
       " {'input_ids': tensor([    3,    29,    21, 25850,     3, 24703,     7,   278,   122,   107,\n",
       "             63,   202,     3,  3995,    23,    82,    15,  2444,     3,  1033,\n",
       "             32,    40,     3,     7,  2907,     3,    15,   202,  9559,   202,\n",
       "              3, 19754,    11,   278,   122,     3,   651,    15,    32,    40,\n",
       "              3,     7,  2907,     3, 19818,     1]),\n",
       "  'input_text': 'n for\\ncomputational linguistics\\ndonghyun choi myeong cheol shin eunggyun kim\\nand dong ryeol shin 202',\n",
       "  'expected_output': '1 ryansql re\\ncursively applying sketchbased slot f'},\n",
       " {'input_ids': tensor([    3,  1725,     3,  1033,    32,    40,     3,     7,  2907,     3,\n",
       "             15,   202,  9559,   202,     3, 19754,    11,   278,   122,     3,\n",
       "            651,    15,    32,    40,     3,     7,  2907,   460,  2658,     3,\n",
       "            651,  3247,  1824,    40,     3,    60,  8385, 13830,  6247, 13278,\n",
       "            390,  4918,     3,    89,     1]),\n",
       "  'input_text': 'ng cheol shin eunggyun kim\\nand dong ryeol shin 2021 ryansql re\\ncursively applying sketchbased slot f',\n",
       "  'expected_output': 'illings for\\ncomplex texttosql in crossdomain datab'},\n",
       " {'input_ids': tensor([  209,     3,   651,  3247,  1824,    40,     3,    60,  8385, 13830,\n",
       "           6247, 13278,   390,  4918,    14,    53,     7,    21,  1561,  1499,\n",
       "            235,     7,  1824,    40,    16,  2269, 22999,   331,   115,     1]),\n",
       "  'input_text': '1 ryansql re\\ncursively applying sketchbased slot fillings for\\ncomplex texttosql in crossdomain datab',\n",
       "  'expected_output': 'ases\\ncomputational linguistics  472309332\\ndaan lei'},\n",
       " {'input_ids': tensor([    3,  1092,    53,     7,    21,  1561,  1499,   235,     7,  1824,\n",
       "             40,    16,  2269, 22999, 16961, 25850,     3, 24703,     7, 10635,\n",
       "          13427,  4271,  2668,   836,   152,  4628,     1]),\n",
       "  'input_text': 'illings for\\ncomplex texttosql in crossdomain databases\\ncomputational linguistics  472309332\\ndaan lei',\n",
       "  'expected_output': 'jen and erik meijer 2001 parsec direct\\nstyle monad'},\n",
       " {'input_ids': tensor([   38,    15,     7, 25850,     3, 24703,     7, 10635, 13427,  4271,\n",
       "           2668,   836,   152,  4628,   354,    35,    11,     3,    15,  9629,\n",
       "            140,    23, 12488,  4402,   260,  7549,  1223,   869,  1911,     9,\n",
       "             26,     1]),\n",
       "  'input_text': 'ases\\ncomputational linguistics  472309332\\ndaan leijen and erik meijer 2001 parsec direct\\nstyle monad',\n",
       "  'expected_output': 'ic parser combinators for the real world\\ntechnical'},\n",
       " {'input_ids': tensor([  528,    29,    11,     3,    15,  9629,   140,    23, 12488,  4402,\n",
       "            260,  7549,  1223,   869,  1911,     9,  4370,   260,     7,    49,\n",
       "          10374,  6230,    21,     8,   490,   296,  2268,     1]),\n",
       "  'input_text': 'jen and erik meijer 2001 parsec direct\\nstyle monadic parser combinators for the real world\\ntechnical',\n",
       "  'expected_output': ' report uucs200127 user model\\ning 2007 11th intern'},\n",
       " {'input_ids': tensor([    3,   447,   260,     7,    49, 10374,  6230,    21,     8,   490,\n",
       "            296,  2268,   934,     3,    76,    76,    75,     7, 23658,  2555,\n",
       "           1139,   825,     3,    53,  4101,   850,   189,  9342,     1]),\n",
       "  'input_text': 'ic parser combinators for the real world\\ntechnical report uucs200127 user model\\ning 2007 11th intern',\n",
       "  'expected_output': 'ational conference um 2007\\ncorfu greece june 2529 '},\n",
       " {'input_ids': tensor([  934,     3,    76,    76,    75,     7, 23658,  2555,  1139,   825,\n",
       "              3,    53,  4101,   850,   189,  1038,  2542,   561,  4101,  4301,\n",
       "             89,    76,     3,  3584,    15,   565,     3,  6959,    15,   944,\n",
       "           3166,     1]),\n",
       "  'input_text': ' report uucs200127 user model\\ning 2007 11th international conference um 2007\\ncorfu greece june 2529 ',\n",
       "  'expected_output': '2007\\nkevin lin ben bogin mark neumann jonathan be\\n'},\n",
       " {'input_ids': tensor([   44,  6318,  2542,   561,  4101,  4301,    89,    76,     3,  3584,\n",
       "             15,   565,     3,  6959,    15,   944,  3166,  4101,     3,  1050,\n",
       "           2494,     3,    40,    77,    36,    29,     3, 12247,    77,  3946,\n",
       "           5854,  2434,     3, 15429,     9,  6736,    36,     1]),\n",
       "  'input_text': 'ational conference um 2007\\ncorfu greece june 2529 2007\\nkevin lin ben bogin mark neumann jonathan be\\n',\n",
       "  'expected_output': 'rant and matt gardner 2019 grammarbased neu\\nral te'},\n",
       " {'input_ids': tensor([ 4101,     3,  1050,  2494,     3,    40,    77,    36,    29,     3,\n",
       "          12247,    77,  3946,  5854,  2434,     3, 15429,     9,  6736,    36,\n",
       "              3,  3569,    11,  6928,    17,     3,  6390,   687,  1360, 19519,\n",
       "            390,  5854,     3,  4900,     3,    17,    15,     1]),\n",
       "  'input_text': '2007\\nkevin lin ben bogin mark neumann jonathan be\\nrant and matt gardner 2019 grammarbased neu\\nral te',\n",
       "  'expected_output': 'xttosql generation\\nxi victoria lin richard socher '},\n",
       " {'input_ids': tensor([    3,  3569,    11,  6928,    17,     3,  6390,   687,  1360, 19519,\n",
       "            390,  5854,     3,  4900,  1499,   235,     7,  1824,    40,  3381,\n",
       "              3,   226,    23,     3,  7287,  3600,     9,     3,    40,    77,\n",
       "           2354,   986,    78,  1703,     1]),\n",
       "  'input_text': 'rant and matt gardner 2019 grammarbased neu\\nral texttosql generation\\nxi victoria lin richard socher ',\n",
       "  'expected_output': 'and caiming xiong\\n2020 bridging textual and tabula'},\n",
       " {'input_ids': tensor([    3,   226,    17,   235,     7,  1824,    40,  3381,     3,   226,\n",
       "             23,     3,  7287,  3600,     9,     3,    40,    77,  2354,   986,\n",
       "             78,  1703,    11,     3,    75, 19874,     3,   226,    23,  2444,\n",
       "           6503,     3,  2160, 12720,  1499,  3471,    11,  3808,    83,     9,\n",
       "              1]),\n",
       "  'input_text': 'xttosql generation\\nxi victoria lin richard socher and caiming xiong\\n2020 bridging textual and tabula',\n",
       "  'expected_output': 'r data for cross\\ndomain texttosql semantic parsing'},\n",
       " {'input_ids': tensor([   11,     3,    75, 19874,     3,   226,    23,  2444,  6503,     3,\n",
       "           2160, 12720,  1499,  3471,    11,  3808,  4885,   331,    21,  2269,\n",
       "           3303,  1499,   235,     7,  1824,    40, 27632,   260,     7,    53,\n",
       "              1]),\n",
       "  'input_text': 'and caiming xiong\\n2020 bridging textual and tabular data for cross\\ndomain texttosql semantic parsing',\n",
       "  'expected_output': ' findings of the\\nassociation for computational lin'},\n",
       " {'input_ids': tensor([    3,    52,   331,    21,  2269,  3303,  1499,   235,     7,  1824,\n",
       "             40, 27632,   260,     7,    53,  7469,    13,     8,  6028,    21,\n",
       "          25850,     3,    40,    77,     1]),\n",
       "  'input_text': 'r data for cross\\ndomain texttosql semantic parsing findings of the\\nassociation for computational lin',\n",
       "  'expected_output': 'guistics emnlp\\n2020 \\nbryan osullivan and ben gamar'},\n",
       " {'input_ids': tensor([ 7469,    13,     8,  6028,    21, 25850,     3, 24703,     7,     3,\n",
       "             15,    51,    29,    40,   102,  6503,  6397,    63,   152,     3,\n",
       "             32,     7,    83, 20580,   152,    11,    36,    29, 17371,    52,\n",
       "              1]),\n",
       "  'input_text': ' findings of the\\nassociation for computational linguistics emnlp\\n2020 \\nbryan osullivan and ben gamar',\n",
       "  'expected_output': 'i 2021 attopar\\nsec fast combinator parsing for byt'},\n",
       " {'input_ids': tensor([    3,  1744,  3040,     7,     3,    15,    51,    29,    40,   102,\n",
       "           6503,  6397,    63,   152,     3,    32,     7,    83, 20580,   152,\n",
       "             11,    36,    29,     3,  8758,  1665,   460,  2658,    44,   235,\n",
       "           1893,  4220,  1006, 10374,  1016,   260,     7,    53,    21,    57,\n",
       "             17,     1]),\n",
       "  'input_text': 'guistics emnlp\\n2020 \\nbryan osullivan and ben gamari 2021 attopar\\nsec fast combinator parsing for byt',\n",
       "  'expected_output': 'estrings and text\\nsoftware available on the haskel'},\n",
       " {'input_ids': tensor([    3,    23,   460,  2658,    44,   235,  1893,  4220,  1006, 10374,\n",
       "           1016,   260,     7,    53,    21,     3, 17770, 16099,     7,    11,\n",
       "           1499,   889,   347,    30,     8,    65,  5768,     1]),\n",
       "  'input_text': 'i 2021 attopar\\nsec fast combinator parsing for bytestrings and text\\nsoftware available on the haskel',\n",
       "  'expected_output': 'l package reposi\\ntory\\ncolin raffel noam shazeer ad'},\n",
       " {'input_ids': tensor([  259,  1007,     7,    11,  1499,   889,   347,    30,     8,    65,\n",
       "           5768,    40,  2642, 14173,    23,    12,   651,  7632,    77,     3,\n",
       "             52,     9, 16387,   150,   265,     3,     7, 10557,    15,    49,\n",
       "              3,     9,    26,     1]),\n",
       "  'input_text': 'estrings and text\\nsoftware available on the haskell package reposi\\ntory\\ncolin raffel noam shazeer ad',\n",
       "  'expected_output': 'am roberts katherine\\nlee sharan narang michael mat'},\n",
       " {'input_ids': tensor([    3,    40,  2642, 14173,    23,    12,   651,  7632,    77,     3,\n",
       "             52,     9, 16387,   150,   265,     3,     7, 10557,    15,    49,\n",
       "              3,     9,  7812,     3,  5840,    49,    17,     7,     3,  8682,\n",
       "            760,   630,    90,    15,     3,     7, 14888,    29,     3,    29,\n",
       "              9,  6287,  2278,     9,    15,    40,  6928,     1]),\n",
       "  'input_text': 'l package reposi\\ntory\\ncolin raffel noam shazeer adam roberts katherine\\nlee sharan narang michael mat',\n",
       "  'expected_output': 'ena yanqi zhou\\nwei li and peter j liu 2020 explori'},\n",
       " {'input_ids': tensor([  183,     3,  5840,    49,    17,     7,     3,  8682,   760,   630,\n",
       "             90,    15,     3,     7, 14888,    29,     3,    29,     9,  6287,\n",
       "           2278,     9,    15,    40,  6928,    35,     9,     3,    63,   152,\n",
       "           1824,    23,     3, 25303,    62,    23,     3,    40,    23,    11,\n",
       "            158,   449,     3,   354,     3,    40,    23,    76,  6503, 26903,\n",
       "             23,     1]),\n",
       "  'input_text': 'am roberts katherine\\nlee sharan narang michael matena yanqi zhou\\nwei li and peter j liu 2020 explori',\n",
       "  'expected_output': 'ng the lim\\nits of transfer learning with a uniﬁed '},\n",
       " {'input_ids': tensor([    3,    35,     9,     3,    63,   152,  1824,    23,     3, 25303,\n",
       "             62,    23,     3,    40,    23,    11,   158,   449,     3,   354,\n",
       "              3,    40,    23,    76,  6503,  6990,     8,     3,  4941,   165,\n",
       "             13,  2025,  1036,    28,     3,     9,     3, 22927,     1]),\n",
       "  'input_text': 'ena yanqi zhou\\nwei li and peter j liu 2020 exploring the lim\\nits of transfer learning with a uniﬁed ',\n",
       "  'expected_output': 'texttotext\\ntransformer journal of machine learning'},\n",
       " {'input_ids': tensor([    3,  1725,     8,     3,  4941,   165,    13,  2025,  1036,    28,\n",
       "              3,     9,     3, 22927,  1499,   235,  6327, 19903,  6378,    13,\n",
       "           1437,  1036,     1]),\n",
       "  'input_text': 'ng the lim\\nits of transfer learning with a uniﬁed texttotext\\ntransformer journal of machine learning',\n",
       "  'expected_output': ' research \\n21167\\nohad rubin and jonathan berant 20'},\n",
       " {'input_ids': tensor([ 1499,   235,  6327, 19903,  6378,    13,  1437,  1036,   585,  1401,\n",
       "          27650,     3,    32,  8399,  9641,    77,    11,     3, 15429,     9,\n",
       "           6736,    36,  3569,   460,     1]),\n",
       "  'input_text': 'texttotext\\ntransformer journal of machine learning research \\n21167\\nohad rubin and jonathan berant 20',\n",
       "  'expected_output': '21 smbop\\nsemiautoregressive bottomup semantic pars'},\n",
       " {'input_ids': tensor([  585,  1401, 27650,     3,    32,  8399,  9641,    77,    11,     3,\n",
       "          15429,     9,  6736,    36,  3569,   460,  2658,     3,     7,  6310,\n",
       "            102,  4772,  8010,    60, 10292,   757,  2007,   413, 27632,   260,\n",
       "              7,     1]),\n",
       "  'input_text': ' research \\n21167\\nohad rubin and jonathan berant 2021 smbop\\nsemiautoregressive bottomup semantic pars',\n",
       "  'expected_output': 'ing in\\nproceedings of the 2021 conference of the n'},\n",
       " {'input_ids': tensor([ 1401,     3,     7,  6310,   102,  4772,  8010,    60, 10292,   757,\n",
       "           2007,   413, 27632,   260,     7,    53,    16, 13339,    13,     8,\n",
       "            460,  2658,  2542,    13,     8,     3,    29,     1]),\n",
       "  'input_text': '21 smbop\\nsemiautoregressive bottomup semantic parsing in\\nproceedings of the 2021 conference of the n',\n",
       "  'expected_output': 'orth\\namerican chapter of the association for compu'},\n",
       " {'input_ids': tensor([    3,    53,    16, 13339,    13,     8,   460,  2658,  2542,    13,\n",
       "              8,  3457, 10211,  5800,    13,     8,  6028,    21,  2890,    76,\n",
       "              1]),\n",
       "  'input_text': 'ing in\\nproceedings of the 2021 conference of the north\\namerican chapter of the association for compu',\n",
       "  'expected_output': 'ta\\ntional linguistics human language technologies '},\n",
       " {'input_ids': tensor([   42,   189, 10211,  5800,    13,     8,  6028,    21,     3,   287,\n",
       "           2562,     9,     3,    17,  6318,     3, 24703,     7,   936,  1612,\n",
       "           2896,     1]),\n",
       "  'input_text': 'orth\\namerican chapter of the association for computa\\ntional linguistics human language technologies ',\n",
       "  'expected_output': '\\npages 311324 online association for computa\\ntiona'},\n",
       " {'input_ids': tensor([    3,    17,     9,     3,    17,  6318,     3, 24703,     7,   936,\n",
       "           1612,  2896,  1688,   220, 20522,  2266,   367,  6028,    21,     3,\n",
       "            287,  2562,     9,     3,  1575,     9,     1]),\n",
       "  'input_text': 'ta\\ntional linguistics human language technologies \\npages 311324 online association for computa\\ntiona',\n",
       "  'expected_output': 'l linguistics\\npeter shaw mingwei chang panupong pa'},\n",
       " {'input_ids': tensor([ 1688,   220, 20522,  2266,   367,  6028,    21,     3,   287,  2562,\n",
       "              9,     3,    17,  6318,     3, 24703,     7,   158,   449,     3,\n",
       "          15622,     3,    51,    53,  1123,    23,     3,    75,  9270,  2131,\n",
       "            413,  2444,  2576,     1]),\n",
       "  'input_text': '\\npages 311324 online association for computa\\ntional linguistics\\npeter shaw mingwei chang panupong pa',\n",
       "  'expected_output': 'supat and\\nkristina toutanova 2021 compositional ge'},\n",
       " {'input_ids': tensor([    3,    40,     3, 24703,     7,   158,   449,     3, 15622,     3,\n",
       "             51,    53,  1123,    23,     3,    75,  9270,  2131,   413,  2444,\n",
       "            330,   413,   144,    11,     3,   157, 22061,    29,     9,   870,\n",
       "              9, 14979,   460,  2658,  5761,   138,   873,     1]),\n",
       "  'input_text': 'l linguistics\\npeter shaw mingwei chang panupong pasupat and\\nkristina toutanova 2021 compositional ge',\n",
       "  'expected_output': 'neral\\nization and natural language variation can a'},\n",
       " {'input_ids': tensor([ 2629,  4665,    11,     3,   157, 22061,    29,     9,   870,     9,\n",
       "          14979,   460,  2658,  5761,   138,   879,     3,  1707,    11,   793,\n",
       "           1612, 12338,    54,     3,     9,     1]),\n",
       "  'input_text': 'supat and\\nkristina toutanova 2021 compositional general\\nization and natural language variation can a',\n",
       "  'expected_output': ' se\\nmantic parsing approach handle both in proceed'},\n",
       " {'input_ids': tensor([    3,   687,   138,     3,  1707,    11,   793,  1612, 12338,    54,\n",
       "              3,     9,   142,   388,  1225,   260,     7,    53,  1295,  2174,\n",
       "            321,    16,  8669,     1]),\n",
       "  'input_text': 'neral\\nization and natural language variation can a se\\nmantic parsing approach handle both in proceed',\n",
       "  'expected_output': '\\nings of the 59th annual meeting of the associatio'},\n",
       " {'input_ids': tensor([ 142,  388, 1225,  260,    7,   53, 1295, 2174,  321,   16, 8669,    3,\n",
       "            53,    7,   13,    8,    3, 3390,  189, 2041, 1338,   13,    8,   38,\n",
       "          5444,  144,   23,   32,    1]),\n",
       "  'input_text': ' se\\nmantic parsing approach handle both in proceed\\nings of the 59th annual meeting of the associatio',\n",
       "  'expected_output': 'n\\nfor computational linguistics and the 11th inter'},\n",
       " {'input_ids': tensor([    3,    53,     7,    13,     8,     3,  3390,   189,  2041,  1338,\n",
       "             13,     8,  6028,    21, 25850,     3, 24703,     7,    11,     8,\n",
       "            850,   189,  1413,     1]),\n",
       "  'input_text': '\\nings of the 59th annual meeting of the association\\nfor computational linguistics and the 11th inter',\n",
       "  'expected_output': 'na\\ntional joint conference on natural language pro'},\n",
       " {'input_ids': tensor([    3,    29,    21, 25850,     3, 24703,     7,    11,     8,   850,\n",
       "            189,  9342,     9,     3,    17,  6318,  4494,  2542,    30,   793,\n",
       "           1612,   813,     1]),\n",
       "  'input_text': 'n\\nfor computational linguistics and the 11th interna\\ntional joint conference on natural language pro',\n",
       "  'expected_output': '\\ncessing volume 1 long papers  pages 922938\\nonline'},\n",
       " {'input_ids': tensor([   3,   29,    9,    3,   17, 6318, 4494, 2542,   30,  793, 1612,  813,\n",
       "          1830,    7,   53, 2908,  209,  307, 5778, 1688,    3, 4508, 3166, 3747,\n",
       "           367,    1]),\n",
       "  'input_text': 'na\\ntional joint conference on natural language pro\\ncessing volume 1 long papers  pages 922938\\nonline',\n",
       "  'expected_output': ' association for computational linguistics\\nnoam sh'},\n",
       " {'input_ids': tensor([ 1830,     7,    53,  2908,   209,   307,  5778,  1688,     3,  4508,\n",
       "           3166,  3747,   367,  6028,    21, 25850,     3, 24703,     7,   150,\n",
       "            265,  6660,     1]),\n",
       "  'input_text': '\\ncessing volume 1 long papers  pages 922938\\nonline association for computational linguistics\\nnoam sh',\n",
       "  'expected_output': 'azeer and mitchell stern 2018 adafactor\\nadaptive l'},\n",
       " {'input_ids': tensor([ 6028,    21, 25850,     3, 24703,     7,   150,   265,     3,     7,\n",
       "          10557,    15,    49,    11,   181,  1033,   195,     3, 13072,   846,\n",
       "              3,     9,    26,     9, 17899, 25326,     3,    40,     1]),\n",
       "  'input_text': ' association for computational linguistics\\nnoam shazeer and mitchell stern 2018 adafactor\\nadaptive l',\n",
       "  'expected_output': 'earning rates with sublinear memory costininternat'},\n",
       " {'input_ids': tensor([    3,     9,   776,    49,    11,   181,  1033,   195,     3, 13072,\n",
       "            846,     3,     9,    26,     9, 17899, 25326,  1036,  1917,    28,\n",
       "            769,   747,   291,  2594,   583,    77,  3870,    29,   144,     1]),\n",
       "  'input_text': 'azeer and mitchell stern 2018 adafactor\\nadaptive learning rates with sublinear memory costininternat',\n",
       "  'expected_output': 'ional conference on machine learning \\npages 459646'},\n",
       " {'input_ids': tensor([12127,  1917,    28,   769,   747,   291,  2594,   583,    77, 27817,\n",
       "           2542,    30,  1437,  1036,  1688,  3479,  4314,  4448,     1]),\n",
       "  'input_text': 'earning rates with sublinear memory costininternational conference on machine learning \\npages 459646',\n",
       "  'expected_output': '04 pmlr\\npeng shi patrick ng zhiguo wang henghui zh'},\n",
       " {'input_ids': tensor([    3,  6318,  2542,    30,  1437,  1036,  1688,  3479,  4314, 25991,\n",
       "            591,  6366,    40,    52,   158,  1725,     3,  5605,  6234,  5206,\n",
       "              3,  1725,     3,   172,   107,    23,  1744,    32,     3, 17789,\n",
       "              3,    88,  1725,  3464,     3,   172,   107,     1]),\n",
       "  'input_text': 'ional conference on machine learning \\npages 45964604 pmlr\\npeng shi patrick ng zhiguo wang henghui zh',\n",
       "  'expected_output': 'u\\nalexander hanbo li jun wang cicero nogueira\\ndos '},\n",
       " {'input_ids': tensor([11484,  6366,    40,    52,   158,  1725,     3,  5605,  6234,  5206,\n",
       "              3,  1725,     3,   172,   107,    23,  1744,    32,     3, 17789,\n",
       "              3,    88,  1725,  3464,     3,   172,   107,    76,  1240,   226,\n",
       "          11849,     3,  2618,   115,    32,     3,    40,    23,     3,  6959,\n",
       "              3, 17789,     3, 13241,    49,    32,   150,  1744, 15809,   103,\n",
       "              7,     1]),\n",
       "  'input_text': '04 pmlr\\npeng shi patrick ng zhiguo wang henghui zhu\\nalexander hanbo li jun wang cicero nogueira\\ndos ',\n",
       "  'expected_output': 'santos and bing xiang 2021 learning con\\ntextual re'},\n",
       " {'input_ids': tensor([    3,    76,  1240,   226, 11849,     3,  2618,   115,    32,     3,\n",
       "             40,    23,     3,  6959,     3, 17789,     3, 13241,    49,    32,\n",
       "            150,  1744, 15809,   103,     7,     3,     7,   288,    32,     7,\n",
       "             11,     3,   115,    53,     3, 19838,  1725,   460,  2658,  1036,\n",
       "            975,  1499,  3471,     3,    60,     1]),\n",
       "  'input_text': 'u\\nalexander hanbo li jun wang cicero nogueira\\ndos santos and bing xiang 2021 learning con\\ntextual re',\n",
       "  'expected_output': 'presentations for semantic parsing with\\ngeneration'},\n",
       " {'input_ids': tensor([    3,     7,   288,    32,     7,    11,     3,   115,    53,     3,\n",
       "          19838,  1725,   460,  2658,  1036,   975,  1499,  3471,  6497,     7,\n",
       "             21, 27632,   260,     7,    53,    28,  3381,     1]),\n",
       "  'input_text': 'santos and bing xiang 2021 learning con\\ntextual representations for semantic parsing with\\ngeneration',\n",
       "  'expected_output': 'augmented pretraining in proceedings\\nof the aaai c'},\n",
       " {'input_ids': tensor([ 9972,    21, 27632,   260,     7,    53,    28,  3381, 28984,   554,\n",
       "          13023,    16, 13339,    13,     8,     3,     9,     9,     9,    23,\n",
       "              3,    75,     1]),\n",
       "  'input_text': 'presentations for semantic parsing with\\ngenerationaugmented pretraining in proceedings\\nof the aaai c',\n",
       "  'expected_output': 'onference on artiﬁcial intelligence \\nvolume 35 pag'},\n",
       " {'input_ids': tensor([    3, 28984,   554, 13023,    16, 13339,    13,     8,     3,     9,\n",
       "              9,     9,    23,  2542,    30,  7353,  6123,  2908,  3097,     3,\n",
       "          11057,     1]),\n",
       "  'input_text': 'augmented pretraining in proceedings\\nof the aaai conference on artiﬁcial intelligence \\nvolume 35 pag',\n",
       "  'expected_output': 'es 1380613814\\nalane suhr mingwei chang peter shaw '},\n",
       " {'input_ids': tensor([   30, 11788,    30,  7353,  6123,  2908,  3097,  1688,     3, 22744,\n",
       "           5176, 22744,  2534,     3,     9,  8102,     3,     7, 19290,     3,\n",
       "             51,    53,  1123,    23,     3,    75,  9270,   158,   449,     3,\n",
       "          15622,     1]),\n",
       "  'input_text': 'onference on artiﬁcial intelligence \\nvolume 35 pages 1380613814\\nalane suhr mingwei chang peter shaw ',\n",
       "  'expected_output': 'and ken\\nton lee 2020 exploring unexplored generali'},\n",
       " {'input_ids': tensor([    3,    15,     7,     3, 22744,  5176, 22744,  2534,     3,     9,\n",
       "           8102,     3,     7, 19290,     3,    51,    53,  1123,    23,     3,\n",
       "             75,  9270,   158,   449,     3, 15622,    11,     3,  2217,    12,\n",
       "             29,    90,    15,  6503,  6990,    73,  9080,  1271,   879,    23,\n",
       "              1]),\n",
       "  'input_text': 'es 1380613814\\nalane suhr mingwei chang peter shaw and ken\\nton lee 2020 exploring unexplored generali',\n",
       "  'expected_output': 'zation\\nchallenges for crossdatabase semantic parsi'},\n",
       " {'input_ids': tensor([   11,     3,  2217,    12,    29,    90,    15,  6503,  6990,    73,\n",
       "           9080,  1271,   879,  1707,  2428,    21,  2269,  6757, 10925, 27632,\n",
       "            260,     7,    23,     1]),\n",
       "  'input_text': 'and ken\\nton lee 2020 exploring unexplored generalization\\nchallenges for crossdatabase semantic parsi',\n",
       "  'expected_output': 'ng in\\nproceedings of the 58th annual meeting of th'},\n",
       " {'input_ids': tensor([    3,   172,   257,  2428,    21,  2269,  6757, 10925, 27632,   260,\n",
       "              7,    53,    16, 13339,    13,     8,     3,  3449,   189,  2041,\n",
       "           1338,    13,     3,   189,     1]),\n",
       "  'input_text': 'zation\\nchallenges for crossdatabase semantic parsing in\\nproceedings of the 58th annual meeting of th',\n",
       "  'expected_output': 'e asso\\nciation for computational linguistics  page'},\n",
       " {'input_ids': tensor([    3,  1725,    16, 13339,    13,     8,     3,  3449,   189,  2041,\n",
       "           1338,    13,     8,    38,     7,    32,     3,    75,    23,   257,\n",
       "             21, 25850,     3, 24703,     7,   543,     1]),\n",
       "  'input_text': 'ng in\\nproceedings of the 58th annual meeting of the asso\\nciation for computational linguistics  page',\n",
       "  'expected_output': 's 8372\\n8388 online association for computational l'},\n",
       " {'input_ids': tensor([    3,    15,    38,     7,    32,     3,    75,    23,   257,    21,\n",
       "          25850,     3, 24703,     7,  1688,     3,  4591,  5865,     3,  4591,\n",
       "           4060,   367,  6028,    21, 25850,     3,    40,     1]),\n",
       "  'input_text': 'e asso\\nciation for computational linguistics  pages 8372\\n8388 online association for computational l',\n",
       "  'expected_output': 'in\\nguistics\\nbailin wang richard shin xiaodong liu '},\n",
       " {'input_ids': tensor([    3,     7,     3,  4591,  5865,     3,  4591,  4060,   367,  6028,\n",
       "             21, 25850,     3,    40,    77,     3,  1744,  3040,     7, 15794,\n",
       "             77,     3, 17789,  2354,   986,     3,     7,  2907,     3, 19838,\n",
       "             32,    26,  2444,     3,    40,    23,    76,     1]),\n",
       "  'input_text': 's 8372\\n8388 online association for computational lin\\nguistics\\nbailin wang richard shin xiaodong liu ',\n",
       "  'expected_output': 'oleksandr\\npolozov and matthew richardson 2020 rats'},\n",
       " {'input_ids': tensor([   16,     3,  1744,  3040,     7, 15794,    77,     3, 17789,  2354,\n",
       "            986,     3,     7,  2907,     3, 19838,    32,    26,  2444,     3,\n",
       "             40,    23,    76,     3,    32,    40, 16789,   232,    52,     3,\n",
       "           3233, 20260,   208,    11,  6928,   532,   210,  2354,   986,   739,\n",
       "           6503, 20063,     1]),\n",
       "  'input_text': 'in\\nguistics\\nbailin wang richard shin xiaodong liu oleksandr\\npolozov and matthew richardson 2020 rats',\n",
       "  'expected_output': 'ql\\nrelationaware schema encoding and linking for\\nt'},\n",
       " {'input_ids': tensor([    3,    32,    40, 16789,   232,    52,     3,  3233, 20260,   208,\n",
       "             11,  6928,   532,   210,  2354,   986,   739,  6503, 20063,  1824,\n",
       "             40,  4689,     9,  3404, 26622,     3,    35,  9886,    11, 17988,\n",
       "             21,     3,    17,     1]),\n",
       "  'input_text': 'oleksandr\\npolozov and matthew richardson 2020 ratsql\\nrelationaware schema encoding and linking for\\nt',\n",
       "  'expected_output': 'exttosql parsers proceedings of the 58th annual\\nme'},\n",
       " {'input_ids': tensor([    3,  1824,    40,  4689,     9,  3404, 26622,     3,    35,  9886,\n",
       "             11, 17988,    21,  1499,   235,     7,  1824,    40,   260,     7,\n",
       "            277, 13339,    13,     8,     3,  3449,   189,  2041,   140,     1]),\n",
       "  'input_text': 'ql\\nrelationaware schema encoding and linking for\\ntexttosql parsers proceedings of the 58th annual\\nme',\n",
       "  'expected_output': 'eting of the association for computational lin\\ngui'},\n",
       " {'input_ids': tensor([    3, 10398,   235,     7,  1824,    40,   260,     7,   277, 13339,\n",
       "             13,     8,     3,  3449,   189,  2041,  1338,    13,     8,  6028,\n",
       "             21, 25850,     3,    40,    77,     3,  7938,     1]),\n",
       "  'input_text': 'exttosql parsers proceedings of the 58th annual\\nmeeting of the association for computational lin\\ngui',\n",
       "  'expected_output': 'stics \\npeng xu dhruv kumar wei yang wenjie zi keyi'},\n",
       " {'input_ids': tensor([    3,    15,  1222,    13,     8,  6028,    21, 25850,     3,    40,\n",
       "             77,     3,  1744,  3040,     7,   158,  1725,     3,   226,    76,\n",
       "              3,    26,   107,    52,    76,   208,     3,  2729,  1635,    62,\n",
       "             23,     3,    63,  1468,    62, 21391,    15,  3686,   843,    23,\n",
       "              1]),\n",
       "  'input_text': 'eting of the association for computational lin\\nguistics \\npeng xu dhruv kumar wei yang wenjie zi keyi',\n",
       "  'expected_output': '\\ntang chenyang huang jackie chi kit cheung si\\nmon '},\n",
       " {'input_ids': tensor([    3,     7,  7636,   158,  1725,     3,   226,    76,     3,    26,\n",
       "            107,    52,    76,   208,     3,  2729,  1635,    62,    23,     3,\n",
       "             63,  1468,    62, 21391,    15,  3686,   843,    23,     3,  8967,\n",
       "              3,  1559,    63,  1468,     3,   107,    76,  1468,     3,  9325,\n",
       "             23,    15,     3,  1436,  3650,     3,  1033,   425,   108,  1911,\n",
       "              1]),\n",
       "  'input_text': 'stics \\npeng xu dhruv kumar wei yang wenjie zi keyi\\ntang chenyang huang jackie chi kit cheung si\\nmon ',\n",
       "  'expected_output': 'jd prince and yanshuai cao 2021 optimiz\\ning deeper'},\n",
       " {'input_ids': tensor([    3,  8967,     3,  1559,    63,  1468,     3,   107,    76,  1468,\n",
       "              3,  9325,    23,    15,     3,  1436,  3650,     3,  1033,   425,\n",
       "            108,  1911,     3,   354,    26, 22277,    11,     3,    63,   152,\n",
       "          14279,     9,    23,   212,    32,   460,  2658, 19769,     3,    53,\n",
       "           7231,     1]),\n",
       "  'input_text': '\\ntang chenyang huang jackie chi kit cheung si\\nmon jd prince and yanshuai cao 2021 optimiz\\ning deeper',\n",
       "  'expected_output': ' transformers on small datasets in pro\\nceedings of'},\n",
       " {'input_ids': tensor([    3,   354,    26, 22277,    11,     3,    63,   152, 14279,     9,\n",
       "             23,   212,    32,   460,  2658, 19769,     3,    53,  7231, 19903,\n",
       "              7,    30,   422, 17953,     7,    16,   813,   197,    15,    26,\n",
       "             53,     7,    13,     1]),\n",
       "  'input_text': 'jd prince and yanshuai cao 2021 optimiz\\ning deeper transformers on small datasets in pro\\nceedings of',\n",
       "  'expected_output': ' the 59th annual meeting of the associa\\ntion for c'},\n",
       " {'input_ids': tensor([19903,     7,    30,   422, 17953,     7,    16,   813,   197,    15,\n",
       "             26,    53,     7,    13,     8,     3,  3390,   189,  2041,  1338,\n",
       "             13,     8,    38,  5444,     9,     3,  1575,    21,     3,    75,\n",
       "              1]),\n",
       "  'input_text': ' transformers on small datasets in pro\\nceedings of the 59th annual meeting of the associa\\ntion for c',\n",
       "  'expected_output': 'omputational linguistics and the 11th in\\nternation'},\n",
       " {'input_ids': tensor([    8,     3,  3390,   189,  2041,  1338,    13,     8,    38,  5444,\n",
       "              9,     3,  1575,    21, 25850,     3, 24703,     7,    11,     8,\n",
       "            850,   189,    16,     3,  2947,   257,     1]),\n",
       "  'input_text': ' the 59th annual meeting of the associa\\ntion for computational linguistics and the 11th in\\nternation',\n",
       "  'expected_output': 'al joint conference on natural language\\nprocessing'},\n",
       " {'input_ids': tensor([    3,    32, 31148,   138,     3, 24703,     7,    11,     8,   850,\n",
       "            189,    16,     3,   449, 16557,  4494,  2542,    30,   793,  1612,\n",
       "           3026,     1]),\n",
       "  'input_text': 'omputational linguistics and the 11th in\\nternational joint conference on natural language\\nprocessing',\n",
       "  'expected_output': ' volume 1 long papers  pages 2089\\n2102 online asso'},\n",
       " {'input_ids': tensor([  491,  4494,  2542,    30,   793,  1612,  3026,  2908,   209,   307,\n",
       "           5778,  1688,   460,  3914,   204, 14388,   367,    38,     7,    32,\n",
       "              1]),\n",
       "  'input_text': 'al joint conference on natural language\\nprocessing volume 1 long papers  pages 2089\\n2102 online asso',\n",
       "  'expected_output': 'ciation for computational lin\\nguistics\\npengcheng y'},\n",
       " {'input_ids': tensor([ 2908,   209,   307,  5778,  1688,   460,  3914,   204, 14388,   367,\n",
       "           6028,    21, 25850,     3,    40,    77,     3,  1744,  3040,     7,\n",
       "            158,  1725,  1559,   122,     3,    63,     1]),\n",
       "  'input_text': ' volume 1 long papers  pages 2089\\n2102 online association for computational lin\\nguistics\\npengcheng y',\n",
       "  'expected_output': 'in and graham neubig 2018 tranx a\\ntransitionbased '},\n",
       " {'input_ids': tensor([    3,    75,    23,   257,    21, 25850,     3,    40,    77,     3,\n",
       "           1744,  3040,     7,   158,  1725,  1559,   122,     3,    63,    77,\n",
       "             11,     3,  3484,  1483,  5854, 12911,   846,     3, 11665,   226,\n",
       "              3,     9,  3508,   390,     1]),\n",
       "  'input_text': 'ciation for computational lin\\nguistics\\npengcheng yin and graham neubig 2018 tranx a\\ntransitionbased ',\n",
       "  'expected_output': 'neural abstract syntax parser for se\\nmantic parsin'},\n",
       " {'input_ids': tensor([   16,    11,     3,  3484,  1483,  5854, 12911,   846,     3, 11665,\n",
       "            226,     3,     9,  3508,   390, 24228,  9838, 28230,   260,     7,\n",
       "             49,    21,   142,   388,  1225,   260,     7,    77,     1]),\n",
       "  'input_text': 'in and graham neubig 2018 tranx a\\ntransitionbased neural abstract syntax parser for se\\nmantic parsin',\n",
       "  'expected_output': 'g and code generation proceedings of\\nthe 2018 conf'},\n",
       " {'input_ids': tensor([24228,  9838, 28230,   260,     7,    49,    21,   142,   388,  1225,\n",
       "            260,     7,    53,    11,  1081,  3381, 13339,    13,     8,   846,\n",
       "              3,  9707,     1]),\n",
       "  'input_text': 'neural abstract syntax parser for se\\nmantic parsing and code generation proceedings of\\nthe 2018 conf',\n",
       "  'expected_output': 'erence on empirical methods in natu\\nral language p'},\n",
       " {'input_ids': tensor([    3,   122,    11,  1081,  3381, 13339,    13,     8,   846,  2542,\n",
       "             30, 23941,  2254,    16,     3,    29,   144,    76,     3,  4900,\n",
       "           1612,     3,   102,     1]),\n",
       "  'input_text': 'g and code generation proceedings of\\nthe 2018 conference on empirical methods in natu\\nral language p',\n",
       "  'expected_output': 'rocessing system demonstrations \\ntao yu rui zhang '},\n",
       " {'input_ids': tensor([    3,    49,  1433,    30, 23941,  2254,    16,     3,    29,   144,\n",
       "             76,     3,  4900,  1612,  3026,   358, 10686,     7,     3,    17,\n",
       "              9,    32,     3,    63,    76,     3,    52,    76,    23,     3,\n",
       "            172,  9270,     1]),\n",
       "  'input_text': 'erence on empirical methods in natu\\nral language processing system demonstrations \\ntao yu rui zhang ',\n",
       "  'expected_output': 'heyang er suyi li eric xue\\nbo pang xi victoria lin'},\n",
       " {'input_ids': tensor([    3,  7818,    15,     7,     7,    53,   358, 10686,     7,     3,\n",
       "             17,     9,    32,     3,    63,    76,     3,    52,    76,    23,\n",
       "              3,   172,  9270,     3, 13133,  1468,     3,    49,  2629,    63,\n",
       "             23,     3,    40,    23,     3,    15,  2234,     3,   226,    76,\n",
       "             15,  3005,  2131,   122,     3,   226,    23,     3,  7287,  3600,\n",
       "              9,     3,    40,    77,     1]),\n",
       "  'input_text': 'rocessing system demonstrations \\ntao yu rui zhang heyang er suyi li eric xue\\nbo pang xi victoria lin',\n",
       "  'expected_output': ' yi chern tan tianze\\nshi zihan li youxuan jiang mi'},\n",
       " {'input_ids': tensor([    3, 13133,  1468,     3,    49,  2629,    63,    23,     3,    40,\n",
       "             23,     3,    15,  2234,     3,   226,    76,    15,  3005,  2131,\n",
       "            122,     3,   226,    23,     3,  7287,  3600,     9,     3,    40,\n",
       "             77,     3,    63,    23,     3,  1703,    29,     3,    17,   152,\n",
       "              3, 12572,   776,     3,  5605,  3686,  2618,     3,    40,    23,\n",
       "             25,   226,    76,   152,     3,   354,    23,  1468,  1337,     1]),\n",
       "  'input_text': 'heyang er suyi li eric xue\\nbo pang xi victoria lin yi chern tan tianze\\nshi zihan li youxuan jiang mi',\n",
       "  'expected_output': 'chihiro yasunaga\\nsungrok shim tao chen alexander f'},\n",
       " {'input_ids': tensor([    3,    63,    23,     3,  1703,    29,     3,    17,   152,     3,\n",
       "          12572,   776,     3,  5605,  3686,  2618,     3,    40,    23,    25,\n",
       "            226,    76,   152,     3,   354,    23,  1468,  2278,    23,  9288,\n",
       "             32,     3,    63,     9,     7,   202,  4711,  1997,  3844,   157,\n",
       "              3,     7, 10813,     3,    17,     9,    32,     3,  1559,  1240,\n",
       "            226, 11849,     3,    89,     1]),\n",
       "  'input_text': ' yi chern tan tianze\\nshi zihan li youxuan jiang michihiro yasunaga\\nsungrok shim tao chen alexander f',\n",
       "  'expected_output': 'abbri zifan\\nli luyao chen yuwen zhang shreya dixit'},\n",
       " {'input_ids': tensor([    3,  1436,  9288,    32,     3,    63,     9,     7,   202,  4711,\n",
       "           1997,  3844,   157,     3,     7, 10813,     3,    17,     9,    32,\n",
       "              3,  1559,  1240,   226, 11849,     3, 12644,  2160,  3686, 12351,\n",
       "              3,    40,    23,     3,    40,    76,    63,     9,    32,     3,\n",
       "           1559,     3,    63,    76,   210,    35,     3,   172,  9270,  6660,\n",
       "             60,    63,     9, 20679,   155,     1]),\n",
       "  'input_text': 'chihiro yasunaga\\nsungrok shim tao chen alexander fabbri zifan\\nli luyao chen yuwen zhang shreya dixit',\n",
       "  'expected_output': ' vin\\ncent zhang caiming xiong richard socher wal\\nt'},\n",
       " {'input_ids': tensor([  703,  2160,  3686, 12351,     3,    40,    23,     3,    40,    76,\n",
       "             63,     9,    32,     3,  1559,     3,    63,    76,   210,    35,\n",
       "              3,   172,  9270,  6660,    60,    63,     9, 20679,   155,  4671,\n",
       "           3151,     3,   172,  9270,     3,    75, 19874,     3,   226,    23,\n",
       "           2444,  2354,   986,    78,  1703,     3,  5380,     3,    17,     1]),\n",
       "  'input_text': 'abbri zifan\\nli luyao chen yuwen zhang shreya dixit vin\\ncent zhang caiming xiong richard socher wal\\nt',\n",
       "  'expected_output': 'er lasecki and dragomir radev 2019 cosql a\\nconvers'},\n",
       " {'input_ids': tensor([ 4671,  3151,     3,   172,  9270,     3,    75, 19874,     3,   226,\n",
       "             23,  2444,  2354,   986,    78,  1703,     3,  5380,     3,   449,\n",
       "             50,     7,  5007,    23,    11,  5439,    32,  5884,     3, 15530,\n",
       "            208,  1360,   576,     7,  1824,    40,     3,     9, 20379,     1]),\n",
       "  'input_text': ' vin\\ncent zhang caiming xiong richard socher wal\\nter lasecki and dragomir radev 2019 cosql a\\nconvers',\n",
       "  'expected_output': 'ational texttosql challenge towards cross\\ndomain n'},\n",
       " {'input_ids': tensor([    3,    49,    50,     7,  5007,    23,    11,  5439,    32,  5884,\n",
       "              3, 15530,   208,  1360,   576,     7,  1824,    40,     3,     9,\n",
       "           3634,   138,  1499,   235,     7,  1824,    40,  1921,  1587,  2269,\n",
       "           3303,     3,    29,     1]),\n",
       "  'input_text': 'er lasecki and dragomir radev 2019 cosql a\\nconversational texttosql challenge towards cross\\ndomain n',\n",
       "  'expected_output': 'atural language interfaces to databases in\\nproceed'},\n",
       " {'input_ids': tensor([   44,  6318,  1499,   235,     7,  1824,    40,  1921,  1587,  2269,\n",
       "           3303,   793,  1612,  3459,     7,    12, 16961,    16,  8669,     1]),\n",
       "  'input_text': 'ational texttosql challenge towards cross\\ndomain natural language interfaces to databases in\\nproceed',\n",
       "  'expected_output': 'ings of the 2019 conference on empirical\\nmethods i'},\n",
       " {'input_ids': tensor([   44,  9709,  1612,  3459,     7,    12, 16961,    16, 13339,    13,\n",
       "              8,  1360,  2542,    30, 23941,  2254,     3,    23,     1]),\n",
       "  'input_text': 'atural language interfaces to databases in\\nproceedings of the 2019 conference on empirical\\nmethods i',\n",
       "  'expected_output': 'n natural language processing and the\\n9th internat'},\n",
       " {'input_ids': tensor([    3,    53,     7,    13,     8,  1360,  2542,    30, 23941,  2254,\n",
       "             16,   793,  1612,  3026,    11,     8,   668,   189,  9342,   144,\n",
       "              1]),\n",
       "  'input_text': 'ings of the 2019 conference on empirical\\nmethods in natural language processing and the\\n9th internat',\n",
       "  'expected_output': 'ional joint conference on natural lan\\nguage proces'},\n",
       " {'input_ids': tensor([   3,   29,  793, 1612, 3026,   11,    8,  668,  189, 1038, 4494, 2542,\n",
       "            30,  793,    3, 1618,    3, 1744,  545, 6345,    1]),\n",
       "  'input_text': 'n natural language processing and the\\n9th international joint conference on natural lan\\nguage proces',\n",
       "  'expected_output': 'sing emnlpijcnlp  pages 1962\\n1979 hong kong china '},\n",
       " {'input_ids': tensor([    3,  6318,  4494,  2542,    30,   793,     3,  1618,     3,  1744,\n",
       "            545,  3026,     3,    15,    51,    29,    40,   102,    23,   354,\n",
       "             75,    29,    40,   102,  1688, 20236, 15393,     3, 23001, 10447,\n",
       "            122, 20576,     1]),\n",
       "  'input_text': 'ional joint conference on natural lan\\nguage processing emnlpijcnlp  pages 1962\\n1979 hong kong china ',\n",
       "  'expected_output': 'association for computa\\ntional linguistics\\ntao yu '},\n",
       " {'input_ids': tensor([10159,     3,    15,    51,    29,    40,   102,    23,   354,    75,\n",
       "             29,    40,   102,  1688, 20236, 15393,     3, 23001, 10447,   122,\n",
       "          20576,  6028,    21,     3,   287,  2562,     9,     3,    17,  6318,\n",
       "              3, 24703,     7,     3,    17,     9,    32,     3,    63,    76,\n",
       "              1]),\n",
       "  'input_text': 'sing emnlpijcnlp  pages 1962\\n1979 hong kong china association for computa\\ntional linguistics\\ntao yu ',\n",
       "  'expected_output': 'rui zhang alex polozov christopher meek\\nand ahmed '},\n",
       " {'input_ids': tensor([ 6028,    21,     3,   287,  2562,     9,     3,    17,  6318,     3,\n",
       "          24703,     7,     3,    17,     9,    32,     3,    63,    76,     3,\n",
       "             52,    76,    23,     3,   172,  9270,  1240,   226,     3,  3233,\n",
       "          20260,   208,     3, 15294, 10775,    49,   140,    15,   157,    11,\n",
       "              3,     9,   107,  2726,     1]),\n",
       "  'input_text': 'association for computa\\ntional linguistics\\ntao yu rui zhang alex polozov christopher meek\\nand ahmed ',\n",
       "  'expected_output': 'hassan awadallah 2021 score pre\\ntraining for conte'},\n",
       " {'input_ids': tensor([    3,    52,    76,    23,     3,   172,  9270,  1240,   226,     3,\n",
       "           3233, 20260,   208,     3, 15294, 10775,    49,   140,    15,   157,\n",
       "             11,     3,     9,   107,  2726,    65,     7,   152,     3,  7396,\n",
       "             26, 30157,   460,  2658,  2604,   554,   761,    21,  3622,    15,\n",
       "              1]),\n",
       "  'input_text': 'rui zhang alex polozov christopher meek\\nand ahmed hassan awadallah 2021 score pre\\ntraining for conte',\n",
       "  'expected_output': 'xt representation in conversational\\nsemantic parsi'},\n",
       " {'input_ids': tensor([   65,     7,   152,     3,  7396,    26, 30157,   460,  2658,  2604,\n",
       "            554,   761,    21,  2625,  6497,    16,  3634,   138, 27632,   260,\n",
       "              7,    23,     1]),\n",
       "  'input_text': 'hassan awadallah 2021 score pre\\ntraining for context representation in conversational\\nsemantic parsi',\n",
       "  'expected_output': 'ng in international conference on\\nlearning represe'},\n",
       " {'input_ids': tensor([    3,   226,    17,  6497,    16,  3634,   138, 27632,   260,     7,\n",
       "             53,    16,  1038,  2542,    30,  1036,     3,    60,  7197,    15,\n",
       "              1]),\n",
       "  'input_text': 'xt representation in conversational\\nsemantic parsing in international conference on\\nlearning represe',\n",
       "  'expected_output': 'ntations 9901tao yu rui zhang kai yang michihiro y'},\n",
       " {'input_ids': tensor([    3,  1725,    16,  1038,  2542,    30,  1036,  6497,     7, 12185,\n",
       "           4542,    17,     9,    32,     3,    63,    76,     3,    52,    76,\n",
       "             23,     3,   172,  9270,     3,  1258,    23,     3,    63,  1468,\n",
       "           2278,    23,  9288,    32,     3,    63,     1]),\n",
       "  'input_text': 'ng in international conference on\\nlearning representations 9901tao yu rui zhang kai yang michihiro y',\n",
       "  'expected_output': 'asunaga\\ndongxu wang zifan li james ma irene li qin'},\n",
       " {'input_ids': tensor([    3,    29,    17,  1628, 12185,  4542,    17,     9,    32,     3,\n",
       "             63,    76,     3,    52,    76,    23,     3,   172,  9270,     3,\n",
       "           1258,    23,     3,    63,  1468,  2278,    23,  9288,    32,     3,\n",
       "             63,     9,     7,   202,  4711,   278,   122,   226,    76,     3,\n",
       "          17789,  3686, 12351,     3,    40,    23,  7620,    15,     7,   954,\n",
       "              3,    23,  1536,    15,     3,    40,    23,     3,  1824,    77,\n",
       "              1]),\n",
       "  'input_text': 'ntations 9901tao yu rui zhang kai yang michihiro yasunaga\\ndongxu wang zifan li james ma irene li qin',\n",
       "  'expected_output': 'gn\\ning yao shanelle roman and et al 2018 spider a\\n'},\n",
       " {'input_ids': tensor([   38,   202,  4711,   278,   122,   226,    76,     3, 17789,  3686,\n",
       "          12351,     3,    40,    23,  7620,    15,     7,   954,     3,    23,\n",
       "           1536,    15,     3,    40,    23,     3,  1824,    53,    29,     3,\n",
       "             53,     3,    63,     9,    32,     3,     7,  2618,   693,  3408,\n",
       "             11,     3,    15,    17,   491,   846, 18612,     3,     9,     1]),\n",
       "  'input_text': 'asunaga\\ndongxu wang zifan li james ma irene li qingn\\ning yao shanelle roman and et al 2018 spider a\\n',\n",
       "  'expected_output': 'largescale humanlabeled dataset for complex and\\ncr'},\n",
       " {'input_ids': tensor([    3,   122,    29,     3,    53,     3,    63,     9,    32,     3,\n",
       "              7,  2618,   693,  3408,    11,     3,    15,    17,   491,   846,\n",
       "          18612,     3,     9,   508,  6649,   936,  9339,   400,    26, 17953,\n",
       "             21,  1561,    11,  5764,     1]),\n",
       "  'input_text': 'gn\\ning yao shanelle roman and et al 2018 spider a\\nlargescale humanlabeled dataset for complex and\\ncr',\n",
       "  'expected_output': 'ossdomain semantic parsing and texttosql task\\nproc'},\n",
       " {'input_ids': tensor([  508,  6649,   936,  9339,   400,    26, 17953,    21,  1561,    11,\n",
       "           2269, 22999, 27632,   260,     7,    53,    11,  1499,   235,     7,\n",
       "           1824,    40,  2491,   813,    75,     1]),\n",
       "  'input_text': 'largescale humanlabeled dataset for complex and\\ncrossdomain semantic parsing and texttosql task\\nproc',\n",
       "  'expected_output': 'eedings of the 2018 conference on empirical\\nmethod'},\n",
       " {'input_ids': tensor([    3,    32,     7,     7, 22999, 27632,   260,     7,    53,    11,\n",
       "           1499,   235,     7,  1824,    40,  2491, 13339,    13,     8,   846,\n",
       "           2542,    30, 23941,  1573,     1]),\n",
       "  'input_text': 'ossdomain semantic parsing and texttosql task\\nproceedings of the 2018 conference on empirical\\nmethod',\n",
       "  'expected_output': 's in natural language processing \\nruiqi zhong tao '},\n",
       " {'input_ids': tensor([    3,  6958,    53,     7,    13,     8,   846,  2542,    30, 23941,\n",
       "           2254,    16,   793,  1612,  3026,     3,    52,    76,    23,  1824,\n",
       "             23,     3,   172, 23001,     3,    17,     9,    32,     1]),\n",
       "  'input_text': 'eedings of the 2018 conference on empirical\\nmethods in natural language processing \\nruiqi zhong tao ',\n",
       "  'expected_output': 'yu and dan klein 2020 seman\\ntic evaluation for tex'},\n",
       " {'input_ids': tensor([    3,     7,    16,   793,  1612,  3026,     3,    52,    76,    23,\n",
       "           1824,    23,     3,   172, 23001,     3,    17,     9,    32,     3,\n",
       "             63,    76,    11,     3,  3768, 21856,  6503,   142,   348,     3,\n",
       "           1225,  5002,    21,     3, 10354,     1]),\n",
       "  'input_text': 's in natural language processing \\nruiqi zhong tao yu and dan klein 2020 seman\\ntic evaluation for tex',\n",
       "  'expected_output': 'ttosql with distilled test suites\\nproceedings of t'},\n",
       " {'input_ids': tensor([    3,    63,    76,    11,     3,  3768, 21856,  6503,   142,   348,\n",
       "              3,  1225,  5002,    21,  1499,   235,     7,  1824,    40,    28,\n",
       "              3, 31703,   794,  3132,     7, 13339,    13,     3,    17,     1]),\n",
       "  'input_text': 'yu and dan klein 2020 seman\\ntic evaluation for texttosql with distilled test suites\\nproceedings of t',\n",
       "  'expected_output': 'he 2020 conference on empirical\\nmethods in natural'},\n",
       " {'input_ids': tensor([    3,    17,   235,     7,  1824,    40,    28,     3, 31703,   794,\n",
       "           3132,     7, 13339,    13,     8,  6503,  2542,    30, 23941,  2254,\n",
       "             16,   793,     1]),\n",
       "  'input_text': 'ttosql with distilled test suites\\nproceedings of the 2020 conference on empirical\\nmethods in natural',\n",
       "  'expected_output': ' language processing emnlp retrieval\\naugmented\\ngen'},\n",
       " {'input_ids': tensor([    3,    88,  6503,  2542,    30, 23941,  2254,    16,   793,  1612,\n",
       "           3026,     3,    15,    51,    29,    40,   102, 24515,   138,     3,\n",
       "          28984,     3,   729,     1]),\n",
       "  'input_text': 'he 2020 conference on empirical\\nmethods in natural language processing emnlp retrieval\\naugmented\\ngen',\n",
       "  'expected_output': 'eration\\nabhinav kimothia simple introductiontable '},\n",
       " {'input_ids': tensor([ 1612,  3026,     3,    15,    51,    29,    40,   102, 24515,   138,\n",
       "              3, 28984,  3381,   703,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436,     9,   650,  5302,  3869,     1]),\n",
       "  'input_text': ' language processing emnlp retrieval\\naugmented\\ngeneration\\nabhinav kimothia simple introductiontable ',\n",
       "  'expected_output': 'of contents\\nabhinav kimothi01 what is rag  3\\n02 ho'},\n",
       " {'input_ids': tensor([    3,    49,   257,   703,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436,     9,   650,  5302,  3869,    13, 10223,   703,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436,  4542,   125,    19,     3,  6151,\n",
       "            220, 11270,  3534,     1]),\n",
       "  'input_text': 'eration\\nabhinav kimothia simple introductiontable of contents\\nabhinav kimothi01 what is rag  3\\n02 ho',\n",
       "  'expected_output': 'w does rag help  6\\n03 what are some popular rag us'},\n",
       " {'input_ids': tensor([   13, 10223,   703,  2907,     9,   208,     3, 19754,    32,  7436,\n",
       "           4542,   125,    19,     3,  6151,   220, 11270,   149,   405,     3,\n",
       "           6151,   199,   431, 12811,   125,    33,   128,  1012,     3,  6151,\n",
       "            178,     1]),\n",
       "  'input_text': 'of contents\\nabhinav kimothi01 what is rag  3\\n02 how does rag help  6\\n03 what are some popular rag us',\n",
       "  'expected_output': 'e cases  7\\n04 rag architecture  8\\n        i indexi'},\n",
       " {'input_ids': tensor([    3,   210,   405,     3,  6151,   199,   431, 12811,   125,    33,\n",
       "            128,  1012,     3,  6151,   169,  1488,   489, 11484,     3,  6151,\n",
       "           4648,   505,     3,    23,  5538,    23,     1]),\n",
       "  'input_text': 'w does rag help  6\\n03 what are some popular rag use cases  7\\n04 rag architecture  8\\n        i indexi',\n",
       "  'expected_output': 'ng pipeline  9\\n            a data loading  10\\n    '},\n",
       " {'input_ids': tensor([    3,    15,  1488,   489, 11484,     3,  6151,  4648,   505,     3,\n",
       "             23,  5538,    53, 12045,   668,     3,     9,   331, 12115,   335,\n",
       "              1]),\n",
       "  'input_text': 'e cases  7\\n04 rag architecture  8\\n        i indexing pipeline  9\\n            a data loading  10\\n    ',\n",
       "  'expected_output': '        b document splitting  14\\n            c emb'},\n",
       " {'input_ids': tensor([    3,  1725, 12045,   668,     3,     9,   331, 12115,   335,     3,\n",
       "            115,  1708, 28503,   968,     3,    75, 10960,     1]),\n",
       "  'input_text': 'ng pipeline  9\\n            a data loading  10\\n            b document splitting  14\\n            c emb',\n",
       "  'expected_output': 'edding  23\\n            d vector stores  29\\n       '},\n",
       " {'input_ids': tensor([    3,   115,  1708, 28503,   968,     3,    75, 25078,    26,    53,\n",
       "           1902,     3,    26, 12938,  3253,  2838,     1]),\n",
       "  'input_text': '        b document splitting  14\\n            c embedding  23\\n            d vector stores  29\\n       ',\n",
       "  'expected_output': ' ii rag pipeline  35\\n            a retrieval  37 \\n'},\n",
       " {'input_ids': tensor([    3,    15,  7249,  1902,     3,    26, 12938,  3253,  2838,     3,\n",
       "             23,    23,     3,  6151, 12045,  3097,     3,     9, 24515,   138,\n",
       "           6862,     1]),\n",
       "  'input_text': 'edding  23\\n            d vector stores  29\\n        ii rag pipeline  35\\n            a retrieval  37 \\n',\n",
       "  'expected_output': '            b augmentation and generation  45\\n05 e'},\n",
       " {'input_ids': tensor([    3,    23,    23,     3,  6151, 12045,  3097,     3,     9, 24515,\n",
       "            138,  6862,     3,   115,     3, 19260,    11,  3381,  3479,     3,\n",
       "           3076,     3,    15,     1]),\n",
       "  'input_text': ' ii rag pipeline  35\\n            a retrieval  37 \\n            b augmentation and generation  45\\n05 e',\n",
       "  'expected_output': 'valuation  46\\n06 rag vs finetuning  56\\n07 evolving'},\n",
       " {'input_ids': tensor([    3,   115,     3, 19260,    11,  3381,  3479,     3,  3076,  5002,\n",
       "           9668, 13574,     3,  6151,     3,   208,     7,  1399,    17,   202,\n",
       "             53, 11526, 10668, 16556,     1]),\n",
       "  'input_text': '            b augmentation and generation  45\\n05 evaluation  46\\n06 rag vs finetuning  56\\n07 evolving',\n",
       "  'expected_output': ' rag llmops stack  59\\n08 multimodal rag  63\\n09 pro'},\n",
       " {'input_ids': tensor([18662,  9668, 13574,     3,  6151,     3,   208,     7,  1399,    17,\n",
       "            202,    53, 11526, 10668, 16556,     3,  6151,     3,   195,    51,\n",
       "           9280,  9013,     3,  3390, 12046,  1249, 20226,     3,  6151,     3,\n",
       "           3891, 14146,   813,     1]),\n",
       "  'input_text': 'valuation  46\\n06 rag vs finetuning  56\\n07 evolving rag llmops stack  59\\n08 multimodal rag  63\\n09 pro',\n",
       "  'expected_output': 'gression of rag systems  66\\n        i naive rag  6'},\n",
       " {'input_ids': tensor([    3,  6151,     3,   195,    51,  9280,  9013,     3,  3390, 12046,\n",
       "           1249, 20226,     3,  6151,     3,  3891, 14146, 13324,    13,     3,\n",
       "           6151,  1002,     3,  3539,     3,    23,     3,    29,     9,   757,\n",
       "              3,  6151,   431,     1]),\n",
       "  'input_text': ' rag llmops stack  59\\n08 multimodal rag  63\\n09 progression of rag systems  66\\n        i naive rag  6',\n",
       "  'expected_output': '6\\n        ii advanced rag  67\\n        iii multimod'},\n",
       " {'input_ids': tensor([    3, 22430,    13,     3,  6151,  1002,     3,  3539,     3,    23,\n",
       "              3,    29,     9,   757,     3,  6151,     3,  3539,     3,    23,\n",
       "             23,  2496,     3,  6151,     3,  3708,     3,    23,    23,    23,\n",
       "           1249,  7360,     1]),\n",
       "  'input_text': 'gression of rag systems  66\\n        i naive rag  66\\n        ii advanced rag  67\\n        iii multimod',\n",
       "  'expected_output': 'al rag  71\\n10 acknowledgements  73\\n11 resources  7'},\n",
       " {'input_ids': tensor([  431,     3,    23,    23,  2496,     3,  6151,     3,  3708,     3,\n",
       "             23,    23,    23,  1249, 20226,     3,  6151,     3,  4450,   335,\n",
       "           8406,  4128,     3,  4552,   850,  1438,   489,     1]),\n",
       "  'input_text': '6\\n        ii advanced rag  67\\n        iii multimodal rag  71\\n10 acknowledgements  73\\n11 resources  7',\n",
       "  'expected_output': '4\\nkeep calm  build ai2generative ai large language'},\n",
       " {'input_ids': tensor([  491,     3,  6151,     3,  4450,   335,  8406,  4128,     3,  4552,\n",
       "            850,  1438,     3,  4581,   453,  4447,   918,     3,     9,    23,\n",
       "            357, 25181,     3,     9,    23,   508,  1612,     1]),\n",
       "  'input_text': 'al rag  71\\n10 acknowledgements  73\\n11 resources  74\\nkeep calm  build ai2generative ai large language',\n",
       "  'expected_output': ' models\\n20221106 20231105020406080100\\nabhinav kimo'},\n",
       " {'input_ids': tensor([  314,   453,  4447,   918,     3,     9,    23,   357, 25181,     3,\n",
       "              9,    23,   508,  1612,  2250,     3, 19818,  2658, 16431,   460,\n",
       "           2773, 19277,  1752,  1755,  2445,  3328,  2079,  2915,   703,  2907,\n",
       "              9,   208,     3, 19754,    32,     1]),\n",
       "  'input_text': '4\\nkeep calm  build ai2generative ai large language models\\n20221106 20231105020406080100\\nabhinav kimo',\n",
       "  'expected_output': 'thiwhat is rag\\nwhat is rag\\ngoogle trends  interest'},\n",
       " {'input_ids': tensor([ 2250,     3, 19818,  2658, 16431,   460,  2773, 19277,  1752,  1755,\n",
       "           2445,  3328,  2079,  2915,   703,  2907,     9,   208,     3, 19754,\n",
       "             32,  7436,  9170,    19,     3,  6151,   125,    19,     3,  6151,\n",
       "          10283,  5001,  1046,     1]),\n",
       "  'input_text': ' models\\n20221106 20231105020406080100\\nabhinav kimothiwhat is rag\\nwhat is rag\\ngoogle trends  interest',\n",
       "  'expected_output': ' over time nov22 to nov23 30th november 2022 will '},\n",
       " {'input_ids': tensor([    3,  7436,  9170,    19,     3,  6151,   125,    19,     3,  6151,\n",
       "          10283,  5001,  1046,   147,    97,     3,  5326,  2884,    12,     3,\n",
       "           5326,  2773,   604,   189,     3,  5326, 18247,   460,  2884,    56,\n",
       "              1]),\n",
       "  'input_text': 'thiwhat is rag\\nwhat is rag\\ngoogle trends  interest over time nov22 to nov23 30th november 2022 will ',\n",
       "  'expected_output': 'be remembered as the watershed moment in artificia'},\n",
       " {'input_ids': tensor([  147,    97,     3,  5326,  2884,    12,     3,  5326,  2773,   604,\n",
       "            189,     3,  5326, 18247,   460,  2884,    56,    36, 14206,    38,\n",
       "              8,   387,  7420,   798,    16,   768,  3286,    23,     9,     1]),\n",
       "  'input_text': ' over time nov22 to nov23 30th november 2022 will be remembered as the watershed moment in artificia',\n",
       "  'expected_output': 'l\\nintelligence openai released chatgpt and the wor'},\n",
       " {'input_ids': tensor([   36, 14206,    38,     8,   387,  7420,   798,    16,  7353,  6123,\n",
       "            539,     9,    23,  1883,  3582,   122,   102,    17,    11,     8,\n",
       "           2275,    52,     1]),\n",
       "  'input_text': 'be remembered as the watershed moment in artificial\\nintelligence openai released chatgpt and the wor',\n",
       "  'expected_output': 'ld was mesmerised interest in\\npreviously obscure t'},\n",
       " {'input_ids': tensor([    3,    40,  6123,   539,     9,    23,  1883,  3582,   122,   102,\n",
       "             17,    11,     8,   296,    47,   140,     7,   935,  3375,  1046,\n",
       "             16,  3150, 21634,     3,    17,     1]),\n",
       "  'input_text': 'l\\nintelligence openai released chatgpt and the world was mesmerised interest in\\npreviously obscure t',\n",
       "  'expected_output': 'erms like generative ai and large language models '},\n",
       " {'input_ids': tensor([    3,    40,    26,    47,   140,     7,   935,  3375,  1046,    16,\n",
       "           3150, 21634,  1353,   114,     3, 25181,     3,     9,    23,    11,\n",
       "            508,  1612,  2250,     1]),\n",
       "  'input_text': 'ld was mesmerised interest in\\npreviously obscure terms like generative ai and large language models ',\n",
       "  'expected_output': 'llms\\nwas unstoppable over the following 12 months\\n'},\n",
       " {'input_ids': tensor([    3,    49,    51,     7,   114,     3, 25181,     3,     9,    23,\n",
       "             11,   508,  1612,  2250,     3,   195,    51,     7,    47,    73,\n",
       "           7618,   102,   179,   147,     8,   826,   586,   767,     1]),\n",
       "  'input_text': 'erms like generative ai and large language models llms\\nwas unstoppable over the following 12 months\\n',\n",
       "  'expected_output': 'the curse of the llms\\nas usage exploded so did the'},\n",
       " {'input_ids': tensor([    3,   195,    51,     7,    47,    73,  7618,   102,   179,   147,\n",
       "              8,   826,   586,   767,     8, 19375,    13,     8,     3,   195,\n",
       "             51,     7,    38,  4742,     3, 29155,    78,   410,     8,     1]),\n",
       "  'input_text': 'llms\\nwas unstoppable over the following 12 months\\nthe curse of the llms\\nas usage exploded so did the',\n",
       "  'expected_output': ' expectations many users started using chatgpt as\\n'},\n",
       " {'input_ids': tensor([    8, 19375,    13,     8,     3,   195,    51,     7,    38,  4742,\n",
       "              3, 29155,    78,   410,     8,  4454,   186,  1105,   708,   338,\n",
       "           3582,   122,   102,    17,    38,     1]),\n",
       "  'input_text': 'the curse of the llms\\nas usage exploded so did the expectations many users started using chatgpt as\\n',\n",
       "  'expected_output': 'a source of information like an alternative to goo'},\n",
       " {'input_ids': tensor([4454,  186, 1105,  708,  338, 3582,  122,  102,   17,   38,    3,    9,\n",
       "          1391,   13,  251,  114,   46, 2433,   12,  281,   32,    1]),\n",
       "  'input_text': ' expectations many users started using chatgpt as\\na source of information like an alternative to goo',\n",
       "  'expected_output': 'gle as a result they also started\\nencountering pro'},\n",
       " {'input_ids': tensor([    3,     9,  1391,    13,   251,   114,    46,  2433,    12, 10283,\n",
       "             38,     3,     9,   741,    79,    92,   708,  6326,    53,   813,\n",
       "              1]),\n",
       "  'input_text': 'a source of information like an alternative to google as a result they also started\\nencountering pro',\n",
       "  'expected_output': 'minent weaknesses of the system concerns around co'},\n",
       " {'input_ids': tensor([    3,  3537,    38,     3,     9,   741,    79,    92,   708,  6326,\n",
       "             53,  8304, 21506,    13,     8,   358,  3315,   300,   576,     1]),\n",
       "  'input_text': 'gle as a result they also started\\nencountering prominent weaknesses of the system concerns around co',\n",
       "  'expected_output': 'pyright\\nprivacy security ability to do mathematica'},\n",
       " {'input_ids': tensor([ 3519,   295, 21506,    13,     8,   358,  3315,   300,  2405,  3535,\n",
       "           4570,  1034,  1418,    12,   103,  7270,    15,  4992,     9,     1]),\n",
       "  'input_text': 'minent weaknesses of the system concerns around copyright\\nprivacy security ability to do mathematica',\n",
       "  'expected_output': 'l calculations etc aside people\\nrealised that ther'},\n",
       " {'input_ids': tensor([    3, 20455,  2632,  4570,  1034,  1418,    12,   103, 18913, 19868,\n",
       "            672,  5915,   151,     3, 19007,    24,     8,    52,     1]),\n",
       "  'input_text': 'pyright\\nprivacy security ability to do mathematical calculations etc aside people\\nrealised that ther',\n",
       "  'expected_output': 'e are two major limitations of large language mode'},\n",
       " {'input_ids': tensor([    3,    40, 19868,   672,  5915,   151,     3, 19007,    24,   132,\n",
       "             33,   192,   779, 10005,    13,   508,  1612,  2175,     1]),\n",
       "  'input_text': 'l calculations etc aside people\\nrealised that there are two major limitations of large language mode',\n",
       "  'expected_output': 'ls\\na knowledge cutoff date hallucinations\\nusers lo'},\n",
       " {'input_ids': tensor([    3,    15,    33,   192,   779, 10005,    13,   508,  1612,  2250,\n",
       "              3,     9,  1103,  1340,  1647,   833,  6358,  6809,    29,  1628,\n",
       "           1105,  6899,     1]),\n",
       "  'input_text': 'e are two major limitations of large language models\\na knowledge cutoff date hallucinations\\nusers lo',\n",
       "  'expected_output': 'ok at llms for knowledge and wisdom yet llms\\nare s'},\n",
       " {'input_ids': tensor([   3,   40,    7,    3,    9, 1103, 1340, 1647,  833, 6358, 6809,   29,\n",
       "          1628, 1105,  320,   44,    3,  195,   51,    7,   21, 1103,   11, 8963,\n",
       "           780,    3,  195,   51,    7,   33,    3,    7,    1]),\n",
       "  'input_text': 'ls\\na knowledge cutoff date hallucinations\\nusers look at llms for knowledge and wisdom yet llms\\nare s',\n",
       "  'expected_output': 'ophisticated predictors of what word comes nexttra'},\n",
       " {'input_ids': tensor([   3, 1825,   44,    3,  195,   51,    7,   21, 1103,   11, 8963,  780,\n",
       "             3,  195,   51,    7,   33, 8732, 9689,  127,    7,   13,  125, 1448,\n",
       "           639,  416, 1313,    1]),\n",
       "  'input_text': 'ok at llms for knowledge and wisdom yet llms\\nare sophisticated predictors of what word comes nexttra',\n",
       "  'expected_output': 'ining an llm is an expensive and\\ntimeconsuming pro'},\n",
       " {'input_ids': tensor([    3, 10775,  3040,   920,  9689,   127,     7,    13,   125,  1448,\n",
       "            639,   416, 13023,    46,     3,   195,    51,    19,    46,  2881,\n",
       "             11,    97, 10862,   813,     1]),\n",
       "  'input_text': 'ophisticated predictors of what word comes nexttraining an llm is an expensive and\\ntimeconsuming pro',\n",
       "  'expected_output': 'cess llms are\\ntrained on massive amount of data th'},\n",
       " {'input_ids': tensor([   16,    53,    46,     3,   195,    51,    19,    46,  2881,    11,\n",
       "             97, 10862,   433,     3,   195,    51,     7,    33,  4252,    30,\n",
       "           3805,   866,    13,   331,     3,   189,     1]),\n",
       "  'input_text': 'ining an llm is an expensive and\\ntimeconsuming process llms are\\ntrained on massive amount of data th',\n",
       "  'expected_output': 'e\\ndata that llms are trained on is\\ntherefore histo'},\n",
       " {'input_ids': tensor([1830,    7,    3,  195,   51,    7,   33, 4252,   30, 3805,  866,   13,\n",
       "           331,    8,  331,   24,    3,  195,   51,    7,   33, 4252,   30,   19,\n",
       "          2459,  112,  235,    1]),\n",
       "  'input_text': 'cess llms are\\ntrained on massive amount of data the\\ndata that llms are trained on is\\ntherefore histo',\n",
       "  'expected_output': 'rical or dated \\neg the latest gpt4 model by openai'},\n",
       " {'input_ids': tensor([    3,    15,   331,    24,     3,   195,    51,     7,    33,  4252,\n",
       "             30,    19,  2459,  4332,    42,     3, 14134,     3,    15,   122,\n",
       "              8,  1251,     3,   122,   102,    17,   591,   825,    57,   539,\n",
       "              9,    23,     1]),\n",
       "  'input_text': 'e\\ndata that llms are trained on is\\ntherefore historical or dated \\neg the latest gpt4 model by openai',\n",
       "  'expected_output': '\\nhas knowledge only till april 2023 and\\nany event '},\n",
       " {'input_ids': tensor([    3,    52,  1950,    42,     3, 14134,     3,    15,   122,     8,\n",
       "           1251,     3,   122,   102,    17,   591,   825,    57,   539,     9,\n",
       "             23,    65,  1103,   163,  6501,     3,     9,  2246,    40,   460,\n",
       "           2773,    11,   136,   605,     1]),\n",
       "  'input_text': 'rical or dated \\neg the latest gpt4 model by openai\\nhas knowledge only till april 2023 and\\nany event ',\n",
       "  'expected_output': 'that happened post that\\ndate the information is no'},\n",
       " {'input_ids': tensor([  65, 1103,  163, 6501,    3,    9, 2246,   40,  460, 2773,   11,  136,\n",
       "           605,   24, 2817,  442,   24,  833,    8,  251,   19,  150,    1]),\n",
       "  'input_text': '\\nhas knowledge only till april 2023 and\\nany event that happened post that\\ndate the information is no',\n",
       "  'expected_output': 't available to\\nthe modeloften it was observed that'},\n",
       " {'input_ids': tensor([  24, 2817,  442,   24,  833,    8,  251,   19,   59,  347,   12,    8,\n",
       "           825,  858,  324,   34,   47, 6970,   24,    1]),\n",
       "  'input_text': 'that happened post that\\ndate the information is not available to\\nthe modeloften it was observed that',\n",
       "  'expected_output': ' llms\\nprovided responses that were factually\\nincor'},\n",
       " {'input_ids': tensor([   3,   17,  347,   12,    8,  825,  858,  324,   34,   47, 6970,   24,\n",
       "             3,  195,   51,    7,  937, 7216,   24,  130,  685,   76, 1427,   16,\n",
       "          5715,    1]),\n",
       "  'input_text': 't available to\\nthe modeloften it was observed that llms\\nprovided responses that were factually\\nincor',\n",
       "  'expected_output': 'rect despite being factually\\nincorrect the llm res'},\n",
       " {'input_ids': tensor([    3,   195,    51,     7,   937,  7216,    24,   130,   685,    76,\n",
       "           1427, 12153,     3,  3565,   271,   685,    76,  1427, 12153,     8,\n",
       "              3,   195,    51,     3,    60,     7,     1]),\n",
       "  'input_text': ' llms\\nprovided responses that were factually\\nincorrect despite being factually\\nincorrect the llm res',\n",
       "  'expected_output': 'ponses\\nsounded extremely confident and\\nlegitimate '},\n",
       " {'input_ids': tensor([    3, 12621,     3,  3565,   271,   685,    76,  1427, 12153,     8,\n",
       "              3,   195,    51,  7216,     3,     7, 14471,  2033,  4881,    11,\n",
       "          12372,     1]),\n",
       "  'input_text': 'rect despite being factually\\nincorrect the llm responses\\nsounded extremely confident and\\nlegitimate ',\n",
       "  'expected_output': 'this characteristic of  lying\\nwith confidence prov'},\n",
       " {'input_ids': tensor([    3,  5041,  2260,     3,     7, 14471,  2033,  4881,    11, 12372,\n",
       "             48, 16115,    13, 12267,    28,  3410,   813,   208,     1]),\n",
       "  'input_text': 'ponses\\nsounded extremely confident and\\nlegitimate this characteristic of  lying\\nwith confidence prov',\n",
       "  'expected_output': 'ed to be one of\\nthe biggest criticisms of chatgpt '},\n",
       " {'input_ids': tensor([   48, 16115,    13, 12267,    28,  3410,  9193,    12,    36,    80,\n",
       "             13,     8,  2630, 12334,     7,    13,  3582,   122,   102,    17,\n",
       "              1]),\n",
       "  'input_text': 'this characteristic of  lying\\nwith confidence proved to be one of\\nthe biggest criticisms of chatgpt ',\n",
       "  'expected_output': 'and\\nllm techniques in generalretrieval augmented g'},\n",
       " {'input_ids': tensor([    3,    15,    26,    12,    36,    80,    13,     8,  2630, 12334,\n",
       "              7,    13,  3582,   122,   102,    17,    11,     3,   195,    51,\n",
       "           2097,    16,   879,    60,  1788,    15,  2165,     3, 28984,     3,\n",
       "            122,     1]),\n",
       "  'input_text': 'ed to be one of\\nthe biggest criticisms of chatgpt and\\nllm techniques in generalretrieval augmented g',\n",
       "  'expected_output': 'eneration\\nkeep calm  build ai3while model retraini'},\n",
       " {'input_ids': tensor([   11,     3,   195,    51,  2097,    16,   879,    60,  1788,    15,\n",
       "           2165,     3, 28984,  3381,   453,  4447,   918,     3,     9,    23,\n",
       "            519, 12124,   109,   825,     3,    60,  9719,    23,     1]),\n",
       "  'input_text': 'and\\nllm techniques in generalretrieval augmented generation\\nkeep calm  build ai3while model retraini',\n",
       "  'expected_output': 'ngfinetuningreinforcement learning are options tha'},\n",
       " {'input_ids': tensor([    3,    35,    49,   257,   453,  4447,   918,     3,     9,    23,\n",
       "            519, 12124,   109,   825,     3,    60, 13023, 13536,    17,   202,\n",
       "             53,    60,    77, 10880,   297,  1036,    33,   931,     3,   189,\n",
       "              9,     1]),\n",
       "  'input_text': 'eneration\\nkeep calm  build ai3while model retrainingfinetuningreinforcement learning are options tha',\n",
       "  'expected_output': 't solve\\nthe aforementioned challenges these approa'},\n",
       " {'input_ids': tensor([    3,  1725, 13536,    17,   202,    53,    60,    77, 10880,   297,\n",
       "           1036,    33,   931,    24,  4602,     8,     3,     9, 22835,  2428,\n",
       "            175,     3, 12497,     9,     1]),\n",
       "  'input_text': 'ngfinetuningreinforcement learning are options that solve\\nthe aforementioned challenges these approa',\n",
       "  'expected_output': 'ches are timeconsuming and\\ncostly in majority of t'},\n",
       " {'input_ids': tensor([    3,    17,  4602,     8,     3,     9, 22835,  2428,   175,  6315,\n",
       "             33,    97, 10862,    11, 11855,    16,  2942,    13,     3,    17,\n",
       "              1]),\n",
       "  'input_text': 't solve\\nthe aforementioned challenges these approaches are timeconsuming and\\ncostly in majority of t',\n",
       "  'expected_output': 'he usecase these costs are prohibitive \\nin may 202'},\n",
       " {'input_ids': tensor([    3,  2951,    33,    97, 10862,    11, 11855,    16,  2942,    13,\n",
       "              8,   169,  6701,   175,  1358,    33, 19551,   757,    16,   164,\n",
       "              3, 19818,     1]),\n",
       "  'input_text': 'ches are timeconsuming and\\ncostly in majority of the usecase these costs are prohibitive \\nin may 202',\n",
       "  'expected_output': '0 researchers in their paper retrievalaugmented ge'},\n",
       " {'input_ids': tensor([    3,    88,   169,  6701,   175,  1358,    33, 19551,   757,    16,\n",
       "            164,  6503,  4768,    16,    70,  1040, 24515,   138, 28984,   873,\n",
       "              1]),\n",
       "  'input_text': 'he usecase these costs are prohibitive \\nin may 2020 researchers in their paper retrievalaugmented ge',\n",
       "  'expected_output': 'neration for\\nknowledgeintensive nlp tasks explored'},\n",
       " {'input_ids': tensor([    3,   632,  4768,    16,    70,  1040, 24515,   138, 28984,  3381,\n",
       "             21,  1103, 28135,     3,    29,    40,   102,  4145, 15883,     1]),\n",
       "  'input_text': '0 researchers in their paper retrievalaugmented generation for\\nknowledgeintensive nlp tasks explored',\n",
       "  'expected_output': ' models which combine pretrained\\nparametric and no'},\n",
       " {'input_ids': tensor([    3,   687,   257,    21,  1103, 28135,     3,    29,    40,   102,\n",
       "           4145, 15883,  2250,    84,  5148,  7140, 10761, 30706,    75,    11,\n",
       "            150,     1]),\n",
       "  'input_text': 'neration for\\nknowledgeintensive nlp tasks explored models which combine pretrained\\nparametric and no',\n",
       "  'expected_output': 'nparametric memory for language generation \\nabhina'},\n",
       " {'input_ids': tensor([ 2250,    84,  5148,  7140, 10761, 30706,    75,    11,   529,  6583,\n",
       "           7959,  2594,    21,  1612,  3381,   703,  2907,     9,     1]),\n",
       "  'input_text': ' models which combine pretrained\\nparametric and nonparametric memory for language generation \\nabhina',\n",
       "  'expected_output': 'v kimothiwhat is rag\\nthe hunger for more\\nwhile the'},\n",
       " {'input_ids': tensor([    3,    29,  6583,  7959,  2594,    21,  1612,  3381,   703,  2907,\n",
       "              9,   208,     3, 19754,    32,  7436,  9170,    19,     3,  6151,\n",
       "              8, 18560,    21,    72,   298,     8,     1]),\n",
       "  'input_text': 'nparametric memory for language generation \\nabhinav kimothiwhat is rag\\nthe hunger for more\\nwhile the',\n",
       "  'expected_output': ' weaknesses of llms were being discussed a paralle'},\n",
       " {'input_ids': tensor([    3,   208,     3, 19754,    32,  7436,  9170,    19,     3,  6151,\n",
       "              8, 18560,    21,    72,   298,     8, 21506,    13,     3,   195,\n",
       "             51,     7,   130,   271,  5172,     3,     9,   260, 13701,     1]),\n",
       "  'input_text': 'v kimothiwhat is rag\\nthe hunger for more\\nwhile the weaknesses of llms were being discussed a paralle',\n",
       "  'expected_output': 'l discourse around\\nproviding context to the models'},\n",
       " {'input_ids': tensor([21506,    13,     3,   195,    51,     7,   130,   271,  5172,     3,\n",
       "              9,  8449, 22739,   300,  1260,  2625,    12,     8,  2250,     1]),\n",
       "  'input_text': ' weaknesses of llms were being discussed a parallel discourse around\\nproviding context to the models',\n",
       "  'expected_output': ' started in essence it meant creating a chatgpt\\non'},\n",
       " {'input_ids': tensor([    3,    40, 22739,   300,  1260,  2625,    12,     8,  2250,   708,\n",
       "             16, 10848,    34,  3679,  1577,     3,     9,  3582,   122,   102,\n",
       "             17,    30,     1]),\n",
       "  'input_text': 'l discourse around\\nproviding context to the models started in essence it meant creating a chatgpt\\non',\n",
       "  'expected_output': ' proprietary data\\nmake llms respond with uptodate '},\n",
       " {'input_ids': tensor([  708,    16, 10848,    34,  3679,  1577,     3,     9,  3582,   122,\n",
       "            102,    17,    30, 16950,   331,   143,     3,   195,    51,     7,\n",
       "           3531,    28,    95,   235,  5522,     1]),\n",
       "  'input_text': ' started in essence it meant creating a chatgpt\\non proprietary data\\nmake llms respond with uptodate ',\n",
       "  'expected_output': 'information\\nmake llms not respond with factually i'},\n",
       " {'input_ids': tensor([16950,   331,   143,     3,   195,    51,     7,  3531,    28,    95,\n",
       "            235,  5522,   251,   143,     3,   195,    51,     7,    59,  3531,\n",
       "             28,   685,    76,  1427,     3,    23,     1]),\n",
       "  'input_text': ' proprietary data\\nmake llms respond with uptodate information\\nmake llms not respond with factually i',\n",
       "  'expected_output': 'naccurate\\ninformation\\nmake llms aware of proprieta'},\n",
       " {'input_ids': tensor([  251,   143,     3,   195,    51,     7,    59,  3531,    28,   685,\n",
       "             76,  1427, 27801,   251,   143,     3,   195,    51,     7,  2718,\n",
       "             13, 18282,    17,     9,     1]),\n",
       "  'input_text': 'information\\nmake llms not respond with factually inaccurate\\ninformation\\nmake llms aware of proprieta',\n",
       "  'expected_output': 'ry informationthe challenge \\nproviding contextprov'},\n",
       " {'input_ids': tensor([    3, 11962,  3663,   342,   251,   143,     3,   195,    51,     7,\n",
       "           2718,    13, 16950,   251,   532,  1921,  1260,  2625,  1409,   208,\n",
       "              1]),\n",
       "  'input_text': 'naccurate\\ninformation\\nmake llms aware of proprietary informationthe challenge \\nproviding contextprov',\n",
       "  'expected_output': 'iding llms with information not in their memory\\nke'},\n",
       " {'input_ids': tensor([   3,  651,  251,  532, 1921, 1260, 2625, 1409, 6961,   53,    3,  195,\n",
       "            51,    7,   28,  251,   59,   16,   70, 2594,    3, 1050,    1]),\n",
       "  'input_text': 'ry informationthe challenge \\nproviding contextproviding llms with information not in their memory\\nke',\n",
       "  'expected_output': 'ep calm  build ai4retriever\\nsearch fetch\\ncontext\\np'},\n",
       " {'input_ids': tensor([    3, 12469,     3,   195,    51,     7,    28,   251,    59,    16,\n",
       "             70,  2594,   453,  4447,   918,     3,     9,    23,   591,    60,\n",
       "           1788,  3258,   960, 27109,  2625,     3,   102,     1]),\n",
       "  'input_text': 'iding llms with information not in their memory\\nkeep calm  build ai4retriever\\nsearch fetch\\ncontext\\np',\n",
       "  'expected_output': 'roprietary and nonproprietary informationuserpromp'},\n",
       " {'input_ids': tensor([    3,    15,   102,  4447,   918,     3,     9,    23,   591,    60,\n",
       "           1788,  3258,   960, 27109,  2625, 16950,    11,   529, 19453,    63,\n",
       "            251, 10041,  1409,  1167,     1]),\n",
       "  'input_text': 'ep calm  build ai4retriever\\nsearch fetch\\ncontext\\nproprietary and nonproprietary informationuserpromp',\n",
       "  'expected_output': 't  prompt  context \\nllmr a g\\nr\\na\\nglookup the exter'},\n",
       " {'input_ids': tensor([    3,  9505,  1753,    17,  1208,    11,   529, 19453,    63,   251,\n",
       "          10041,  1409,  1167,    17,  9005,  2625,     3,   195,    51,    52,\n",
       "              3,     9,     3,   122,     3,    52,     3,     9,     3,   122,\n",
       "          16800,   413,     8,  1215,   449,     1]),\n",
       "  'input_text': 'roprietary and nonproprietary informationuserprompt  prompt  context \\nllmr a g\\nr\\na\\nglookup the exter',\n",
       "  'expected_output': 'nal source to\\nretrieve the   relevant information\\n'},\n",
       " {'input_ids': tensor([    3,    17,  9005,  2625,     3,   195,    51,    52,     3,     9,\n",
       "              3,   122,     3,    52,     3,     9,     3,   122, 16800,   413,\n",
       "              8,  3866,  1391,    12, 21830,     8,  2193,   251,     1]),\n",
       "  'input_text': 't  prompt  context \\nllmr a g\\nr\\na\\nglookup the external source to\\nretrieve the   relevant information\\n',\n",
       "  'expected_output': 'add the retrieved information to\\nthe user prompt\\nu'},\n",
       " {'input_ids': tensor([    3,    29,   138,  1391,    12, 21830,     8,  2193,   251,   617,\n",
       "              8,     3, 31340,   251,    12,     8,  1139,  9005,     3,    76,\n",
       "              1]),\n",
       "  'input_text': 'nal source to\\nretrieve the   relevant information\\nadd the retrieved information to\\nthe user prompt\\nu',\n",
       "  'expected_output': 'se llm to generate response to\\nuser prompt with th'},\n",
       " {'input_ids': tensor([  617,     8,     3, 31340,   251,    12,     8,  1139,  9005,   169,\n",
       "              3,   195,    51,    12,  3806,  1773,    12,  1139,  9005,    28,\n",
       "              3,   189,     1]),\n",
       "  'input_text': 'add the retrieved information to\\nthe user prompt\\nuse llm to generate response to\\nuser prompt with th',\n",
       "  'expected_output': 'e context\\nretrieved relevant information is augmen'},\n",
       " {'input_ids': tensor([  142,     3,   195,    51,    12,  3806,  1773,    12,  1139,  9005,\n",
       "             28,     8,  2625,     3, 31340,  2193,   251,    19,   185,   122,\n",
       "            904,     1]),\n",
       "  'input_text': 'se llm to generate response to\\nuser prompt with the context\\nretrieved relevant information is augmen',\n",
       "  'expected_output': 'ted to the prompt as context\\nllm is asked to gener'},\n",
       " {'input_ids': tensor([    3,    15,  2625,     3, 31340,  2193,   251,    19,     3, 28984,\n",
       "             12,     8,  9005,    38,  2625,     3,   195,    51,    19,  1380,\n",
       "             12, 11467,     1]),\n",
       "  'input_text': 'e context\\nretrieved relevant information is augmented to the prompt as context\\nllm is asked to gener',\n",
       "  'expected_output': 'ate response to the prompt in the context\\naugmente'},\n",
       " {'input_ids': tensor([    3,  1054,    12,     8,  9005,    38,  2625,     3,   195,    51,\n",
       "             19,  1380,    12,  3806,  1773,    12,     8,  9005,    16,     8,\n",
       "           2625, 15189,    15,     1]),\n",
       "  'input_text': 'ted to the prompt as context\\nllm is asked to generate response to the prompt in the context\\naugmente',\n",
       "  'expected_output': 'd information\\nabhinav kimothiwhat is rag\\nso what i'},\n",
       " {'input_ids': tensor([    3,   342,  1773,    12,     8,  9005,    16,     8,  2625,     3,\n",
       "          28984,   251,   703,  2907,     9,   208,     3, 19754,    32,  7436,\n",
       "           9170,    19,     3,  6151,    78,   125,     3,    23,     1]),\n",
       "  'input_text': 'ate response to the prompt in the context\\naugmented information\\nabhinav kimothiwhat is rag\\nso what i',\n",
       "  'expected_output': 's rag\\nin 2023 rag has become one of the most used '},\n",
       " {'input_ids': tensor([    3,    26,   251,   703,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436,  9170,    19,     3,  6151,    78,   125,    19,     3,  6151,\n",
       "             16,   460,  2773,     3,  6151,    65,   582,    80,    13,     8,\n",
       "            167,   261,     1]),\n",
       "  'input_text': 'd information\\nabhinav kimothiwhat is rag\\nso what is rag\\nin 2023 rag has become one of the most used ',\n",
       "  'expected_output': 'technique in the domain of large\\nlanguage models\\nw'},\n",
       " {'input_ids': tensor([   3,    7,    3, 6151,   16,  460, 2773,    3, 6151,   65,  582,   80,\n",
       "            13,    8,  167,  261, 3317,   16,    8, 3303,   13,  508, 1612, 2250,\n",
       "             3,  210,    1]),\n",
       "  'input_text': 's rag\\nin 2023 rag has become one of the most used technique in the domain of large\\nlanguage models\\nw',\n",
       "  'expected_output': 'hat is rag\\nuser enters a promptquery\\nretriever sea'},\n",
       " {'input_ids': tensor([ 3317,    16,     8,  3303,    13,   508,  1612,  2250,   125,    19,\n",
       "              3,  6151,  1139,  2058,     7,     3,     9,  9005,   835,   651,\n",
       "          21830,    52,  2805,     1]),\n",
       "  'input_text': 'technique in the domain of large\\nlanguage models\\nwhat is rag\\nuser enters a promptquery\\nretriever sea',\n",
       "  'expected_output': 'rches and fetches information relevant to the prom'},\n",
       " {'input_ids': tensor([    3,   547,    19,     3,  6151,  1139,  2058,     7,     3,     9,\n",
       "           9005,   835,   651, 21830,    52, 14397,    11, 27109,    15,     7,\n",
       "            251,  2193,    12,     8, 15207,     1]),\n",
       "  'input_text': 'hat is rag\\nuser enters a promptquery\\nretriever searches and fetches information relevant to the prom',\n",
       "  'expected_output': 'pt\\neg from the internet or internet data warehouse'},\n",
       " {'input_ids': tensor([    3,    52,  2951,    11, 27109,    15,     7,   251,  2193,    12,\n",
       "              8,  9005,     3,    15,   122,    45,     8,  1396,    42,  1396,\n",
       "            331, 11625,     1]),\n",
       "  'input_text': 'rches and fetches information relevant to the prompt\\neg from the internet or internet data warehouse',\n",
       "  'expected_output': '\\nuser receives the response\\na naive rag workflow\\nk'},\n",
       " {'input_ids': tensor([    3,   102,    17,     3,    15,   122,    45,     8,  1396,    42,\n",
       "           1396,   331, 11625,  1139,   911,     7,     8,  1773,     3,     9,\n",
       "              3,    29,     9,   757,     3,  6151, 16101,     3,   157,     1]),\n",
       "  'input_text': 'pt\\neg from the internet or internet data warehouse\\nuser receives the response\\na naive rag workflow\\nk',\n",
       "  'expected_output': 'eep calm  build ai5retrieverweb pages\\napis  dynami'},\n",
       " {'input_ids': tensor([ 1139,   911,     7,     8,  1773,     3,     9,     3,    29,     9,\n",
       "            757,     3,  6151, 16101,   453,  4447,   918,     3,     9,    23,\n",
       "            755,    60,  1788,  3258,  8398,  1688,     3, 13306,     7,     3,\n",
       "          24805,    51,    23,     1]),\n",
       "  'input_text': '\\nuser receives the response\\na naive rag workflow\\nkeep calm  build ai5retrieverweb pages\\napis  dynami',\n",
       "  'expected_output': 'c dbs\\ndocument reposwithout rag  \\nabhinav kimothih'},\n",
       " {'input_ids': tensor([    3,    15,    15,   102,  4447,   918,     3,     9,    23,   755,\n",
       "             60,  1788,  3258,  8398,  1688,     3, 13306,     7,  4896,     3,\n",
       "             26,   115,     7,  1708, 14173, 23016,     3,  6151,   703,  2907,\n",
       "              9,   208,     3, 19754,    32,  7436,   107,     1]),\n",
       "  'input_text': 'eep calm  build ai5retrieverweb pages\\napis  dynamic dbs\\ndocument reposwithout rag  \\nabhinav kimothih',\n",
       "  'expected_output': 'ow does rag help\\nhow does rag help\\nunlimited knowl'},\n",
       " {'input_ids': tensor([    3,    75,     3,    26,   115,     7,  1708, 14173, 23016,     3,\n",
       "           6151,   703,  2907,     9,   208,     3, 19754,    32,  7436,  4067,\n",
       "            405,     3,  6151,   199,   149,   405,     3,  6151,   199, 11875,\n",
       "            214,    40,     1]),\n",
       "  'input_text': 'c dbs\\ndocument reposwithout rag  \\nabhinav kimothihow does rag help\\nhow does rag help\\nunlimited knowl',\n",
       "  'expected_output': 'edge\\nthe retriever of an rag system can have acces'},\n",
       " {'input_ids': tensor([    3,  2381,   405,     3,  6151,   199,   149,   405,     3,  6151,\n",
       "            199, 11875,  1103,     8, 21830,    52,    13,    46,     3,  6151,\n",
       "            358,    54,    43,  4991,     1]),\n",
       "  'input_text': 'ow does rag help\\nhow does rag help\\nunlimited knowledge\\nthe retriever of an rag system can have acces',\n",
       "  'expected_output': 's to external sources of information therefore\\nthe'},\n",
       " {'input_ids': tensor([ 3023,     8, 21830,    52,    13,    46,     3,  6151,   358,    54,\n",
       "             43,   592,    12,  3866,  2836,    13,   251,  2459,     8,     1]),\n",
       "  'input_text': 'edge\\nthe retriever of an rag system can have access to external sources of information therefore\\nthe',\n",
       "  'expected_output': ' llm is not limited to its internal knowledge the '},\n",
       " {'input_ids': tensor([   3,    7,   12, 3866, 2836,   13,  251, 2459,    8,    3,  195,   51,\n",
       "            19,   59, 1643,   12,  165, 3224, 1103,    8,    1]),\n",
       "  'input_text': 's to external sources of information therefore\\nthe llm is not limited to its internal knowledge the ',\n",
       "  'expected_output': 'external sources can be proprietary\\ndocuments and '},\n",
       " {'input_ids': tensor([    3,   195,    51,    19,    59,  1643,    12,   165,  3224,  1103,\n",
       "              8,  3866,  2836,    54,    36, 16950,  2691,    11,     1]),\n",
       "  'input_text': ' llm is not limited to its internal knowledge the external sources can be proprietary\\ndocuments and ',\n",
       "  'expected_output': 'data or even the internet\\nan llm has knowledge\\nonl'},\n",
       " {'input_ids': tensor([ 3866,  2836,    54,    36, 16950,  2691,    11,   331,    42,   237,\n",
       "              8,  1396,    46,     3,   195,    51,    65,  1103,    30,    40,\n",
       "              1]),\n",
       "  'input_text': 'external sources can be proprietary\\ndocuments and data or even the internet\\nan llm has knowledge\\nonl',\n",
       "  'expected_output': 'y of the data it has\\nbeen trained on\\nalso called p'},\n",
       " {'input_ids': tensor([ 331,   42,  237,    8, 1396,   46,    3,  195,   51,   65, 1103,  163,\n",
       "            13,    8,  331,   34,   65,  118, 4252,   30,   92,  718,    3,  102,\n",
       "             1]),\n",
       "  'input_text': 'data or even the internet\\nan llm has knowledge\\nonly of the data it has\\nbeen trained on\\nalso called p',\n",
       "  'expected_output': 'arametric\\nmemory information\\nstored in the model\\np'},\n",
       " {'input_ids': tensor([    3,    63,    13,     8,   331,    34,    65,   118,  4252,    30,\n",
       "             92,   718, 30706,    75,  2594,   251,  5816,    16,     8,   825,\n",
       "              3,   102,     1]),\n",
       "  'input_text': 'y of the data it has\\nbeen trained on\\nalso called parametric\\nmemory information\\nstored in the model\\np',\n",
       "  'expected_output': 'arametersdatabasesother sources\\nretriever  searche'},\n",
       " {'input_ids': tensor([    3,  2551,  7959,  2594,   251,  5816,    16,     8,   825,  8755,\n",
       "           6757, 10925,     7,  9269,  2836, 21830,    52,   960,    15,     1]),\n",
       "  'input_text': 'arametric\\nmemory information\\nstored in the model\\nparametersdatabasesother sources\\nretriever  searche',\n",
       "  'expected_output': 's and fetches information that the llm has not\\nnec'},\n",
       " {'input_ids': tensor([    3,  2551,  4401,     7,  6757, 10925,     7,  9269,  2836, 21830,\n",
       "             52, 14397,    11, 27109,    15,     7,   251,    24,     8,     3,\n",
       "            195,    51,    65,    59,  9705,     1]),\n",
       "  'input_text': 'arametersdatabasesother sources\\nretriever  searches and fetches information that the llm has not\\nnec',\n",
       "  'expected_output': 'essarily been trained on this adds to the llm memo'},\n",
       " {'input_ids': tensor([    3,     7,    11, 27109,    15,     7,   251,    24,     8,     3,\n",
       "            195,    51,    65,    59,  6539,   118,  4252,    30,    48,   617,\n",
       "              7,    12,     8,     3,   195,    51, 22986,     1]),\n",
       "  'input_text': 's and fetches information that the llm has not\\nnecessarily been trained on this adds to the llm memo',\n",
       "  'expected_output': 'ry and is passed as\\ncontext in the prompts also ca'},\n",
       " {'input_ids': tensor([   3,   15,    7,    7, 1665,  120,  118, 4252,   30,   48,  617,    7,\n",
       "            12,    8,    3,  195,   51, 2594,   11,   19, 2804,   38, 2625,   16,\n",
       "             8, 9005,    7,   92,  212,    1]),\n",
       "  'input_text': 'essarily been trained on this adds to the llm memory and is passed as\\ncontext in the prompts also ca',\n",
       "  'expected_output': 'lled nonparametric memory information\\navailable ou'},\n",
       " {'input_ids': tensor([   3,  651,   11,   19, 2804,   38, 2625,   16,    8, 9005,    7,   92,\n",
       "           718,  529, 6583, 7959, 2594,  251,  347,  407,    1]),\n",
       "  'input_text': 'ry and is passed as\\ncontext in the prompts also called nonparametric memory information\\navailable ou',\n",
       "  'expected_output': 'tside the model parameters\\nexpandable to all sourc'},\n",
       " {'input_ids': tensor([   3,   40, 1361,  529, 6583, 7959, 2594,  251,  347, 1067,    8,  825,\n",
       "          8755, 4405,  179,   12,   66,    3,    7, 1211,   75,    1]),\n",
       "  'input_text': 'lled nonparametric memory information\\navailable outside the model parameters\\nexpandable to all sourc',\n",
       "  'expected_output': 'es\\neasier to updatemaintain\\nmuch cheaper than retr'},\n",
       " {'input_ids': tensor([   3,   17, 1583,    8,  825, 8755, 4405,  179,   12,   66, 2836, 1842,\n",
       "            12, 2270, 7484,   17,    9,   77,  231, 8139,  145,    3,   60,   17,\n",
       "            52,    1]),\n",
       "  'input_text': 'tside the model parameters\\nexpandable to all sources\\neasier to updatemaintain\\nmuch cheaper than retr',\n",
       "  'expected_output': 'ainingfinetuning\\nthe effort lies in creation of th'},\n",
       " {'input_ids': tensor([    3,    15,     7,  1842,    12,  2270,  7484,    17,     9,    77,\n",
       "            231,  8139,   145,     3,    60, 13023, 13536,    17,   202,    53,\n",
       "              8,  1941,  7797,    16,  3409,    13,     3,   189,     1]),\n",
       "  'input_text': 'es\\neasier to updatemaintain\\nmuch cheaper than retrainingfinetuning\\nthe effort lies in creation of th',\n",
       "  'expected_output': 'e knowledge basewith rag\\nconfidence in responses\\nw'},\n",
       " {'input_ids': tensor([    3,     9,    77,    53, 13536,    17,   202,    53,     8,  1941,\n",
       "           7797,    16,  3409,    13,     8,  1103,  1247,  4065,     3,  6151,\n",
       "           3410,    16,  7216,     3,   210,     1]),\n",
       "  'input_text': 'ainingfinetuning\\nthe effort lies in creation of the knowledge basewith rag\\nconfidence in responses\\nw',\n",
       "  'expected_output': 'ith the context extra information that is retrieve'},\n",
       " {'input_ids': tensor([    3,    15,  1103,  1247,  4065,     3,  6151,  3410,    16,  7216,\n",
       "             28,     8,  2625,   996,   251,    24,    19, 21830,     1]),\n",
       "  'input_text': 'e knowledge basewith rag\\nconfidence in responses\\nwith the context extra information that is retrieve',\n",
       "  'expected_output': 'd made available to the llm\\nthe confidence in llm '},\n",
       " {'input_ids': tensor([   34,   107,     8,  2625,   996,   251,    24,    19,     3, 31340,\n",
       "            263,   347,    12,     8,     3,   195,    51,     8,  3410,    16,\n",
       "              3,   195,    51,     1]),\n",
       "  'input_text': 'ith the context extra information that is retrieved made available to the llm\\nthe confidence in llm ',\n",
       "  'expected_output': 'responses is increased\\ncontext awareness source ci'},\n",
       " {'input_ids': tensor([   3,   26,  263,  347,   12,    8,    3,  195,   51,    8, 3410,   16,\n",
       "             3,  195,   51, 7216,   19, 1936, 2625, 4349, 1391,    3,   75,   23,\n",
       "             1]),\n",
       "  'input_text': 'd made available to the llm\\nthe confidence in llm responses is increased\\ncontext awareness source ci',\n",
       "  'expected_output': 'tation reduced hallucinations\\nadded information\\nas'},\n",
       " {'input_ids': tensor([ 7216,    19,  1936,  2625,  4349,  1391,     3, 13903,  3915,  6358,\n",
       "           6809,    29,  1628,   974,   251,    38,     1]),\n",
       "  'input_text': 'responses is increased\\ncontext awareness source citation reduced hallucinations\\nadded information\\nas',\n",
       "  'expected_output': 'sists llms in\\ngenerating responses\\nthat are accura'},\n",
       " {'input_ids': tensor([    3,  6821,  3915,  6358,  6809,    29,  1628,   974,   251, 13041,\n",
       "              3,   195,    51,     7,    16,     3, 11600,  7216,    24,    33,\n",
       "              3,  6004,  2414,     1]),\n",
       "  'input_text': 'tation reduced hallucinations\\nadded information\\nassists llms in\\ngenerating responses\\nthat are accura',\n",
       "  'expected_output': 'te and\\ncontextually appropriateaccess to sources o'},\n",
       " {'input_ids': tensor([    3,     7,   343,     7,     3,   195,    51,     7,    16,     3,\n",
       "          11600,  7216,    24,    33,  4034,    11, 28131,   120,  2016, 20393,\n",
       "             12,  2836,     3,    32,     1]),\n",
       "  'input_text': 'sists llms in\\ngenerating responses\\nthat are accurate and\\ncontextually appropriateaccess to sources o',\n",
       "  'expected_output': 'f\\ninformation improves the\\ntransparency of the llm'},\n",
       " {'input_ids': tensor([    3,    17,    15,    11, 28131,   120,  2016, 20393,    12,  2836,\n",
       "             13,   251,  1172,     7,     8, 13567,    13,     8,     3,   195,\n",
       "             51,     1]),\n",
       "  'input_text': 'te and\\ncontextually appropriateaccess to sources of\\ninformation improves the\\ntransparency of the llm',\n",
       "  'expected_output': '\\nresponsesrag enabled llm\\nsystems are observed to\\n'},\n",
       " {'input_ids': tensor([    3,    89,   251,  1172,     7,     8, 13567,    13,     8,     3,\n",
       "            195,    51,  7216,  6151,  9367,     3,   195,    51,  1002,    33,\n",
       "           6970,    12,     1]),\n",
       "  'input_text': 'f\\ninformation improves the\\ntransparency of the llm\\nresponsesrag enabled llm\\nsystems are observed to\\n',\n",
       "  'expected_output': 'be less prone to\\nhallucinations than the\\nones with'},\n",
       " {'input_ids': tensor([ 7216,  6151,  9367,     3,   195,    51,  1002,    33,  6970,    12,\n",
       "             36,   705,     3, 15897,    12,  6358,  6809,    29,  1628,   145,\n",
       "              8,  2102,    28,     1]),\n",
       "  'input_text': '\\nresponsesrag enabled llm\\nsystems are observed to\\nbe less prone to\\nhallucinations than the\\nones with',\n",
       "  'expected_output': 'out rag\\nkeep calm  build ai6abhinav kimothiwhat ar'},\n",
       " {'input_ids': tensor([   36,   705,     3, 15897,    12,  6358,  6809,    29,  1628,   145,\n",
       "              8,  2102,   406,     3,  6151,   453,  4447,   918,     3,     9,\n",
       "             23,   948,     9,   115,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436,  9170,  1584,     1]),\n",
       "  'input_text': 'be less prone to\\nhallucinations than the\\nones without rag\\nkeep calm  build ai6abhinav kimothiwhat ar',\n",
       "  'expected_output': 'e some popular rag use cases\\nrag use cases \\nthe de'},\n",
       " {'input_ids': tensor([   91,     3,  6151,   453,  4447,   918,     3,     9,    23,   948,\n",
       "              9,   115,  2907,     9,   208,     3, 19754,    32,  7436,  9170,\n",
       "             33,   128,  1012,     3,  6151,   169,  1488,     3,  6151,   169,\n",
       "           1488,     8,    20,     1]),\n",
       "  'input_text': 'out rag\\nkeep calm  build ai6abhinav kimothiwhat are some popular rag use cases\\nrag use cases \\nthe de',\n",
       "  'expected_output': 'velopment of rag technique is rooted in use cases '},\n",
       " {'input_ids': tensor([    3,    15,   128,  1012,     3,  6151,   169,  1488,     3,  6151,\n",
       "            169,  1488,     8,   606,    13,     3,  6151,  3317,    19,     3,\n",
       "          19471,    16,   169,  1488,     1]),\n",
       "  'input_text': 'e some popular rag use cases\\nrag use cases \\nthe development of rag technique is rooted in use cases ',\n",
       "  'expected_output': 'that were limited by the\\ninherent weaknesses of th'},\n",
       " {'input_ids': tensor([    3,   162,  8745,   297,    13,     3,  6151,  3317,    19,     3,\n",
       "          19471,    16,   169,  1488,    24,   130,  1643,    57,     8, 18340,\n",
       "          21506,    13,     3,   189,     1]),\n",
       "  'input_text': 'velopment of rag technique is rooted in use cases that were limited by the\\ninherent weaknesses of th',\n",
       "  'expected_output': 'e llms as of today some commercial applications of'},\n",
       " {'input_ids': tensor([   24,   130,  1643,    57,     8, 18340, 21506,    13,     8,     3,\n",
       "            195,    51,     7,    38,    13,   469,   128,  1328,  1564,    13,\n",
       "              1]),\n",
       "  'input_text': 'that were limited by the\\ninherent weaknesses of the llms as of today some commercial applications of',\n",
       "  'expected_output': '\\nrag are in  \\nby providing access to proprietary e'},\n",
       " {'input_ids': tensor([    3,    15,     3,   195,    51,     7,    38,    13,   469,   128,\n",
       "           1328,  1564,    13,     3,  6151,    33,    16,    57,  1260,   592,\n",
       "             12, 16950,     3,    15,     1]),\n",
       "  'input_text': 'e llms as of today some commercial applications of\\nrag are in  \\nby providing access to proprietary e',\n",
       "  'expected_output': 'nterprise document to an llm the\\nresponses are lim'},\n",
       " {'input_ids': tensor([    3,  6151,    33,    16,    57,  1260,   592,    12, 16950,  5399,\n",
       "           1708,    12,    46,     3,   195,    51,     8,  7216,    33,     3,\n",
       "           4941,     1]),\n",
       "  'input_text': '\\nrag are in  \\nby providing access to proprietary enterprise document to an llm the\\nresponses are lim',\n",
       "  'expected_output': 'ited to what is provided within them a retriever c'},\n",
       " {'input_ids': tensor([    3,    29,   449,   102,  7854,  1708,    12,    46,     3,   195,\n",
       "             51,     8,  7216,    33,  1643,    12,   125,    19,   937,   441,\n",
       "            135,     3,     9, 21830,    52,     3,    75,     1]),\n",
       "  'input_text': 'nterprise document to an llm the\\nresponses are limited to what is provided within them a retriever c',\n",
       "  'expected_output': 'an\\nsearch for the most relevant documents and prov'},\n",
       " {'input_ids': tensor([   34,    15,    26,    12,   125,    19,   937,   441,   135,     3,\n",
       "              9, 21830,    52,    54,   960,    21,     8,   167,  2193,  2691,\n",
       "             11,   813,   208,     1]),\n",
       "  'input_text': 'ited to what is provided within them a retriever can\\nsearch for the most relevant documents and prov',\n",
       "  'expected_output': 'ide the information to\\nthe llm check out this blog'},\n",
       " {'input_ids': tensor([  46,  960,   21,    8,  167, 2193, 2691,   11,  370,    8,  251,   12,\n",
       "             8,    3,  195,   51,  691,   91,   48,  875,    1]),\n",
       "  'input_text': 'an\\nsearch for the most relevant documents and provide the information to\\nthe llm check out this blog',\n",
       "  'expected_output': ' for an exampledocument question answering systems'},\n",
       " {'input_ids': tensor([    3,  1599,     8,   251,    12,     8,     3,   195,    51,   691,\n",
       "             91,    48,   875,    21,    46,   677, 28244,   822, 18243,  1002,\n",
       "              1]),\n",
       "  'input_text': 'ide the information to\\nthe llm check out this blog for an exampledocument question answering systems',\n",
       "  'expected_output': '\\nllms can be customised to productservice manuals '},\n",
       " {'input_ids': tensor([   21,    46,   677, 28244,   822, 18243,  1002,     3,   195,    51,\n",
       "              7,    54,    36,     3, 30853,    12,   556,  5114,  3354,     7,\n",
       "              1]),\n",
       "  'input_text': ' for an exampledocument question answering systems\\nllms can be customised to productservice manuals ',\n",
       "  'expected_output': 'domain\\nknowledge guidelines etc using rag the agen'},\n",
       " {'input_ids': tensor([    3,   195,    51,     7,    54,    36,     3, 30853,    12,   556,\n",
       "           5114,  3354,     7,  3303,  1103,  5749,   672,   338,     3,  6151,\n",
       "              8,     3,     9,   729,     1]),\n",
       "  'input_text': '\\nllms can be customised to productservice manuals domain\\nknowledge guidelines etc using rag the agen',\n",
       "  'expected_output': 't can also route users to\\nmore specialised agents '},\n",
       " {'input_ids': tensor([ 3303,  1103,  5749,   672,   338,     3,  6151,     8,  3102,    54,\n",
       "             92,  2981,  1105,    12,    72,     3, 26725,  4373,     1]),\n",
       "  'input_text': 'domain\\nknowledge guidelines etc using rag the agent can also route users to\\nmore specialised agents ',\n",
       "  'expected_output': 'depending on their query searchunify has an\\nllmrag'},\n",
       " {'input_ids': tensor([    3,    17,    54,    92,  2981,  1105,    12,    72,     3, 26725,\n",
       "           4373,  3345,    30,    70, 11417,   960,   202,  4921,    65,    46,\n",
       "              3,   195,    51,  6151,     1]),\n",
       "  'input_text': 't can also route users to\\nmore specialised agents depending on their query searchunify has an\\nllmrag',\n",
       "  'expected_output': ' powered conversational agent for their users\\nimag'},\n",
       " {'input_ids': tensor([ 3345,    30,    70, 11417,   960,   202,  4921,    65,    46,     3,\n",
       "            195,    51,  6151, 10028,  3634,   138,  3102,    21,    70,  1105,\n",
       "              3,    23,  7493,     1]),\n",
       "  'input_text': 'depending on their query searchunify has an\\nllmrag powered conversational agent for their users\\nimag',\n",
       "  'expected_output': 'ine an event like a sports or a new event a retrie'},\n",
       " {'input_ids': tensor([10028,  3634,   138,  3102,    21,    70,  1105,  3034,    46,   605,\n",
       "            114,     3,     9,  2100,    42,     3,     9,   126,   605,     3,\n",
       "              9,     3,    60,  1788,    15,     1]),\n",
       "  'input_text': ' powered conversational agent for their users\\nimagine an event like a sports or a new event a retrie',\n",
       "  'expected_output': 'ver can connect to\\nrealtime updatesdata via apis a'},\n",
       " {'input_ids': tensor([   16,    15,    46,   605,   114,     3,     9,  2100,    42,     3,\n",
       "              9,   126,   605,     3,     9, 21830,    52,    54,  1979,    12,\n",
       "            490,   715,  3864,  6757,  1009,     3, 13306,     7,     3,     9,\n",
       "              1]),\n",
       "  'input_text': 'ine an event like a sports or a new event a retriever can connect to\\nrealtime updatesdata via apis a',\n",
       "  'expected_output': 'nd pass this information to the llm to\\ncreate a vi'},\n",
       " {'input_ids': tensor([  548,    54,  1979,    12,   490,   715,  3864,  6757,  1009,     3,\n",
       "          13306,     7,    11,  1903,    48,   251,    12,     8,     3,   195,\n",
       "             51,    12,   482,     3,     9,  5931,     1]),\n",
       "  'input_text': 'ver can connect to\\nrealtime updatesdata via apis and pass this information to the llm to\\ncreate a vi',\n",
       "  'expected_output': 'rtual commentator these can further be augmented w'},\n",
       " {'input_ids': tensor([    3,   727,  1903,    48,   251,    12,     8,     3,   195,    51,\n",
       "             12,   482,     3,     9,  4291,  1670,  1016,   175,    54,   856,\n",
       "             36,     3, 28984,     3,   210,     1]),\n",
       "  'input_text': 'nd pass this information to the llm to\\ncreate a virtual commentator these can further be augmented w',\n",
       "  'expected_output': 'ith\\ntext to speech modelsibm leveraged the technol'},\n",
       " {'input_ids': tensor([    3,    52,    17,  3471,  1670,  1016,   175,    54,   856,    36,\n",
       "              3, 28984,    28,  1499,    12,  5023,  2250,    23,   115,    51,\n",
       "          11531,    26,     8, 25389,    40,     1]),\n",
       "  'input_text': 'rtual commentator these can further be augmented with\\ntext to speech modelsibm leveraged the technol',\n",
       "  'expected_output': 'ogy for commentary\\nduring the 2023 us openconversa'},\n",
       " {'input_ids': tensor([   34,   107,  1499,    12,  5023,  2250,    23,   115,    51, 11531,\n",
       "             26,     8,   748,    21, 18204,   383,     8,   460,  2773,   178,\n",
       "            539,  1018,  2660,     9,     1]),\n",
       "  'input_text': 'ith\\ntext to speech modelsibm leveraged the technology for commentary\\nduring the 2023 us openconversa',\n",
       "  'expected_output': 'tional agents\\nrealtime event commentary\\ncontent ge'},\n",
       " {'input_ids': tensor([    3,    32,   122,    63,    21, 18204,   383,     8,   460,  2773,\n",
       "            178,   539,  1018,  2660,   257,   138,  4373,   490,   715,   605,\n",
       "          18204,   738,   873,     1]),\n",
       "  'input_text': 'ogy for commentary\\nduring the 2023 us openconversational agents\\nrealtime event commentary\\ncontent ge',\n",
       "  'expected_output': 'neration\\nthe widest use of llms has probably been '},\n",
       " {'input_ids': tensor([    3,    17,  6318,  4373,   490,   715,   605, 18204,   738,  3381,\n",
       "              8,  1148,     7,    17,   169,    13,     3,   195,    51,     7,\n",
       "             65,  1077,   118,     1]),\n",
       "  'input_text': 'tional agents\\nrealtime event commentary\\ncontent generation\\nthe widest use of llms has probably been ',\n",
       "  'expected_output': 'in content generation using\\nrag the generation can'},\n",
       " {'input_ids': tensor([   3,  687,  257,    8, 1148,    7,   17,  169,   13,    3,  195,   51,\n",
       "             7,   65, 1077,  118,   16,  738, 3381,  338,    3, 6151,    8, 3381,\n",
       "            54,    1]),\n",
       "  'input_text': 'neration\\nthe widest use of llms has probably been in content generation using\\nrag the generation can',\n",
       "  'expected_output': ' be personalised to readers incorporate real\\ntime '},\n",
       " {'input_ids': tensor([   16,   738,  3381,   338,     3,  6151,     8,  3381,    54,    36,\n",
       "              3, 24001,    12,  3962,  6300,   490,    97,     1]),\n",
       "  'input_text': 'in content generation using\\nrag the generation can be personalised to readers incorporate real\\ntime ',\n",
       "  'expected_output': 'trends and be contextually appropriate yarnit is a'},\n",
       " {'input_ids': tensor([   36,     3, 24001,    12,  3962,  6300,   490,    97,  5001,    11,\n",
       "             36, 28131,   120,  2016, 14313,   155,    19,     3,     9,     1]),\n",
       "  'input_text': ' be personalised to readers incorporate real\\ntime trends and be contextually appropriate yarnit is a',\n",
       "  'expected_output': 'n ai based\\ncontent marketing platform that uses ra'},\n",
       " {'input_ids': tensor([ 5001,    11,    36, 28131,   120,  2016, 14313,   155,    19,    46,\n",
       "              3,     9,    23,     3,   390,   738,  1070,  1585,    24,  2284,\n",
       "              3,    52,     9,     1]),\n",
       "  'input_text': 'trends and be contextually appropriate yarnit is an ai based\\ncontent marketing platform that uses ra',\n",
       "  'expected_output': 'g for multiple tasks\\npersonalised recommendation\\nr'},\n",
       " {'input_ids': tensor([    3,    29,     3,     9,    23,     3,   390,   738,  1070,  1585,\n",
       "             24,  2284,     3,  6151,    21,  1317,  4145,     3, 24001, 10919,\n",
       "              3,    52,     1]),\n",
       "  'input_text': 'n ai based\\ncontent marketing platform that uses rag for multiple tasks\\npersonalised recommendation\\nr',\n",
       "  'expected_output': 'ecommendation engines have been a game changes in '},\n",
       " {'input_ids': tensor([    3,   122,    21,  1317,  4145,     3, 24001, 10919, 10919,  7277,\n",
       "             43,   118,     3,     9,   467,  1112,    16,     1]),\n",
       "  'input_text': 'g for multiple tasks\\npersonalised recommendation\\nrecommendation engines have been a game changes in ',\n",
       "  'expected_output': 'the digital\\neconomy llms are capable of powering t'},\n",
       " {'input_ids': tensor([   3,   15,  287,  904,   26,  257, 7277,   43,  118,    3,    9,  467,\n",
       "          1112,   16,    8, 1125, 2717,    3,  195,   51,    7,   33, 3919,   13,\n",
       "           579,   53,    3,   17,    1]),\n",
       "  'input_text': 'ecommendation engines have been a game changes in the digital\\neconomy llms are capable of powering t',\n",
       "  'expected_output': 'he next evolution in content\\nrecommendations check'},\n",
       " {'input_ids': tensor([   8, 1125, 2717,    3,  195,   51,    7,   33, 3919,   13,  579,   53,\n",
       "             8,  416, 9009,   16,  738, 5719,  691,    1]),\n",
       "  'input_text': 'the digital\\neconomy llms are capable of powering the next evolution in content\\nrecommendations check',\n",
       "  'expected_output': ' out amans blog on the utility of llms in\\nrecommen'},\n",
       " {'input_ids': tensor([   3,   88,  416, 9009,   16,  738, 5719,  691,   91,  183, 3247,  875,\n",
       "            30,    8, 6637,   13,    3,  195,   51,    7,   16,    3,   60,  287,\n",
       "           904,    1]),\n",
       "  'input_text': 'he next evolution in content\\nrecommendations check out amans blog on the utility of llms in\\nrecommen',\n",
       "  'expected_output': 'dation systems\\nvirtual assistants\\nvirtual personal'},\n",
       " {'input_ids': tensor([   91,   183,  3247,   875,    30,     8,  6637,    13,     3,   195,\n",
       "             51,     7,    16, 10919,  1002,  4291,  6165,     7,  4291,   525,\n",
       "              1]),\n",
       "  'input_text': ' out amans blog on the utility of llms in\\nrecommendation systems\\nvirtual assistants\\nvirtual personal',\n",
       "  'expected_output': ' assistants like siri alexa and others are in plan'},\n",
       " {'input_ids': tensor([   3,   26,  257, 1002, 4291, 6165,    7, 4291,  525, 6165,    7,  114,\n",
       "           108,   52,   23, 1240,  226,    9,   11,  717,   33,   16,  515,    1]),\n",
       "  'input_text': 'dation systems\\nvirtual assistants\\nvirtual personal assistants like siri alexa and others are in plan',\n",
       "  'expected_output': 's to use\\nllms to enhance the experience coupled wi'},\n",
       " {'input_ids': tensor([ 6165,     7,   114,   108,    52,    23,  1240,   226,     9,    11,\n",
       "            717,    33,    16,  1390,    12,   169,     3,   195,    51,     7,\n",
       "             12,  3391,     8,   351, 14286, 11064,     1]),\n",
       "  'input_text': ' assistants like siri alexa and others are in plans to use\\nllms to enhance the experience coupled wi',\n",
       "  'expected_output': 'th more context on user\\nbehaviour these assistants'},\n",
       " {'input_ids': tensor([    3,     7,    12,   169,     3,   195,    51,     7,    12,  3391,\n",
       "              8,   351, 14286,    28,    72,  2625,    30,  1139,  7916,   175,\n",
       "           6165,     7,     1]),\n",
       "  'input_text': 's to use\\nllms to enhance the experience coupled with more context on user\\nbehaviour these assistants',\n",
       "  'expected_output': ' can become highly personalised  \\nkeep calm  build'},\n",
       " {'input_ids': tensor([    3,   189,    72,  2625,    30,  1139,  7916,   175,  6165,     7,\n",
       "             54,   582,  1385,     3, 24001,   453,  4447,   918,     1]),\n",
       "  'input_text': 'th more context on user\\nbehaviour these assistants can become highly personalised  \\nkeep calm  build',\n",
       "  'expected_output': ' ai7promptsearch relevant\\ninformation\\nllm\\nendpoint'},\n",
       " {'input_ids': tensor([   54,   582,  1385,     3, 24001,   453,  4447,   918,     3,     9,\n",
       "             23,   940,  1409,  1167,    17, 13173,  2193,   251,     3,   195,\n",
       "             51,   414,  2700,     1]),\n",
       "  'input_text': ' can become highly personalised  \\nkeep calm  build ai7promptsearch relevant\\ninformation\\nllm\\nendpoint',\n",
       "  'expected_output': '      \\nknowledge sources\\nabhinav kimothirag archit'},\n",
       " {'input_ids': tensor([    3,     9,    23,   940,  1409,  1167,    17, 13173,  2193,   251,\n",
       "              3,   195,    51,   414,  2700,  1103,  2836,   703,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436,  6151, 11508,   155,     1]),\n",
       "  'input_text': ' ai7promptsearch relevant\\ninformation\\nllm\\nendpoint      \\nknowledge sources\\nabhinav kimothirag archit',\n",
       "  'expected_output': 'ecture\\nrag architecture \\nlets revisit the five hig'},\n",
       " {'input_ids': tensor([ 1103,  2836,   703,  2907,     9,   208,     3, 19754,    32,  7436,\n",
       "           6151,  4648,     3,  6151,  4648,  8857, 20442,     8,   874,  7102,\n",
       "            122,     1]),\n",
       "  'input_text': '      \\nknowledge sources\\nabhinav kimothirag architecture\\nrag architecture \\nlets revisit the five hig',\n",
       "  'expected_output': 'h level steps of an rag enabled system\\nprompt\\nrele'},\n",
       " {'input_ids': tensor([    3,    15,    75,  2693,     3,  6151,  4648,  8857, 20442,     8,\n",
       "            874,   306,   593,  2245,    13,    46,     3,  6151,  9367,   358,\n",
       "           9005,     3,    60,   109,     1]),\n",
       "  'input_text': 'ecture\\nrag architecture \\nlets revisit the five high level steps of an rag enabled system\\nprompt\\nrele',\n",
       "  'expected_output': 'vant \\ncontext\\nprompt  context\\ngenerated response\\nr'},\n",
       " {'input_ids': tensor([   3,  107,  593, 2245,   13,   46,    3, 6151, 9367,  358, 9005, 2193,\n",
       "          2625, 9005, 2625, 6126, 1773,    3,   52,    1]),\n",
       "  'input_text': 'h level steps of an rag enabled system\\nprompt\\nrelevant \\ncontext\\nprompt  context\\ngenerated response\\nr',\n",
       "  'expected_output': 'ag system\\nuser writes a prompt or a query that is '},\n",
       " {'input_ids': tensor([    3,  6451,  2625,  9005,  2625,  6126,  1773,     3,  6151,   358,\n",
       "           1139, 11858,     3,     9,  9005,    42,     3,     9, 11417,    24,\n",
       "             19,     1]),\n",
       "  'input_text': 'vant \\ncontext\\nprompt  context\\ngenerated response\\nrag system\\nuser writes a prompt or a query that is ',\n",
       "  'expected_output': 'passed to an orchestrator\\norchestrator sends a sea'},\n",
       " {'input_ids': tensor([    3,     9,   122,   358,  1139, 11858,     3,     9,  9005,    42,\n",
       "              3,     9, 11417,    24,    19,  2804,    12,    46, 13873,    17,\n",
       "            127, 13873,    17,   127,  1299,     7,     3,     9,  2805,     1]),\n",
       "  'input_text': 'ag system\\nuser writes a prompt or a query that is passed to an orchestrator\\norchestrator sends a sea',\n",
       "  'expected_output': 'rch query to the retriever\\nretriever fetches the r'},\n",
       " {'input_ids': tensor([ 2804,    12,    46, 13873,    17,   127, 13873,    17,   127,  1299,\n",
       "              7,     3,     9,   960, 11417,    12,     8, 21830,    52, 21830,\n",
       "             52, 27109,    15,     7,     8,     3,    52,     1]),\n",
       "  'input_text': 'passed to an orchestrator\\norchestrator sends a search query to the retriever\\nretriever fetches the r',\n",
       "  'expected_output': 'elevant information from the knowledge sources and'},\n",
       " {'input_ids': tensor([    3,    52,   524, 11417,    12,     8, 21830,    52, 21830,    52,\n",
       "          27109,    15,     7,     8,  2193,   251,    45,     8,  1103,  2836,\n",
       "             11,     1]),\n",
       "  'input_text': 'rch query to the retriever\\nretriever fetches the relevant information from the knowledge sources and',\n",
       "  'expected_output': ' sends back\\norchestrator augments the prompt with '},\n",
       " {'input_ids': tensor([25377,   288,   251,    45,     8,  1103,  2836,    11,  1299,     7,\n",
       "            223, 13873,    17,   127, 15189,     7,     8,  9005,    28,     1]),\n",
       "  'input_text': 'elevant information from the knowledge sources and sends back\\norchestrator augments the prompt with ',\n",
       "  'expected_output': 'the context and sends to the llm\\nllm responds with'},\n",
       " {'input_ids': tensor([ 1299,     7,   223, 13873,    17,   127, 15189,     7,     8,  9005,\n",
       "             28,     8,  2625,    11,  1299,     7,    12,     8,     3,   195,\n",
       "             51,     3,   195,    51,  3531,     7,    28,     1]),\n",
       "  'input_text': ' sends back\\norchestrator augments the prompt with the context and sends to the llm\\nllm responds with',\n",
       "  'expected_output': ' the generated text which is displayed to the user'},\n",
       " {'input_ids': tensor([   8, 2625,   11, 1299,    7,   12,    8,    3,  195,   51,    3,  195,\n",
       "            51, 3531,    7,   28,    8, 6126, 1499,   84,   19, 6099,   12,    8,\n",
       "          1139,    1]),\n",
       "  'input_text': 'the context and sends to the llm\\nllm responds with the generated text which is displayed to the user',\n",
       "  'expected_output': ' via the orchestrator\\ntwo pipelines become importa'},\n",
       " {'input_ids': tensor([    8,  6126,  1499,    84,    19,  6099,    12,     8,  1139,  1009,\n",
       "              8, 13873,    17,   127,   192, 12045,     7,   582,  4830,     9,\n",
       "              1]),\n",
       "  'input_text': ' the generated text which is displayed to the user via the orchestrator\\ntwo pipelines become importa',\n",
       "  'expected_output': 'nt in setting up the rag system the first one bein'},\n",
       " {'input_ids': tensor([ 1009,     8, 13873,    17,   127,   192, 12045,     7,   582,   359,\n",
       "             16,  1898,    95,     8,     3,  6151,   358,     8,   166,    80,\n",
       "             36,    77,     1]),\n",
       "  'input_text': ' via the orchestrator\\ntwo pipelines become important in setting up the rag system the first one bein',\n",
       "  'expected_output': 'g\\nsetting up the knowledge sources for efficient s'},\n",
       " {'input_ids': tensor([   3,   29,   17,   16, 1898,   95,    8,    3, 6151,  358,    8,  166,\n",
       "            80,  271, 1898,   95,    8, 1103, 2836,   21, 2918,    3,    7,    1]),\n",
       "  'input_text': 'nt in setting up the rag system the first one being\\nsetting up the knowledge sources for efficient s',\n",
       "  'expected_output': 'earch and retrieval and the\\nsecond one being the f'},\n",
       " {'input_ids': tensor([    3,   122,  1898,    95,     8,  1103,  2836,    21,  2918,   960,\n",
       "             11, 24515,   138,    11,     8,   511,    80,   271,     8,     3,\n",
       "             89,     1]),\n",
       "  'input_text': 'g\\nsetting up the knowledge sources for efficient search and retrieval and the\\nsecond one being the f',\n",
       "  'expected_output': 'ive steps of the generation \\nindexing pipeline\\ndat'},\n",
       " {'input_ids': tensor([    3,    15,  7064,    11, 24515,   138,    11,     8,   511,    80,\n",
       "            271,     8,   874,  2245,    13,     8,  3381,  5538,    53, 12045,\n",
       "           3927,     1]),\n",
       "  'input_text': 'earch and retrieval and the\\nsecond one being the five steps of the generation \\nindexing pipeline\\ndat',\n",
       "  'expected_output': 'a for the knowledge is ingested from the source an'},\n",
       " {'input_ids': tensor([    3,   757,  2245,    13,     8,  3381,  5538,    53, 12045,   331,\n",
       "             21,     8,  1103,    19,     3,    53,    15,  6265,    45,     8,\n",
       "           1391,    46,     1]),\n",
       "  'input_text': 'ive steps of the generation \\nindexing pipeline\\ndata for the knowledge is ingested from the source an',\n",
       "  'expected_output': 'd indexed this\\ninvolves steps like splitting creat'},\n",
       " {'input_ids': tensor([    3,     9,    21,     8,  1103,    19,     3,    53,    15,  6265,\n",
       "             45,     8,  1391,    11,     3, 30564,    48,  5806,  2245,   114,\n",
       "          28503,  8830,     1]),\n",
       "  'input_text': 'a for the knowledge is ingested from the source and indexed this\\ninvolves steps like splitting creat',\n",
       "  'expected_output': 'ion of embeddings and storage of\\ndata\\nrag pipeline'},\n",
       " {'input_ids': tensor([    3,    26,     3, 30564,    48,  5806,  2245,   114, 28503,  3409,\n",
       "             13, 25078,    26,    53,     7,    11,  1606,    13,   331,     3,\n",
       "           6151, 12045,     1]),\n",
       "  'input_text': 'd indexed this\\ninvolves steps like splitting creation of embeddings and storage of\\ndata\\nrag pipeline',\n",
       "  'expected_output': '\\nthis involves the actual rag process which takes '},\n",
       " {'input_ids': tensor([    3,    23,   106,    13, 25078,    26,    53,     7,    11,  1606,\n",
       "             13,   331,     3,  6151, 12045,    48,  5806,     8,  1805,     3,\n",
       "           6151,   433,    84,  1217,     1]),\n",
       "  'input_text': 'ion of embeddings and storage of\\ndata\\nrag pipeline\\nthis involves the actual rag process which takes ',\n",
       "  'expected_output': 'the user query at\\nrun time and retrieves the relev'},\n",
       " {'input_ids': tensor([   48,  5806,     8,  1805,     3,  6151,   433,    84,  1217,     8,\n",
       "           1139, 11417,    44,   661,    97,    11, 21830,     7,     8, 22560,\n",
       "              1]),\n",
       "  'input_text': '\\nthis involves the actual rag process which takes the user query at\\nrun time and retrieves the relev',\n",
       "  'expected_output': 'ant data from the index then passes\\nthat to the mo'},\n",
       " {'input_ids': tensor([    8,  1139, 11417,    44,   661,    97,    11, 21830,     7,     8,\n",
       "           2193,   331,    45,     8,  5538,   258,  9016,    24,    12,     8,\n",
       "           2288,     1]),\n",
       "  'input_text': 'the user query at\\nrun time and retrieves the relevant data from the index then passes\\nthat to the mo',\n",
       "  'expected_output': 'del\\nkeep calm  build ai8retriever\\nfetch\\nshort cont'},\n",
       " {'input_ids': tensor([    3,   288,   331,    45,     8,  5538,   258,  9016,    24,    12,\n",
       "              8,   825,   453,  4447,   918,     3,     9,    23,   927,    60,\n",
       "           1788,  3258, 27109,   710,  3622,     1]),\n",
       "  'input_text': 'ant data from the index then passes\\nthat to the model\\nkeep calm  build ai8retriever\\nfetch\\nshort cont',\n",
       "  'expected_output': 'ext on the fly indexing\\nabhinav kimothiindexing pi'},\n",
       " {'input_ids': tensor([   20,    40,   453,  4447,   918,     3,     9,    23,   927,    60,\n",
       "           1788,  3258, 27109,   710,  2625,    30,     8,  3971,  5538,    53,\n",
       "            703,  2907,     9,   208,     3, 19754,    32,  7436, 18288,    53,\n",
       "           2816,     1]),\n",
       "  'input_text': 'del\\nkeep calm  build ai8retriever\\nfetch\\nshort context on the fly indexing\\nabhinav kimothiindexing pi',\n",
       "  'expected_output': 'peline\\nindexing pipeline \\nthe indexing pipeline se'},\n",
       " {'input_ids': tensor([    3, 10398,    30,     8,  3971,  5538,    53,   703,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436, 18288,    53, 12045,  5538,    53,\n",
       "          12045,     8,  5538,    53, 12045,   142,     1]),\n",
       "  'input_text': 'ext on the fly indexing\\nabhinav kimothiindexing pipeline\\nindexing pipeline \\nthe indexing pipeline se',\n",
       "  'expected_output': 'ts up the knowledge source for the rag system it i'},\n",
       " {'input_ids': tensor([  158,   747,  5538,    53, 12045,     8,  5538,    53, 12045,  3369,\n",
       "             95,     8,  1103,  1391,    21,     8,     3,  6151,   358,    34,\n",
       "              3,    23,     1]),\n",
       "  'input_text': 'peline\\nindexing pipeline \\nthe indexing pipeline sets up the knowledge source for the rag system it i',\n",
       "  'expected_output': 's\\ngenerally considered an offline process however '},\n",
       " {'input_ids': tensor([    3,    17,     7,    95,     8,  1103,  1391,    21,     8,     3,\n",
       "           6151,   358,    34,    19,  2389,  1702,    46, 13461,   433,   983,\n",
       "              1]),\n",
       "  'input_text': 'ts up the knowledge source for the rag system it is\\ngenerally considered an offline process however ',\n",
       "  'expected_output': 'information can also be\\nfetched in real time it in'},\n",
       " {'input_ids': tensor([    3,     7,  2389,  1702,    46, 13461,   433,   983,   251,    54,\n",
       "             92,    36,     3,    89, 25872,    16,   490,    97,    34,    16,\n",
       "              1]),\n",
       "  'input_text': 's\\ngenerally considered an offline process however information can also be\\nfetched in real time it in',\n",
       "  'expected_output': 'volves four primary steps\\nloading splitting embedd'},\n",
       " {'input_ids': tensor([  251,    54,    92,    36,     3,    89, 25872,    16,   490,    97,\n",
       "             34,  5806,   662,  2329,  2245, 12115, 28503, 25078,    26,     1]),\n",
       "  'input_text': 'information can also be\\nfetched in real time it involves four primary steps\\nloading splitting embedd',\n",
       "  'expected_output': 'ing storing\\nthis step involves\\nextracting\\ninformat'},\n",
       " {'input_ids': tensor([ 5063,   162,     7,   662,  2329,  2245, 12115, 28503, 25078,    26,\n",
       "             53,     3, 17445,    48,  1147,  5806,  5819,    53,    16,  8995,\n",
       "              1]),\n",
       "  'input_text': 'volves four primary steps\\nloading splitting embedding storing\\nthis step involves\\nextracting\\ninformat',\n",
       "  'expected_output': 'ion from\\ndifferent\\nknowledge sources\\na loading the'},\n",
       " {'input_ids': tensor([    3,    53,     3, 17445,    48,  1147,  5806,  5819,    53,   251,\n",
       "             45,   315,  1103,  2836,     3,     9, 12115,     8,     1]),\n",
       "  'input_text': 'ing storing\\nthis step involves\\nextracting\\ninformation from\\ndifferent\\nknowledge sources\\na loading the',\n",
       "  'expected_output': 'm into\\ndocumentsthis step involves\\nsplitting\\ndocum'},\n",
       " {'input_ids': tensor([    3,    23,   106,    45,   315,  1103,  2836,     3,     9, 12115,\n",
       "            135,   139,  2691,  8048,  1147,  5806, 28503,   103,  6361,     1]),\n",
       "  'input_text': 'ion from\\ndifferent\\nknowledge sources\\na loading them into\\ndocumentsthis step involves\\nsplitting\\ndocum',\n",
       "  'expected_output': 'ents into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks'},\n",
       " {'input_ids': tensor([    3,    51,   139,  2691,  8048,  1147,  5806, 28503,  2691,   139,\n",
       "           2755,  1865,   179, 16749,     7,  2755, 16749,     7,     1]),\n",
       "  'input_text': 'm into\\ndocumentsthis step involves\\nsplitting\\ndocuments into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks',\n",
       "  'expected_output': ' are easier\\nto search and to\\nuse in llm context\\nwi'},\n",
       " {'input_ids': tensor([    3,   295,     7,   139,  2755,  1865,   179, 16749,     7,  2755,\n",
       "          16749,     7,    33,  1842,    12,   960,    11,    12,   169,    16,\n",
       "              3,   195,    51,  2625, 11064,     1]),\n",
       "  'input_text': 'ents into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks are easier\\nto search and to\\nuse in llm context\\nwi',\n",
       "  'expected_output': 'ndowsthis step involves\\nconverting text\\ndocuments '},\n",
       " {'input_ids': tensor([   33,  1842,    12,   960,    11,    12,   169,    16,     3,   195,\n",
       "             51,  2625,  3196,  8048,  1147,  5806,     3, 21049,  1499,  2691,\n",
       "              1]),\n",
       "  'input_text': ' are easier\\nto search and to\\nuse in llm context\\nwindowsthis step involves\\nconverting text\\ndocuments ',\n",
       "  'expected_output': 'into\\nnumerical vectors\\nml models are\\nmathematical\\n'},\n",
       " {'input_ids': tensor([    3,    29, 15198,     7,  8048,  1147,  5806,     3, 21049,  1499,\n",
       "           2691,   139, 25194, 12938,     7,     3,    51,    40,  2250,    33,\n",
       "          18913,     1]),\n",
       "  'input_text': 'ndowsthis step involves\\nconverting text\\ndocuments into\\nnumerical vectors\\nml models are\\nmathematical\\n',\n",
       "  'expected_output': 'models and\\ntherefore require\\nnumerical datathis st'},\n",
       " {'input_ids': tensor([  139, 25194, 12938,     7,     3,    51,    40,  2250,    33, 18913,\n",
       "           2250,    11,  2459,  1457, 25194,   331,  8048,     3,     7,    17,\n",
       "              1]),\n",
       "  'input_text': 'into\\nnumerical vectors\\nml models are\\nmathematical\\nmodels and\\ntherefore require\\nnumerical datathis st',\n",
       "  'expected_output': 'ep involves\\nstoring the\\nembeddings\\nvectors vectors'},\n",
       " {'input_ids': tensor([ 2250,    11,  2459,  1457, 25194,   331,  8048,  1147,  5806,     3,\n",
       "          17445,     8, 25078,    26,    53,     7, 12938,     7, 12938,     7,\n",
       "              1]),\n",
       "  'input_text': 'models and\\ntherefore require\\nnumerical datathis step involves\\nstoring the\\nembeddings\\nvectors vectors',\n",
       "  'expected_output': '\\nare typically stored\\nin vector\\ndatabases which\\nar'},\n",
       " {'input_ids': tensor([    3,    15,   102,  5806,     3, 17445,     8, 25078,    26,    53,\n",
       "              7, 12938,     7, 12938,     7,    33,  3115,  5816,    16, 12938,\n",
       "          16961,    84,  1584,     1]),\n",
       "  'input_text': 'ep involves\\nstoring the\\nembeddings\\nvectors vectors\\nare typically stored\\nin vector\\ndatabases which\\nar',\n",
       "  'expected_output': 'e best suited for\\nsearching\\noffline indexing pipel'},\n",
       " {'input_ids': tensor([   33,  3115,  5816,    16, 12938, 16961,    84,    33,   200,     3,\n",
       "          10967,    21,  4549, 13461,  5538,    53,  7119,    40,     1]),\n",
       "  'input_text': '\\nare typically stored\\nin vector\\ndatabases which\\nare best suited for\\nsearching\\noffline indexing pipel',\n",
       "  'expected_output': 'ines are typically used when a knowledge base\\nwith'},\n",
       " {'input_ids': tensor([    3,    15,   200,     3, 10967,    21,  4549, 13461,  5538,    53,\n",
       "          12045,     7,    33,  3115,   261,   116,     3,     9,  1103,  1247,\n",
       "             28,     1]),\n",
       "  'input_text': 'e best suited for\\nsearching\\noffline indexing pipelines are typically used when a knowledge base\\nwith',\n",
       "  'expected_output': ' large amount of data is being built for repeated '},\n",
       " {'input_ids': tensor([    3,  4477,    33,  3115,   261,   116,     3,     9,  1103,  1247,\n",
       "             28,   508,   866,    13,   331,    19,   271,  1192,    21, 12171,\n",
       "              1]),\n",
       "  'input_text': 'ines are typically used when a knowledge base\\nwith large amount of data is being built for repeated ',\n",
       "  'expected_output': 'usage eg a\\nnumber of enterprise documents manuals '},\n",
       " {'input_ids': tensor([  508,   866,    13,   331,    19,   271,  1192,    21, 12171,  4742,\n",
       "              3,    15,   122,     3,     9,   381,    13,  5399,  2691,  3354,\n",
       "              7,     1]),\n",
       "  'input_text': ' large amount of data is being built for repeated usage eg a\\nnumber of enterprise documents manuals ',\n",
       "  'expected_output': 'etc \\nin cases where only a fixed small amount of o'},\n",
       " {'input_ids': tensor([4742,    3,   15,  122,    3,    9,  381,   13, 5399, 2691, 3354,    7,\n",
       "           672,   16, 1488,  213,  163,    3,    9, 3599,  422,  866,   13,    3,\n",
       "            32,    1]),\n",
       "  'input_text': 'usage eg a\\nnumber of enterprise documents manuals etc \\nin cases where only a fixed small amount of o',\n",
       "  'expected_output': 'ne time data is required\\neg a 300 word blog there '},\n",
       " {'input_ids': tensor([ 672,   16, 1488,  213,  163,    3,    9, 3599,  422,  866,   13,   80,\n",
       "            97,  331,   19,  831,    3,   15,  122,    3,    9, 3147, 1448,  875,\n",
       "           132,    1]),\n",
       "  'input_text': 'etc \\nin cases where only a fixed small amount of one time data is required\\neg a 300 word blog there ',\n",
       "  'expected_output': 'is no need for storing the data the blog\\ntext can '},\n",
       " {'input_ids': tensor([    3,    29,    15,    97,   331,    19,   831,     3,    15,   122,\n",
       "              3,     9,  3147,  1448,   875,   132,    19,   150,   174,    21,\n",
       "              3, 17445,     8,   331,     8,   875,  1499,    54,     1]),\n",
       "  'input_text': 'ne time data is required\\neg a 300 word blog there is no need for storing the data the blog\\ntext can ',\n",
       "  'expected_output': 'either be directly passed in the llm context windo'},\n",
       " {'input_ids': tensor([   19,   150,   174,    21,     3, 17445,     8,   331,     8,   875,\n",
       "           1499,    54,   893,    36,  1461,  2804,    16,     8,     3,   195,\n",
       "             51,  2625,  2943,    32,     1]),\n",
       "  'input_text': 'is no need for storing the data the blog\\ntext can either be directly passed in the llm context windo',\n",
       "  'expected_output': 'w or a\\ntemporary vector index can be created\\nuserp'},\n",
       " {'input_ids': tensor([  893,    36,  1461,  2804,    16,     8,     3,   195,    51,  2625,\n",
       "           2034,    42,     3,     9,  7234, 12938,  5538,    54,    36,   990,\n",
       "           1139,   102,     1]),\n",
       "  'input_text': 'either be directly passed in the llm context window or a\\ntemporary vector index can be created\\nuserp',\n",
       "  'expected_output': 'rompt  prompt  context \\nllm response\\nno search\\nnee'},\n",
       " {'input_ids': tensor([    3,   210,    42,     3,     9,  7234, 12938,  5538,    54,    36,\n",
       "            990,  1139,  1409,  1167,    17,  9005,  2625,     3,   195,    51,\n",
       "           1773,   150,   960,     3,    29,    15,    15,     1]),\n",
       "  'input_text': 'w or a\\ntemporary vector index can be created\\nuserprompt  prompt  context \\nllm response\\nno search\\nnee',\n",
       "  'expected_output': 'ded since\\ncontext is fixed\\nkeep calm  build ai9thi'},\n",
       " {'input_ids': tensor([   3, 3522,  102,   17, 9005, 2625,    3,  195,   51, 1773,  150,  960,\n",
       "           906,  437, 2625,   19, 3599,  453, 4447,  918,    3,    9,   23, 1298,\n",
       "          7436,    1]),\n",
       "  'input_text': 'rompt  prompt  context \\nllm response\\nno search\\nneeded since\\ncontext is fixed\\nkeep calm  build ai9thi',\n",
       "  'expected_output': 's is a good time to introduce two popular framewor'},\n",
       " {'input_ids': tensor([  20,   26,  437, 2625,   19, 3599,  453, 4447,  918,    3,    9,   23,\n",
       "          1298, 8048,   19,    3,    9,  207,   97,   12, 4277,  192, 1012, 2835,\n",
       "           210,  127,    1]),\n",
       "  'input_text': 'ded since\\ncontext is fixed\\nkeep calm  build ai9this is a good time to introduce two popular framewor',\n",
       "  'expected_output': 'ks that are being used to\\ndevelop llm powered appl'},\n",
       " {'input_ids': tensor([    3,     7,    19,     3,     9,   207,    97,    12,  4277,   192,\n",
       "           1012,  4732,     7,    24,    33,   271,   261,    12,  1344,     3,\n",
       "            195,    51, 10028,  1120,    40,     1]),\n",
       "  'input_text': 's is a good time to introduce two popular frameworks that are being used to\\ndevelop llm powered appl',\n",
       "  'expected_output': 'ications\\nabhinav kimothiindexing pipeline loading '},\n",
       " {'input_ids': tensor([    3,   157,     7,    24,    33,   271,   261,    12,  1344,     3,\n",
       "            195,    51, 10028,  1564,   703,  2907,     9,   208,     3, 19754,\n",
       "             32,  7436, 18288,    53, 12045, 12115,     1]),\n",
       "  'input_text': 'ks that are being used to\\ndevelop llm powered applications\\nabhinav kimothiindexing pipeline loading ',\n",
       "  'expected_output': 'data\\nloading data\\nas weve been discussing the util'},\n",
       " {'input_ids': tensor([    3,   447,  1628,   703,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436, 18288,    53, 12045, 12115,   331, 12115,   331,    38,    62,\n",
       "            162,   118, 12104,     8, 10818,     1]),\n",
       "  'input_text': 'ications\\nabhinav kimothiindexing pipeline loading data\\nloading data\\nas weve been discussing the util',\n",
       "  'expected_output': 'ity of rag is to access data for all sorts of\\nsour'},\n",
       " {'input_ids': tensor([  331, 12115,   331,    38,    62,   162,   118, 12104,     8,  6637,\n",
       "             13,     3,  6151,    19,    12,   592,   331,    21,    66, 10549,\n",
       "             13,     3,     7,  1211,     1]),\n",
       "  'input_text': 'data\\nloading data\\nas weve been discussing the utility of rag is to access data for all sorts of\\nsour',\n",
       "  'expected_output': 'ces these sources can be  \\nwebsites  html pages\\ndo'},\n",
       " {'input_ids': tensor([    3,   485,    13,     3,  6151,    19,    12,   592,   331,    21,\n",
       "             66, 10549,    13,  2836,   175,  2836,    54,    36,  3395,     3,\n",
       "          10500,  1688,   103,     1]),\n",
       "  'input_text': 'ity of rag is to access data for all sorts of\\nsources these sources can be  \\nwebsites  html pages\\ndo',\n",
       "  'expected_output': 'cuments like word pdf etc\\ncode in python java etc\\n'},\n",
       " {'input_ids': tensor([ 1830,   175,  2836,    54,    36,  3395,     3, 10500,  1688,  2691,\n",
       "            114,  1448,  9210,   672,  1081,    16,     3,   102,    63,   189,\n",
       "            106,     3, 27578,   672,     1]),\n",
       "  'input_text': 'ces these sources can be  \\nwebsites  html pages\\ndocuments like word pdf etc\\ncode in python java etc\\n',\n",
       "  'expected_output': 'data in json csv etc\\napis\\nfile directories\\ndatabas'},\n",
       " {'input_ids': tensor([  123,  4128,   114,  1448,  9210,   672,  1081,    16,     3,   102,\n",
       "             63,   189,   106,     3, 27578,   672,   331,    16,     3,   354,\n",
       "            739,     3,    75,     7,   208,   672,     3, 13306,     7,  1042,\n",
       "           2090,   725,   331,  4883,     1]),\n",
       "  'input_text': 'cuments like word pdf etc\\ncode in python java etc\\ndata in json csv etc\\napis\\nfile directories\\ndatabas',\n",
       "  'expected_output': 'es\\nand many more\\nthe first step is to extract the '},\n",
       " {'input_ids': tensor([  331,    16,     3,   354,   739,     3,    75,     7,   208,   672,\n",
       "              3, 13306,     7,  1042,  2090,   725, 16961,    11,   186,    72,\n",
       "              8,   166,  1147,    19,    12,  5819,     8,     1]),\n",
       "  'input_text': 'data in json csv etc\\napis\\nfile directories\\ndatabases\\nand many more\\nthe first step is to extract the ',\n",
       "  'expected_output': 'information present in these source locations \\nuse'},\n",
       " {'input_ids': tensor([   3,   15,    7,   11,  186,   72,    8,  166, 1147,   19,   12, 5819,\n",
       "             8,  251,  915,   16,  175, 1391, 3248,  169,    1]),\n",
       "  'input_text': 'es\\nand many more\\nthe first step is to extract the information present in these source locations \\nuse',\n",
       "  'expected_output': ' cases good for tasks that\\nrequire text search and'},\n",
       " {'input_ids': tensor([ 251,  915,   16,  175, 1391, 3248,  169, 1488,  207,   21, 4145,   24,\n",
       "          1457, 1499,  960,   11,    1]),\n",
       "  'input_text': 'information present in these source locations \\nuse cases good for tasks that\\nrequire text search and',\n",
       "  'expected_output': ' retrieval like\\ninformation retrieval or content\\nd'},\n",
       " {'input_ids': tensor([ 1488,   207,    21,  4145,    24,  1457,  1499,   960,    11, 24515,\n",
       "            138,   114,   251, 24515,   138,    42,   738,     3,    26,     1]),\n",
       "  'input_text': ' cases good for tasks that\\nrequire text search and retrieval like\\ninformation retrieval or content\\nd',\n",
       "  'expected_output': 'iscovery\\nfeatures excels in data indexing and\\nlang'},\n",
       " {'input_ids': tensor([24515,   138,   114,   251, 24515,   138,    42,   738,  9087,   753,\n",
       "          11197,     7,    16,   331,  5538,    53,    11, 12142,     1]),\n",
       "  'input_text': ' retrieval like\\ninformation retrieval or content\\ndiscovery\\nfeatures excels in data indexing and\\nlang',\n",
       "  'expected_output': 'uage model enhancement\\nconnectors provides connect'},\n",
       " {'input_ids': tensor([   19,  9817,    63,   753, 11197,     7,    16,   331,  5538,    53,\n",
       "             11,  1612,   825, 15220, 14195,     7,   795,  1979,     1]),\n",
       "  'input_text': 'iscovery\\nfeatures excels in data indexing and\\nlanguage model enhancement\\nconnectors provides connect',\n",
       "  'expected_output': 'ors to\\naccess data from databases\\nexternal apis or'},\n",
       " {'input_ids': tensor([    3,    76,   545,   825, 15220, 14195,     7,   795, 14195,     7,\n",
       "             12,   592,   331,    45, 16961,  3866,     3, 13306,     7,    42,\n",
       "              1]),\n",
       "  'input_text': 'uage model enhancement\\nconnectors provides connectors to\\naccess data from databases\\nexternal apis or',\n",
       "  'expected_output': ' other datasetsuse cases good for applications tha'},\n",
       " {'input_ids': tensor([   42,     7,    12,   592,   331,    45, 16961,  3866,     3, 13306,\n",
       "              7,    42,   119, 17953,     7,  1074,  1488,   207,    21,  1564,\n",
       "              3,   189,     9,     1]),\n",
       "  'input_text': 'ors to\\naccess data from databases\\nexternal apis or other datasetsuse cases good for applications tha',\n",
       "  'expected_output': 't\\nneed enhanced ai capabilities like\\nlanguage unde'},\n",
       " {'input_ids': tensor([  119, 17953,     7,  1074,  1488,   207,    21,  1564,    24,   174,\n",
       "           8358,     3,     9,    23,  5644,   114,  1612,  3550,     1]),\n",
       "  'input_text': ' other datasetsuse cases good for applications that\\nneed enhanced ai capabilities like\\nlanguage unde',\n",
       "  'expected_output': 'rstanding tasks and more\\nsophisticated text genera'},\n",
       " {'input_ids': tensor([    3,    17,   174,  8358,     3,     9,    23,  5644,   114,  1612,\n",
       "           1705,  4145,    11,    72,  8732,  1499, 17993,     1]),\n",
       "  'input_text': 't\\nneed enhanced ai capabilities like\\nlanguage understanding tasks and more\\nsophisticated text genera',\n",
       "  'expected_output': 'tion\\nfeatures stands out for its versatility\\nand a'},\n",
       " {'input_ids': tensor([    3,    52, 11018,  4145,    11,    72,  8732,  1499,  3381,   753,\n",
       "           5024,    91,    21,   165, 23761,    11,     3,     9,     1]),\n",
       "  'input_text': 'rstanding tasks and more\\nsophisticated text generation\\nfeatures stands out for its versatility\\nand a',\n",
       "  'expected_output': 'daptability in building robust\\napplications with l'},\n",
       " {'input_ids': tensor([    3,  1575,   753,  5024,    91,    21,   165, 23761,    11,  3374,\n",
       "           2020,    16,   740,  6268,  1564,    28,     3,    40,     1]),\n",
       "  'input_text': 'tion\\nfeatures stands out for its versatility\\nand adaptability in building robust\\napplications with l',\n",
       "  'expected_output': 'lms\\nagents makes creating agents using\\nlarge langu'},\n",
       " {'input_ids': tensor([    3,    26,  6789,  2020,    16,   740,  6268,  1564,    28,     3,\n",
       "            195,    51,     7,  4373,   656,  1577,  4373,   338,   508, 12142,\n",
       "             76,     1]),\n",
       "  'input_text': 'daptability in building robust\\napplications with llms\\nagents makes creating agents using\\nlarge langu',\n",
       "  'expected_output': 'age models simple through\\ntheir agents api\\nboth fr'},\n",
       " {'input_ids': tensor([    3,    40,    51,     7,  4373,   656,  1577,  4373,   338,   508,\n",
       "           1612,  2250,   650,   190,    70,  4373,     3, 13306,   321,  2515,\n",
       "              1]),\n",
       "  'input_text': 'lms\\nagents makes creating agents using\\nlarge language models simple through\\ntheir agents api\\nboth fr',\n",
       "  'expected_output': 'ameworks are rapidly evolving and adding new capab'},\n",
       " {'input_ids': tensor([ 1246,  2250,   650,   190,    70,  4373,     3, 13306,   321,  4732,\n",
       "              7,    33,  7313, 16556,    11,  2651,   126,  2468,     9,   115,\n",
       "              1]),\n",
       "  'input_text': 'age models simple through\\ntheir agents api\\nboth frameworks are rapidly evolving and adding new capab',\n",
       "  'expected_output': 'ilities every week\\nits not an eitheror situation a'},\n",
       " {'input_ids': tensor([  183,    15, 13631,    33,  7313, 16556,    11,  2651,   126,  5644,\n",
       "            334,   471,   165,    59,    46,   893,   127,  1419,     3,     9,\n",
       "              1]),\n",
       "  'input_text': 'ameworks are rapidly evolving and adding new capabilities every week\\nits not an eitheror situation a',\n",
       "  'expected_output': 'nd you can use both together or neither \\nkeep calm'},\n",
       " {'input_ids': tensor([   3,  173, 2197,  334,  471,  165,   59,   46,  893,  127, 1419,   11,\n",
       "            25,   54,  169,  321,  544,   42, 7598,  453, 4447,    1]),\n",
       "  'input_text': 'ilities every week\\nits not an eitheror situation and you can use both together or neither \\nkeep calm',\n",
       "  'expected_output': '  build ai10abhinav kimothiindexing pipeline loadi'},\n",
       " {'input_ids': tensor([    3,   727,    25,    54,   169,   321,   544,    42,  7598,   453,\n",
       "           4447,   918,     3,     9,    23,  1714,     9,   115,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436, 18288,    53, 12045,  4002,    23,\n",
       "              1]),\n",
       "  'input_text': 'nd you can use both together or neither \\nkeep calm  build ai10abhinav kimothiindexing pipeline loadi',\n",
       "  'expected_output': 'ng data\\nexample  loading a youtube video\\ntranscrip'},\n",
       " {'input_ids': tensor([  918,     3,     9,    23,  1714,     9,   115,  2907,     9,   208,\n",
       "              3, 19754,    32,  7436, 18288,    53, 12045, 12115,   331,   677,\n",
       "          12115,     3,     9,    25,  9863,   671,  3017,  2685,   102,     1]),\n",
       "  'input_text': '  build ai10abhinav kimothiindexing pipeline loading data\\nexample  loading a youtube video\\ntranscrip',\n",
       "  'expected_output': 't using langchain loaders\\nlets begin by sourcing t'},\n",
       " {'input_ids': tensor([    3,  1725,   331,   677, 12115,     3,     9,    25,  9863,   671,\n",
       "          20146,   338, 12142, 19836,  4002,   277,  8857,  1731,    57,     3,\n",
       "          19035,     3,    17,     1]),\n",
       "  'input_text': 'ng data\\nexample  loading a youtube video\\ntranscript using langchain loaders\\nlets begin by sourcing t',\n",
       "  'expected_output': 'he transcript from this video  \\ndalle 2 explained '},\n",
       " {'input_ids': tensor([    3,    17,   338, 12142, 19836,  4002,   277,  8857,  1731,    57,\n",
       "              3, 19035,     8, 20146,    45,    48,   671,     3,    26, 13701,\n",
       "            204,  5243,     1]),\n",
       "  'input_text': 't using langchain loaders\\nlets begin by sourcing the transcript from this video  \\ndalle 2 explained ',\n",
       "  'expected_output': 'by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbel'},\n",
       " {'input_ids': tensor([    3,    88, 20146,    45,    48,   671,     3,    26, 13701,   204,\n",
       "           5243,    57,   539,     9,    23,  4893,  1986,  4188,  9863,   287,\n",
       "           9237,   208,  1824,    17,   122,   102,     7,  8511,   354,    89,\n",
       "            208,   122,    36,    40,     1]),\n",
       "  'input_text': 'he transcript from this video  \\ndalle 2 explained by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbel',\n",
       "  'expected_output': 'ow is the code using youtubeloader from langchaind'},\n",
       " {'input_ids': tensor([   57,   539,     9,    23,  4893,  1986,  4188,  9863,   287,  9237,\n",
       "            208,  1824,    17,   122,   102,     7,  8511,   354,    89,   208,\n",
       "            122,   666,    19,     8,  1081,   338,    25,  9863,  7134,    49,\n",
       "             45, 12142, 19836,    26,     1]),\n",
       "  'input_text': 'by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbelow is the code using youtubeloader from langchaind',\n",
       "  'expected_output': 'ocumentloaders\\ndocumentpagecontenthave you ever se'},\n",
       " {'input_ids': tensor([    3,  2381,    19,     8,  1081,   338,    25,  9863,  7134,    49,\n",
       "             45, 12142, 19836, 28244,  7134,   277,  1708,  6492, 14819,  7965,\n",
       "             25,   664,   142,     1]),\n",
       "  'input_text': 'ow is the code using youtubeloader from langchaindocumentloaders\\ndocumentpagecontenthave you ever se',\n",
       "  'expected_output': 'en a polar bear\\nplaying bass or a robot painted li'},\n",
       " {'input_ids': tensor([    3,    32,  1071,   297,  7134,   277,  1708,  6492, 14819,  7965,\n",
       "             25,   664,   894,     3,     9,     3,  9618,  4595,  1556,  7981,\n",
       "             42,     3,     9,  7567,  7445,     3,    40,    23,     1]),\n",
       "  'input_text': 'ocumentloaders\\ndocumentpagecontenthave you ever seen a polar bear\\nplaying bass or a robot painted li',\n",
       "  'expected_output': 'ke a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnan'},\n",
       " {'input_ids': tensor([    3,    35,     3,     9,     3,  9618,  4595,  1556,  7981,    42,\n",
       "              3,     9,  7567,  7445,   114,     3,     9,  6686,     9,     7,\n",
       "              7,    32,   737,    17,   317,    78,     3,    26, 13701,   204,\n",
       "             19, 12234,     7,    29,   152,     1]),\n",
       "  'input_text': 'en a polar bear\\nplaying bass or a robot painted like a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnan',\n",
       "  'expected_output': 'd clever systems can work together to make new\\nthi'},\n",
       " {'input_ids': tensor([    3,  1050,     3,     9,  6686,     9,     7,     7,    32,   737,\n",
       "             17,   317,    78,     3,    26, 13701,   204,    19, 12234,     7,\n",
       "             29,   232, 13183,  1002,    54,   161,   544,    12,   143,   126,\n",
       "              3,  7436,     1]),\n",
       "  'input_text': 'ke a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnand clever systems can work together to make new\\nthi',\n",
       "  'expected_output': 'ngs  amplifying our creative potential metadatasou'},\n",
       " {'input_ids': tensor([    3,    26, 13183,  1002,    54,   161,   544,    12,   143,   126,\n",
       "            378, 11483,    40,  8587,    69,  1812,  1055, 26686,     7,  1063,\n",
       "              1]),\n",
       "  'input_text': 'd clever systems can work together to make new\\nthings  amplifying our creative potential metadatasou',\n",
       "  'expected_output': 'rce\\nqtgpskkjfvg title dalle 2 explained descriptio'},\n",
       " {'input_ids': tensor([    3,  1725,     7, 11483,    40,  8587,    69,  1812,  1055, 26686,\n",
       "           7928,     3,  1824,    17,   122,   102,     7,  8511,   354,    89,\n",
       "            208,   122,  2233,     3,    26, 13701,   204,  5243,    20, 11815,\n",
       "             23,    32,     1]),\n",
       "  'input_text': 'ngs  amplifying our creative potential metadatasource\\nqtgpskkjfvg title dalle 2 explained descriptio',\n",
       "  'expected_output': 'n unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytim'},\n",
       " {'input_ids': tensor([    3,    52,   565,     3,  1824,    17,   122,   102,     7,  8511,\n",
       "            354,    89,   208,   122,  2233,     3,    26, 13701,   204,  5243,\n",
       "           4210,  7752,   903, 13362, 11989,  2469,  4389, 30250, 16137,  4893,\n",
       "             23,    63,  2998,     1]),\n",
       "  'input_text': 'rce\\nqtgpskkjfvg title dalle 2 explained description unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytim',\n",
       "  'expected_output': 'gcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000'},\n",
       " {'input_ids': tensor([    3,    29,  7752,   903, 13362, 11989,  2469,  4389, 30250, 16137,\n",
       "           4893,    23,    63,  2998,   122,   287,  2099,  1824,    17,   122,\n",
       "            102,     7,  8511,   354,    89,   208,   122,   107,  1824, 18517,\n",
       "          19962,  8099,  5522,     3, 19818, 26363,  5176,  6078,     1]),\n",
       "  'input_text': 'n unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytimgcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000',\n",
       "  'expected_output': '000 length 167 author openailangchain document loa'},\n",
       " {'input_ids': tensor([    3,   122,   287,  2099,  1824,    17,   122,   102,     7,  8511,\n",
       "            354,    89,   208,   122,   107,  1824, 18517, 19962,  8099,  5522,\n",
       "              3, 19818, 26363,  5176,  6078,  2313,  2475,     3, 27650,  2291,\n",
       "            539,     9,    23,  4612, 19836,  1708,  6899,     9,     1]),\n",
       "  'input_text': 'gcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000000 length 167 author openailangchain document loa',\n",
       "  'expected_output': 'der  youtubeloader\\nloader object\\nthe document obje'},\n",
       " {'input_ids': tensor([ 6078,  2475,     3, 27650,  2291,   539,     9,    23,  4612, 19836,\n",
       "           1708,  4002,    49,    25,  9863,  7134,    49,  4002,    49,  3735,\n",
       "              8,  1708,     3,    32,   115,  1924,     1]),\n",
       "  'input_text': '000 length 167 author openailangchain document loader  youtubeloader\\nloader object\\nthe document obje',\n",
       "  'expected_output': 'ct contains the pagecontent which is the transcrip'},\n",
       " {'input_ids': tensor([   74,    25,  9863,  7134,    49,  4002,    49,  3735,     8,  1708,\n",
       "           3735,  2579,     8,   543, 14819,    84,    19,     8,  3017,  2685,\n",
       "            102,     1]),\n",
       "  'input_text': 'der  youtubeloader\\nloader object\\nthe document object contains the pagecontent which is the transcrip',\n",
       "  'expected_output': 't extracted\\nfrom the youtube video as well as the '},\n",
       " {'input_ids': tensor([    3,    75,    17,  2579,     8,   543, 14819,    84,    19,     8,\n",
       "          20146, 21527,    45,     8,    25,  9863,   671,    38,   168,    38,\n",
       "              8,     1]),\n",
       "  'input_text': 'ct contains the pagecontent which is the transcript extracted\\nfrom the youtube video as well as the ',\n",
       "  'expected_output': 'metadata description\\nkeep calm  build ai11abhinav '},\n",
       " {'input_ids': tensor([    3,    17, 21527,    45,     8,    25,  9863,   671,    38,   168,\n",
       "             38,     8, 26686,  4210,   453,  4447,   918,     3,     9,    23,\n",
       "           2596,     9,   115,  2907,     9,   208,     1]),\n",
       "  'input_text': 't extracted\\nfrom the youtube video as well as the metadata description\\nkeep calm  build ai11abhinav ',\n",
       "  'expected_output': 'kimothiindexing pipeline loading data\\nexample  loa'},\n",
       " {'input_ids': tensor([26686,  4210,   453,  4447,   918,     3,     9,    23,  2596,     9,\n",
       "            115,  2907,     9,   208,     3, 19754,    32,  7436, 18288,    53,\n",
       "          12045, 12115,   331,   677,  6899,     9,     1]),\n",
       "  'input_text': 'metadata description\\nkeep calm  build ai11abhinav kimothiindexing pipeline loading data\\nexample  loa',\n",
       "  'expected_output': 'ding a webpage text using\\nllamaindex reader\\nthis i'},\n",
       " {'input_ids': tensor([    3, 19754,    32,  7436, 18288,    53, 12045, 12115,   331,   677,\n",
       "          12115,     3,     9, 17652,  1499,   338,     3,   195,   265,     9,\n",
       "          18288,  5471,    48,     3,    23,     1]),\n",
       "  'input_text': 'kimothiindexing pipeline loading data\\nexample  loading a webpage text using\\nllamaindex reader\\nthis i',\n",
       "  'expected_output': 's a blog published on medium \\nwhat is a finetuned '},\n",
       " {'input_ids': tensor([    3,    26,    53,     3,     9, 17652,  1499,   338,     3,   195,\n",
       "            265,     9, 18288,  5471,    48,    19,     3,     9,   875,  1790,\n",
       "             30,  2768,   125,    19,     3,     9,  1399,    17,   444,    26,\n",
       "              1]),\n",
       "  'input_text': 'ding a webpage text using\\nllamaindex reader\\nthis is a blog published on medium \\nwhat is a finetuned ',\n",
       "  'expected_output': 'llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67'},\n",
       " {'input_ids': tensor([    3,     7,     3,     9,   875,  1790,    30,  2768,   125,    19,\n",
       "              3,     9,  1399,    17,   444,    26,     3,   195,    51,  4893,\n",
       "           5700,   440,   287,    51, 20779,     9,    23,  9170,   159,     9,\n",
       "          13536,    17,   444,    26,   195,    51,  3708,     1]),\n",
       "  'input_text': 's a blog published on medium \\nwhat is a finetuned llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67',\n",
       "  'expected_output': 'bf0b5df081\\nbelow is the code using simplewebpagere'},\n",
       " {'input_ids': tensor([    3,   195,    51,  4893,  5700,   440,   287,    51, 20779,     9,\n",
       "             23,  9170,   159,     9, 13536,    17,   444,    26,   195,    51,\n",
       "           3708,   115,    89,   632,   115,   755,    26,    89,  4018,   536,\n",
       "            666,    19,     8,  1081,   338,   650,  8398,  6492,    60,     1]),\n",
       "  'input_text': 'llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67bf0b5df081\\nbelow is the code using simplewebpagere',\n",
       "  'expected_output': 'ader from llamahub\\ndocumentid17761da46a3a4ce58590c'},\n",
       " {'input_ids': tensor([    3,   115,    89,   632,   115,   755,    26,    89,  4018,   536,\n",
       "            666,    19,     8,  1081,   338,   650,  8398,  6492,  5236,    49,\n",
       "             45,     3,   195,   265,     9, 16420,  1708,    23,    26, 26793,\n",
       "           4241,    26,     9,  4448,     9,   519,     9,   591,   565,  3449,\n",
       "            755,  2394,    75,     1]),\n",
       "  'input_text': 'bf0b5df081\\nbelow is the code using simplewebpagereader from llamahub\\ndocumentid17761da46a3a4ce58590c',\n",
       "  'expected_output': '65ee446788f\\nembeddingnone metadata excludedembedme'},\n",
       " {'input_ids': tensor([    3,     9,   588,    45,     3,   195,   265,     9, 16420,  1708,\n",
       "             23,    26, 26793,  4241,    26,     9,  4448,     9,   519,     9,\n",
       "            591,   565,  3449,   755,  2394,    75,  4122,    15,    15,  3628,\n",
       "           3708,  4060,    89, 25078,    26,    53,    29,   782, 26686, 17981,\n",
       "           1778,  4143,   526,     1]),\n",
       "  'input_text': 'ader from llamahub\\ndocumentid17761da46a3a4ce58590c65ee446788f\\nembeddingnone metadata excludedembedme',\n",
       "  'expected_output': 'tadatakeys\\nexcludedllmmetadatakeys relationships\\nh'},\n",
       " {'input_ids': tensor([ 7123,    15,    15,  3628,  3708,  4060,    89, 25078,    26,    53,\n",
       "             29,   782, 26686, 17981,  1778,  4143,  3493,     9,  6757,  4397,\n",
       "              7, 19678,   195,    51,  3493,     9,  6757,  4397,     7,  3079,\n",
       "              3,   107,     1]),\n",
       "  'input_text': '65ee446788f\\nembeddingnone metadata excludedembedmetadatakeys\\nexcludedllmmetadatakeys relationships\\nh',\n",
       "  'expected_output': 'ash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca25'},\n",
       " {'input_ids': tensor([    3,    17,     9,  6757,  4397,     7, 19678,   195,    51,  3493,\n",
       "              9,  6757,  4397,     7,  3079,    65,   107,  4389,  4450,   115,\n",
       "            519,  7398,   591,    26,   519, 12982,   536,  8699,   357,   658,\n",
       "           3264,    26,   536,    26,  6348,  3707,    15,   357,    75,   519,\n",
       "             75,   115,    26, 27452,    26,    26,   658,  1828,     1]),\n",
       "  'input_text': 'tadatakeys\\nexcludedllmmetadatakeys relationships\\nhash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca25',\n",
       "  'expected_output': '6fab9fa363e0\\n9ed85 textdoctype htmlhtml langenhead'},\n",
       " {'input_ids': tensor([    3,  3198,  4389,  4450,   115,   519,  7398,   591,    26,   519,\n",
       "          12982,   536,  8699,   357,   658,  3264,    26,   536,    26,  6348,\n",
       "           3707,    15,   357,    75,   519,    75,   115,    26, 27452,    26,\n",
       "             26,   658, 19337, 12644,  1298,    89,     9,  3420,   519,    15,\n",
       "            632,   668,    15,    26,  4433,  1499,  7171,  6137,     3, 10500,\n",
       "          10500,     3, 11265,  3313,     1]),\n",
       "  'input_text': 'ash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca256fab9fa363e0\\n9ed85 textdoctype htmlhtml langenhead',\n",
       "  'expected_output': 'title data\\nrhtruewhat is a finetuned llm finetunin'},\n",
       " {'input_ids': tensor([  431, 12644,  1298,    89,     9,  3420,   519,    15,   632,   668,\n",
       "             15,    26,  4433,  1499,  7171,  6137,     3, 10500, 10500,     3,\n",
       "          11265,  3313, 21869,   331,     3,    52,   107,  2666,    15,  9170,\n",
       "             19,     3,     9,  1399,    17,   444,    26,     3,   195,    51,\n",
       "           1399,    17,   202,    77,     1]),\n",
       "  'input_text': '6fab9fa363e0\\n9ed85 textdoctype htmlhtml langenheadtitle data\\nrhtruewhat is a finetuned llm finetunin',\n",
       "  'expected_output': 'g large language models\\n by abhinav kimothi  \\n\\nbod'},\n",
       " {'input_ids': tensor([ 2233,   331,     3,    52,   107,  2666,    15,  9170,    19,     3,\n",
       "              9,  1399,    17,   444,    26,     3,   195,    51,  1399,    17,\n",
       "            202,    53,   508,  1612,  2250,    57,   703,  2907,     9,   208,\n",
       "              3, 19754,    32,  7436,     3, 19987,     1]),\n",
       "  'input_text': 'title data\\nrhtruewhat is a finetuned llm finetuning large language models\\n by abhinav kimothi  \\n\\nbod',\n",
       "  'expected_output': 'yhtml startcharidxnone endcharidxnone\\ntexttemplate'},\n",
       " {'input_ids': tensor([    3,   122,   508,  1612,  2250,    57,   703,  2907,     9,   208,\n",
       "              3, 19754,    32,  7436,   643, 10500,   456,  3441,  4055,   226,\n",
       "             29,   782,   414,  3441,  4055,   226,    29,   782,  1499,  3524,\n",
       "          17628,     1]),\n",
       "  'input_text': 'g large language models\\n by abhinav kimothi  \\n\\nbodyhtml startcharidxnone endcharidxnone\\ntexttemplate',\n",
       "  'expected_output': 'metadatastrnncontent metadatatemplatekey\\nvalue met'},\n",
       " {'input_ids': tensor([    3,    63, 10500,   456,  3441,  4055,   226,    29,   782,   414,\n",
       "           3441,  4055,   226,    29,   782,  1499,  3524, 17628,  3493,     9,\n",
       "             26,   144, 12163,    29,    29, 14819, 26686,  3524, 17628,  4397,\n",
       "            701,  1736,     1]),\n",
       "  'input_text': 'yhtml startcharidxnone endcharidxnone\\ntexttemplatemetadatastrnncontent metadatatemplatekey\\nvalue met',\n",
       "  'expected_output': 'adataseperatornllamaindex llamahub web page reader'},\n",
       " {'input_ids': tensor([26686,     7,    17,    52,    29,    29, 14819, 26686,  3524, 17628,\n",
       "           4397,   701, 26686,     7,    15,   883,  1016,    29,   195,   265,\n",
       "              9, 18288,     3,   195,   265,     9, 16420,   765,   543,  5471,\n",
       "              1]),\n",
       "  'input_text': 'metadatastrnncontent metadatatemplatekey\\nvalue metadataseperatornllamaindex llamahub web page reader',\n",
       "  'expected_output': '\\nthe llamaindex document object contains more  att'},\n",
       " {'input_ids': tensor([    3,     9,  6757,     7,    15,   883,  1016,    29,   195,   265,\n",
       "              9, 18288,     3,   195,   265,     9, 16420,   765,   543,  5471,\n",
       "              8,     3,   195,   265,     9, 18288,  1708,  3735,  2579,    72,\n",
       "             44,    17,     1]),\n",
       "  'input_text': 'adataseperatornllamaindex llamahub web page reader\\nthe llamaindex document object contains more  att',\n",
       "  'expected_output': 'ributes than a langchain\\ndocument  apart from text'},\n",
       " {'input_ids': tensor([    8,     3,   195,   265,     9, 18288,  1708,  3735,  2579,    72,\n",
       "          12978,   145,     3,     9, 12142, 19836,  1708,  3943,    45,  1499,\n",
       "              1]),\n",
       "  'input_text': '\\nthe llamaindex document object contains more  attributes than a langchain\\ndocument  apart from text',\n",
       "  'expected_output': ' and metadata it also has id templates and other\\nc'},\n",
       " {'input_ids': tensor([    3,  6520,  2810,     7,   145,     3,     9, 12142, 19836,  1708,\n",
       "           3943,    45,  1499,    11, 26686,    34,    92,    65,     3,    23,\n",
       "             26,  7405,    11,   119,     3,    75,     1]),\n",
       "  'input_text': 'ributes than a langchain\\ndocument  apart from text and metadata it also has id templates and other\\nc',\n",
       "  'expected_output': 'ustomizations availableloader object\\nkeep calm  bu'},\n",
       " {'input_ids': tensor([   11, 26686,    34,    92,    65,     3,    23,    26,  7405,    11,\n",
       "            119, 22039,     7,   347,  7134,    49,  3735,   453,  4447,  8524,\n",
       "              1]),\n",
       "  'input_text': ' and metadata it also has id templates and other\\ncustomizations availableloader object\\nkeep calm  bu',\n",
       "  'expected_output': 'ild ai12abhinav kimothiindexing pipeline loading d'},\n",
       " {'input_ids': tensor([  178,   235,    51,  1707,     7,   347,  7134,    49,  3735,   453,\n",
       "           4447,   918,     3,     9,    23,  2122,     9,   115,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436, 18288,    53, 12045, 12115,     3,\n",
       "             26,     1]),\n",
       "  'input_text': 'ustomizations availableloader object\\nkeep calm  build ai12abhinav kimothiindexing pipeline loading d',\n",
       "  'expected_output': 'ata\\nboth langchain and llamaindex offer loader int'},\n",
       " {'input_ids': tensor([    3,   173,    26,     3,     9,    23,  2122,     9,   115,  2907,\n",
       "              9,   208,     3, 19754,    32,  7436, 18288,    53, 12045, 12115,\n",
       "            331,   321, 12142, 19836,    11,     3,   195,   265,     9, 18288,\n",
       "            462,  4002,    49,    16,    17,     1]),\n",
       "  'input_text': 'ild ai12abhinav kimothiindexing pipeline loading data\\nboth langchain and llamaindex offer loader int',\n",
       "  'expected_output': 'egrations with more than a\\nhundred data sources an'},\n",
       " {'input_ids': tensor([   44,     9,   321, 12142, 19836,    11,     3,   195,   265,     9,\n",
       "          18288,   462,  4002,    49,  5660,     7,    28,    72,   145,     3,\n",
       "              9,  6189,   331,  2836,    46,     1]),\n",
       "  'input_text': 'ata\\nboth langchain and llamaindex offer loader integrations with more than a\\nhundred data sources an',\n",
       "  'expected_output': 'd the list keeps on growing\\nlangchain document loa'},\n",
       " {'input_ids': tensor([    3,    15,   122,  2661,     7,    28,    72,   145,     3,     9,\n",
       "           6189,   331,  2836,    11,     8,   570,  5689,    30,  1710, 12142,\n",
       "          19836,  1708,  6899,     9,     1]),\n",
       "  'input_text': 'egrations with more than a\\nhundred data sources and the list keeps on growing\\nlangchain document loa',\n",
       "  'expected_output': 'ders llamahub data loaders\\nllamaindex provides dat'},\n",
       " {'input_ids': tensor([    3,    26,     8,   570,  5689,    30,  1710, 12142, 19836,  1708,\n",
       "           4002,   277,     3,   195,   265,     9, 16420,   331,  4002,   277,\n",
       "              3,   195,   265,     9, 18288,   795,  3927,     1]),\n",
       "  'input_text': 'd the list keeps on growing\\nlangchain document loaders llamahub data loaders\\nllamaindex provides dat',\n",
       "  'expected_output': 'a loaders via\\nllamahublangchain provides integrati'},\n",
       " {'input_ids': tensor([   74,     7,     3,   195,   265,     9, 16420,   331,  4002,   277,\n",
       "              3,   195,   265,     9, 18288,   795,   331,  4002,   277,  1009,\n",
       "              3,   195,   265,     9, 16420,  4612, 19836,   795, 20552,    23,\n",
       "              1]),\n",
       "  'input_text': 'ders llamahub data loaders\\nllamaindex provides data loaders via\\nllamahublangchain provides integrati',\n",
       "  'expected_output': 'ons with a\\nvariety of sources\\nthese document loade'},\n",
       " {'input_ids': tensor([    3,     9,  4002,   277,  1009,     3,   195,   265,     9, 16420,\n",
       "           4612, 19836,   795,  5660,     7,    28,     3,     9,  1196,    13,\n",
       "           2836,   175,  1708,  4002,    15,     1]),\n",
       "  'input_text': 'a loaders via\\nllamahublangchain provides integrations with a\\nvariety of sources\\nthese document loade',\n",
       "  'expected_output': 'rs are particularly helpful in quickly making conn'},\n",
       " {'input_ids': tensor([  30,    7,   28,    3,    9, 1196,   13, 2836,  175, 1708, 4002,  277,\n",
       "            33, 1989, 2690,   16, 1224,  492,  975,   29,    1]),\n",
       "  'input_text': 'ons with a\\nvariety of sources\\nthese document loaders are particularly helpful in quickly making conn',\n",
       "  'expected_output': 'ections\\nand accessing information for specific sou'},\n",
       " {'input_ids': tensor([   3,   52,    7,   33, 1989, 2690,   16, 1224,  492, 5992,   11,  592,\n",
       "            53,  251,   21,  806, 7236,    1]),\n",
       "  'input_text': 'rs are particularly helpful in quickly making connections\\nand accessing information for specific sou',\n",
       "  'expected_output': 'rces custom loaders can also be\\ndeveloped\\nit is wo'},\n",
       " {'input_ids': tensor([   3,   15, 4985,    7,   11,  592,   53,  251,   21,  806, 2836, 1653,\n",
       "          4002,  277,   54,   92,   36, 1597,   34,   19, 2275,    1]),\n",
       "  'input_text': 'ections\\nand accessing information for specific sources custom loaders can also be\\ndeveloped\\nit is wo',\n",
       "  'expected_output': 'rthwhile exploring documentation for both\\nloading '},\n",
       " {'input_ids': tensor([    3,    52,  2319,  1653,  4002,   277,    54,    92,    36,  1597,\n",
       "             34,    19, 20167,  6990,  7192,    21,   321, 12115,     1]),\n",
       "  'input_text': 'rces custom loaders can also be\\ndeveloped\\nit is worthwhile exploring documentation for both\\nloading ',\n",
       "  'expected_output': 'documents from a list of sources may turn out to b'},\n",
       " {'input_ids': tensor([    3,    52,   189, 12124,   109,  6990,  7192,    21,   321, 12115,\n",
       "           2691,    45,     3,     9,   570,    13,  2836,   164,   919,    91,\n",
       "             12,     3,   115,     1]),\n",
       "  'input_text': 'rthwhile exploring documentation for both\\nloading documents from a list of sources may turn out to b',\n",
       "  'expected_output': 'e a complicated\\nprocess make sure to plan for all '},\n",
       " {'input_ids': tensor([2691,   45,    3,    9,  570,   13, 2836,  164,  919,   91,   12,   36,\n",
       "             3,    9, 6446,  433,  143,  417,   12,  515,   21,   66,    1]),\n",
       "  'input_text': 'documents from a list of sources may turn out to be a complicated\\nprocess make sure to plan for all ',\n",
       "  'expected_output': 'the sources and loaders in advance\\nmore often than'},\n",
       " {'input_ids': tensor([   3,   15,    3,    9, 6446,  433,  143,  417,   12,  515,   21,   66,\n",
       "             8, 2836,   11, 4002,  277,   16, 3245,   72,  557,  145,    1]),\n",
       "  'input_text': 'e a complicated\\nprocess make sure to plan for all the sources and loaders in advance\\nmore often than',\n",
       "  'expected_output': ' naught transformationscleanups to the loaded data'},\n",
       " {'input_ids': tensor([    8,  2836,    11,  4002,   277,    16,  3245,    72,   557,   145,\n",
       "              3,    29,  9313,  6586,     7, 16480,   413,     7,    12,     8,\n",
       "          10346,   331,     1]),\n",
       "  'input_text': 'the sources and loaders in advance\\nmore often than naught transformationscleanups to the loaded data',\n",
       "  'expected_output': ' will\\nbe required like removing duplicate content '},\n",
       " {'input_ids': tensor([    3,    29,  9313,  6586,     7, 16480,   413,     7,    12,     8,\n",
       "          10346,   331,    56,    36,   831,   114,     3,  8499, 19197,   738,\n",
       "              1]),\n",
       "  'input_text': ' naught transformationscleanups to the loaded data will\\nbe required like removing duplicate content ',\n",
       "  'expected_output': 'html parsing etc langchain\\nalso provides a variety'},\n",
       " {'input_ids': tensor([   56,    36,   831,   114,     3,  8499, 19197,   738,     3, 10500,\n",
       "            260,     7,    53,   672, 12142, 19836,    92,   795,     3,     9,\n",
       "           1196,     1]),\n",
       "  'input_text': ' will\\nbe required like removing duplicate content html parsing etc langchain\\nalso provides a variety',\n",
       "  'expected_output': ' of document transformershttpspythonlangchaincomdo'},\n",
       " {'input_ids': tensor([    3, 10500,   260,     7,    53,   672, 12142, 19836,    92,   795,\n",
       "              3,     9,  1196,    13,  1708, 19903,     7,  5948,     7,   102,\n",
       "             63,   189,   106,  4612, 19836,   287,    26,    32,     1]),\n",
       "  'input_text': 'html parsing etc langchain\\nalso provides a variety of document transformershttpspythonlangchaincomdo',\n",
       "  'expected_output': 'csgetstartedintroductionhttpsdocsllamaindexaiensta'},\n",
       " {'input_ids': tensor([   13,  1708, 19903,     7,  5948,     7,   102,    63,   189,   106,\n",
       "           4612, 19836,   287,  7171,     7,  2782,  3624,  1054, 20322,    32,\n",
       "           8291,  5948,     7,  7171,     7,   195,   265,     9, 18288,     9,\n",
       "           8065,    17,     9,     1]),\n",
       "  'input_text': ' of document transformershttpspythonlangchaincomdocsgetstartedintroductionhttpsdocsllamaindexaiensta',\n",
       "  'expected_output': 'ble llamaindex\\nlangchain\\nkeep calm  build ai13abhi'},\n",
       " {'input_ids': tensor([    3,    75,     7,  2782,  3624,  1054, 20322,    32,  8291,  5948,\n",
       "              7,  7171,     7,   195,   265,     9, 18288,     9,  8065,  3869,\n",
       "              3,   195,   265,     9, 18288, 12142, 19836,   453,  4447,   918,\n",
       "              3,     9,    23,  2368,     9,   115,   107,    23,     1]),\n",
       "  'input_text': 'csgetstartedintroductionhttpsdocsllamaindexaienstable llamaindex\\nlangchain\\nkeep calm  build ai13abhi',\n",
       "  'expected_output': 'nav kimothiindexing pipeline document splitting\\ndo'},\n",
       " {'input_ids': tensor([    3,  2296,     3,   195,   265,     9, 18288, 12142, 19836,   453,\n",
       "           4447,   918,     3,     9,    23,  2368,     9,   115,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436, 18288,    53, 12045,  1708, 28503,\n",
       "            103,     1]),\n",
       "  'input_text': 'ble llamaindex\\nlangchain\\nkeep calm  build ai13abhinav kimothiindexing pipeline document splitting\\ndo',\n",
       "  'expected_output': 'cument splitting\\nonce the data is loaded the next '},\n",
       " {'input_ids': tensor([    3, 14128,     3, 19754,    32,  7436, 18288,    53, 12045,  1708,\n",
       "          28503,  1708, 28503,   728,     8,   331,    19, 10346,     8,   416,\n",
       "              1]),\n",
       "  'input_text': 'nav kimothiindexing pipeline document splitting\\ndocument splitting\\nonce the data is loaded the next ',\n",
       "  'expected_output': 'step in the indexing pipeline is splitting the\\ndoc'},\n",
       " {'input_ids': tensor([  123,   297, 28503,   728,     8,   331,    19, 10346,     8,   416,\n",
       "           1147,    16,     8,  5538,    53, 12045,    19, 28503,     8,     3,\n",
       "           7171,     1]),\n",
       "  'input_text': 'cument splitting\\nonce the data is loaded the next step in the indexing pipeline is splitting the\\ndoc',\n",
       "  'expected_output': 'uments into manageable chunks the question arises '},\n",
       " {'input_ids': tensor([ 1147,    16,     8,  5538,    53, 12045,    19, 28503,     8,  2691,\n",
       "            139,  1865,   179, 16749,     7,     8,   822,  7931,     7,     1]),\n",
       "  'input_text': 'step in the indexing pipeline is splitting the\\ndocuments into manageable chunks the question arises ',\n",
       "  'expected_output': 'around the need of this\\nstep why is splitting of d'},\n",
       " {'input_ids': tensor([    3,    76,  4128,   139,  1865,   179, 16749,     7,     8,   822,\n",
       "           7931,     7,   300,     8,   174,    13,    48,  1147,   572,    19,\n",
       "          28503,    13,     3,    26,     1]),\n",
       "  'input_text': 'uments into manageable chunks the question arises around the need of this\\nstep why is splitting of d',\n",
       "  'expected_output': 'ocuments necessary there are two reasons for that '},\n",
       " {'input_ids': tensor([  300,     8,   174,    13,    48,  1147,   572,    19, 28503,    13,\n",
       "           2691,  1316,   132,    33,   192,  2081,    21,    24,     1]),\n",
       "  'input_text': 'around the need of this\\nstep why is splitting of documents necessary there are two reasons for that ',\n",
       "  'expected_output': ' \\nease of search context window size\\nlarge chunks '},\n",
       " {'input_ids': tensor([    3,    32,  1071,  4128,  1316,   132,    33,   192,  2081,    21,\n",
       "             24,  4226,    13,   960,  2625,  2034,   812,   508, 16749,     7,\n",
       "              1]),\n",
       "  'input_text': 'ocuments necessary there are two reasons for that  \\nease of search context window size\\nlarge chunks ',\n",
       "  'expected_output': 'of data are harder to\\nsearch over splitting data i'},\n",
       " {'input_ids': tensor([ 4226,    13,   960,  2625,  2034,   812,   508, 16749,     7,    13,\n",
       "            331,    33,  7501,    12,   960,   147, 28503,   331,     3,    23,\n",
       "              1]),\n",
       "  'input_text': ' \\nease of search context window size\\nlarge chunks of data are harder to\\nsearch over splitting data i',\n",
       "  'expected_output': 'nto\\nsmaller chunks therefore helps in\\nbetter index'},\n",
       " {'input_ids': tensor([   13,   331,    33,  7501,    12,   960,   147, 28503,   331,   139,\n",
       "           2755, 16749,     7,  2459,  1691,    16,   394,  5538,     1]),\n",
       "  'input_text': 'of data are harder to\\nsearch over splitting data into\\nsmaller chunks therefore helps in\\nbetter index',\n",
       "  'expected_output': 'ationllms allow only a finite number of\\ntokens in '},\n",
       " {'input_ids': tensor([    3,    29,   235,  2755, 16749,     7,  2459,  1691,    16,   394,\n",
       "           5538,   257,   195,    51,     7,   995,   163,     3,     9,   361,\n",
       "           7980,   381,    13, 14145,     7,    16,     1]),\n",
       "  'input_text': 'nto\\nsmaller chunks therefore helps in\\nbetter indexationllms allow only a finite number of\\ntokens in ',\n",
       "  'expected_output': 'prompts and completions the\\ncontext therefore cann'},\n",
       " {'input_ids': tensor([    3,   257,   195,    51,     7,   995,   163,     3,     9,   361,\n",
       "           7980,   381,    13, 14145,     7,    16,  9005,     7,    11,  6929,\n",
       "              7,     8,  2625,  2459,    54,    29,     1]),\n",
       "  'input_text': 'ationllms allow only a finite number of\\ntokens in prompts and completions the\\ncontext therefore cann',\n",
       "  'expected_output': 'ot be larger than\\nwhat the context window permits\\n'},\n",
       " {'input_ids': tensor([ 9005,     7,    11,  6929,     7,     8,  2625,  2459,  1178,    36,\n",
       "           2186,   145,   125,     8,  2625,  2034, 14079,     1]),\n",
       "  'input_text': 'prompts and completions the\\ncontext therefore cannot be larger than\\nwhat the context window permits\\n',\n",
       "  'expected_output': 'chunking strategies \\nwhile splitting documents int'},\n",
       " {'input_ids': tensor([    3,    32,    17,    36,  2186,   145,   125,     8,  2625,  2034,\n",
       "          14079, 16749,    53,  3266,   298, 28503,  2691,    16,    17,     1]),\n",
       "  'input_text': 'ot be larger than\\nwhat the context window permits\\nchunking strategies \\nwhile splitting documents int',\n",
       "  'expected_output': 'o chunks might sound a simple concept there are\\nce'},\n",
       " {'input_ids': tensor([16749,    53,  3266,   298, 28503,  2691,   139, 16749,     7,   429,\n",
       "           1345,     3,     9,   650,  2077,   132,    33,   197,     1]),\n",
       "  'input_text': 'chunking strategies \\nwhile splitting documents into chunks might sound a simple concept there are\\nce',\n",
       "  'expected_output': 'rtain best practices that researchers have discove'},\n",
       " {'input_ids': tensor([    3,    32, 16749,     7,   429,  1345,     3,     9,   650,  2077,\n",
       "            132,    33,   824,   200,  2869,    24,  4768,    43,  5025,    32,\n",
       "            162,     1]),\n",
       "  'input_text': 'o chunks might sound a simple concept there are\\ncertain best practices that researchers have discove',\n",
       "  'expected_output': 'red there are a few\\nconsiderations that may influe'},\n",
       " {'input_ids': tensor([   3,   52,   17,    9,   77,  200, 2869,   24, 4768,   43, 3883,  132,\n",
       "            33,    3,    9,  360, 4587,    7,   24,  164,   16, 6947,   15,    1]),\n",
       "  'input_text': 'rtain best practices that researchers have discovered there are a few\\nconsiderations that may influe',\n",
       "  'expected_output': 'nce the overall chunking strategy \\nnature of conte'},\n",
       " {'input_ids': tensor([ 1131,   132,    33,     3,     9,   360,  4587,     7,    24,   164,\n",
       "           2860,     8,  1879, 16749,    53,  1998,  1405,    13,  3622,    15,\n",
       "              1]),\n",
       "  'input_text': 'red there are a few\\nconsiderations that may influence the overall chunking strategy \\nnature of conte',\n",
       "  'expected_output': 'nt\\nconsider whether you are working with lengthy d'},\n",
       " {'input_ids': tensor([    3,  3772,     8,  1879, 16749,    53,  1998,  1405,    13,   738,\n",
       "           1099,   823,    25,    33,   464,    28, 17574,     3,    26,     1]),\n",
       "  'input_text': 'nce the overall chunking strategy \\nnature of content\\nconsider whether you are working with lengthy d',\n",
       "  'expected_output': 'ocuments such as articles or\\nbooks or shorter cont'},\n",
       " {'input_ids': tensor([    3,    29,    17,  1099,   823,    25,    33,   464,    28, 17574,\n",
       "           2691,   224,    38,  2984,    42,  1335,    42, 10951,  3622,     1]),\n",
       "  'input_text': 'nt\\nconsider whether you are working with lengthy documents such as articles or\\nbooks or shorter cont',\n",
       "  'expected_output': 'ent like tweets or instant messages the chosen mod'},\n",
       " {'input_ids': tensor([    3,    32,  1071,  4128,   224,    38,  2984,    42,  1335,    42,\n",
       "          10951,   738,   114, 10657,     7,    42,  5087,  4175,     8,  3934,\n",
       "           1794,     1]),\n",
       "  'input_text': 'ocuments such as articles or\\nbooks or shorter content like tweets or instant messages the chosen mod',\n",
       "  'expected_output': 'el for\\nyour goal and consequently the appropriate '},\n",
       " {'input_ids': tensor([    3,   295,   114, 10657,     7,    42,  5087,  4175,     8,  3934,\n",
       "            825,    21,    39,  1288,    11, 21612,   120,     8,  2016,     1]),\n",
       "  'input_text': 'ent like tweets or instant messages the chosen model for\\nyour goal and consequently the appropriate ',\n",
       "  'expected_output': 'chunking strategy depend on your\\nresponse\\nembeddin'},\n",
       " {'input_ids': tensor([    3,    15,    40,    21,    39,  1288,    11, 21612,   120,     8,\n",
       "           2016, 16749,    53,  1998,  6002,    30,    39,  1773, 25078,  2644,\n",
       "              1]),\n",
       "  'input_text': 'el for\\nyour goal and consequently the appropriate chunking strategy depend on your\\nresponse\\nembeddin',\n",
       "  'expected_output': 'g model being used\\nwe will discuss embeddings in d'},\n",
       " {'input_ids': tensor([16749,    53,  1998,  6002,    30,    39,  1773, 25078,    26,    53,\n",
       "            825,   271,   261,    62,    56,  2497, 25078,    26,    53,     7,\n",
       "             16,     3,    26,     1]),\n",
       "  'input_text': 'chunking strategy depend on your\\nresponse\\nembedding model being used\\nwe will discuss embeddings in d',\n",
       "  'expected_output': 'etail in the next section but the choice of\\nembedd'},\n",
       " {'input_ids': tensor([    3,   122,   825,   271,   261,    62,    56,  2497, 25078,    26,\n",
       "             53,     7,    16,  2736,    16,     8,   416,  1375,    68,     8,\n",
       "           1160,    13, 25078,    26,     1]),\n",
       "  'input_text': 'g model being used\\nwe will discuss embeddings in detail in the next section but the choice of\\nembedd',\n",
       "  'expected_output': 'ing model also dictates the chunking strategy some'},\n",
       " {'input_ids': tensor([    3,    15,  5756,    16,     8,   416,  1375,    68,     8,  1160,\n",
       "             13, 25078,    26,    53,   825,    92, 19810,     7,     8, 16749,\n",
       "             53,  1998,   128,     1]),\n",
       "  'input_text': 'etail in the next section but the choice of\\nembedding model also dictates the chunking strategy some',\n",
       "  'expected_output': ' models perform\\nbetter with chunks of specific len'},\n",
       " {'input_ids': tensor([    3,    53,   825,    92, 19810,     7,     8, 16749,    53,  1998,\n",
       "            128,  2250,  1912,   394,    28, 16749,     7,    13,   806,    90,\n",
       "             29,     1]),\n",
       "  'input_text': 'ing model also dictates the chunking strategy some models perform\\nbetter with chunks of specific len',\n",
       "  'expected_output': 'gth\\nexpected length and complexity of user queries'},\n",
       " {'input_ids': tensor([ 2250,  1912,   394,    28, 16749,     7,    13,   806,  2475,  1644,\n",
       "           2475,    11, 11641,    13,  1139, 13154,     1]),\n",
       "  'input_text': ' models perform\\nbetter with chunks of specific length\\nexpected length and complexity of user queries',\n",
       "  'expected_output': '\\ndetermine whether the content will be short and s'},\n",
       " {'input_ids': tensor([    3,   122,   189,  1644,  2475,    11, 11641,    13,  1139, 13154,\n",
       "           2082,   823,     8,   738,    56,    36,   710,    11,     3,     7,\n",
       "              1]),\n",
       "  'input_text': 'gth\\nexpected length and complexity of user queries\\ndetermine whether the content will be short and s',\n",
       "  'expected_output': 'pecific or long and complex\\nthis factor will influ'},\n",
       " {'input_ids': tensor([2082,  823,    8,  738,   56,   36,  710,   11,  806,   42,  307,   11,\n",
       "          1561,   48, 2945,   56,   16, 6947,    1]),\n",
       "  'input_text': '\\ndetermine whether the content will be short and specific or long and complex\\nthis factor will influ',\n",
       "  'expected_output': 'ence the approach to chunking the content ensuring'},\n",
       " {'input_ids': tensor([  158,    75,  3286,    42,   307,    11,  1561,    48,  2945,    56,\n",
       "           2860,     8,  1295,    12, 16749,    53,     8,   738,     3,  5833,\n",
       "              1]),\n",
       "  'input_text': 'pecific or long and complex\\nthis factor will influence the approach to chunking the content ensuring',\n",
       "  'expected_output': ' a closer\\ncorrelation between the embedded query a'},\n",
       " {'input_ids': tensor([    3,  1433,     8,  1295,    12, 16749,    53,     8,   738,     3,\n",
       "           5833,     3,     9,  4645, 18712,   344,     8, 13612, 11417,     3,\n",
       "              9,     1]),\n",
       "  'input_text': 'ence the approach to chunking the content ensuring a closer\\ncorrelation between the embedded query a',\n",
       "  'expected_output': 'nd the embedded chunks\\napplication specific requir'},\n",
       " {'input_ids': tensor([    3,     9,  4645, 18712,   344,     8, 13612, 11417,    11,     8,\n",
       "          13612, 16749,     7,   917,   806,     3,    60,  1169,    52,     1]),\n",
       "  'input_text': ' a closer\\ncorrelation between the embedded query and the embedded chunks\\napplication specific requir',\n",
       "  'expected_output': 'ements\\nthe application use case such as semantic s'},\n",
       " {'input_ids': tensor([    3,   727,     8, 13612, 16749,     7,   917,   806,  1502,     8,\n",
       "            917,   169,   495,   224,    38, 27632,     3,     7,     1]),\n",
       "  'input_text': 'nd the embedded chunks\\napplication specific requirements\\nthe application use case such as semantic s',\n",
       "  'expected_output': 'earch question answering\\nsummarization or other pu'},\n",
       " {'input_ids': tensor([    3, 10420,     8,   917,   169,   495,   224,    38, 27632,   960,\n",
       "            822, 18243,  4505,  1635,  1707,    42,   119,  4353,     1]),\n",
       "  'input_text': 'ements\\nthe application use case such as semantic search question answering\\nsummarization or other pu',\n",
       "  'expected_output': 'rposes will also determine how text should be\\nchun'},\n",
       " {'input_ids': tensor([    3,    15,  7064,   822, 18243,  4505,  1635,  1707,    42,   119,\n",
       "           3659,    56,    92,  2082,   149,  1499,   225,    36,     3,   524,\n",
       "            202,     1]),\n",
       "  'input_text': 'earch question answering\\nsummarization or other purposes will also determine how text should be\\nchun',\n",
       "  'expected_output': 'ked if the results need to be input into another l'},\n",
       " {'input_ids': tensor([    3,    52,  2748,    15,     7,    56,    92,  2082,   149,  1499,\n",
       "            225,    36, 16749,    15,    26,     3,    99,     8,   772,   174,\n",
       "             12,    36,  3785,   139,   430,     3,    40,     1]),\n",
       "  'input_text': 'rposes will also determine how text should be\\nchunked if the results need to be input into another l',\n",
       "  'expected_output': 'anguage model with a token\\nlimit it is crucial to '},\n",
       " {'input_ids': tensor([    3,  5100,     3,    99,     8,   772,   174,    12,    36,  3785,\n",
       "            139,   430,  1612,   825,    28,     3,     9, 14145,  2006,    34,\n",
       "             19,  4462,    12,     1]),\n",
       "  'input_text': 'ked if the results need to be input into another language model with a token\\nlimit it is crucial to ',\n",
       "  'expected_output': 'factor this into your decisionmaking process\\nkeep '},\n",
       " {'input_ids': tensor([   46,  1744,   545,   825,    28,     3,     9, 14145,  2006,    34,\n",
       "             19,  4462,    12,  2945,    48,   139,    39,  1357,  5239,   433,\n",
       "            453,     1]),\n",
       "  'input_text': 'anguage model with a token\\nlimit it is crucial to factor this into your decisionmaking process\\nkeep ',\n",
       "  'expected_output': 'calm  build ai14abhinav kimothiindexing pipeline d'},\n",
       " {'input_ids': tensor([ 2945,    48,   139,    39,  1357,  5239,   433,   453,  4447,   918,\n",
       "              3,     9,    23,  2534,     9,   115,  2907,     9,   208,     3,\n",
       "          19754,    32,  7436, 18288,    53, 12045,     3,    26,     1]),\n",
       "  'input_text': 'factor this into your decisionmaking process\\nkeep calm  build ai14abhinav kimothiindexing pipeline d',\n",
       "  'expected_output': 'ocument splitting\\nchunking methods \\ndepending on t'},\n",
       " {'input_ids': tensor([ 4447,   918,     3,     9,    23,  2534,     9,   115,  2907,     9,\n",
       "            208,     3, 19754,    32,  7436, 18288,    53, 12045,  1708, 28503,\n",
       "          16749,    53,  2254,  3345,    30,     3,    17,     1]),\n",
       "  'input_text': 'calm  build ai14abhinav kimothiindexing pipeline document splitting\\nchunking methods \\ndepending on t',\n",
       "  'expected_output': 'he aforementioned considerations a number of text '},\n",
       " {'input_ids': tensor([    3,    32,  1071,   297, 28503, 16749,    53,  2254,  3345,    30,\n",
       "              8,     3,     9, 22835,  4587,     7,     3,     9,   381,    13,\n",
       "           1499,     1]),\n",
       "  'input_text': 'ocument splitting\\nchunking methods \\ndepending on the aforementioned considerations a number of text ',\n",
       "  'expected_output': 'splitters are\\navailable at a broad level text spli'},\n",
       " {'input_ids': tensor([    3,    88,     3,     9, 22835,  4587,     7,     3,     9,   381,\n",
       "             13,  1499,  5679,  4849,    33,   347,    44,     3,     9,  4358,\n",
       "            593,  1499,     3,     7,  5900,     1]),\n",
       "  'input_text': 'he aforementioned considerations a number of text splitters are\\navailable at a broad level text spli',\n",
       "  'expected_output': 'tters operate in the following manner\\ndivide the t'},\n",
       " {'input_ids': tensor([ 5679,  4849,    33,   347,    44,     3,     9,  4358,   593,  1499,\n",
       "           5679,  4849,  4368,    16,     8,   826,  3107, 14514,     8,     3,\n",
       "             17,     1]),\n",
       "  'input_text': 'splitters are\\navailable at a broad level text splitters operate in the following manner\\ndivide the t',\n",
       "  'expected_output': 'ext into compact semantically meaningful units oft'},\n",
       " {'input_ids': tensor([    3,    17,  4849,  4368,    16,     8,   826,  3107, 14514,     8,\n",
       "           1499,   139,  6607, 27632,  1427,  7892,  3173,    13,    17,     1]),\n",
       "  'input_text': 'tters operate in the following manner\\ndivide the text into compact semantically meaningful units oft',\n",
       "  'expected_output': 'en sentences \\nmerge these smaller units into large'},\n",
       " {'input_ids': tensor([    3, 10398,   139,  6607, 27632,  1427,  7892,  3173,   557, 16513,\n",
       "           7986,   175,  2755,  3173,   139,   508,     1]),\n",
       "  'input_text': 'ext into compact semantically meaningful units often sentences \\nmerge these smaller units into large',\n",
       "  'expected_output': 'r chunks until a specific size is achieved\\nmeasure'},\n",
       " {'input_ids': tensor([    3,    35, 16513,  7986,   175,  2755,  3173,   139,  2186, 16749,\n",
       "              7,   552,     3,     9,   806,   812,    19,  5153,  3613,     1]),\n",
       "  'input_text': 'en sentences \\nmerge these smaller units into larger chunks until a specific size is achieved\\nmeasure',\n",
       "  'expected_output': 'd by a length function \\nupon reaching the predeter'},\n",
       " {'input_ids': tensor([    3,    52, 16749,     7,   552,     3,     9,   806,   812,    19,\n",
       "           5153,  8413,    57,     3,     9,  2475,  1681,  1286,  7232,     8,\n",
       "            554,   221,   449,     1]),\n",
       "  'input_text': 'r chunks until a specific size is achieved\\nmeasured by a length function \\nupon reaching the predeter',\n",
       "  'expected_output': 'mined size treat that chunk as an independent\\nsegm'},\n",
       " {'input_ids': tensor([    3,    26,    57,     3,     9,  2475,  1681,  1286,  7232,     8,\n",
       "            554, 22755,   812,  2665,    24, 16749,    38,    46,  2547,   142,\n",
       "            122,    51,     1]),\n",
       "  'input_text': 'd by a length function \\nupon reaching the predetermined size treat that chunk as an independent\\nsegm',\n",
       "  'expected_output': 'ent of text  thereafter start creating a new text '},\n",
       " {'input_ids': tensor([ 2000,    26,   812,  2665,    24, 16749,    38,    46,  2547,  5508,\n",
       "             13,  1499, 24546,   456,  1577,     3,     9,   126,  1499,     1]),\n",
       "  'input_text': 'mined size treat that chunk as an independent\\nsegment of text  thereafter start creating a new text ',\n",
       "  'expected_output': 'chunk with some degree\\nof overlap to maintain cont'},\n",
       " {'input_ids': tensor([    3,   295,    13,  1499, 24546,   456,  1577,     3,     9,   126,\n",
       "           1499, 16749,    28,   128,  1952,    13, 21655,    12,  1961,  3622,\n",
       "              1]),\n",
       "  'input_text': 'ent of text  thereafter start creating a new text chunk with some degree\\nof overlap to maintain cont',\n",
       "  'expected_output': 'extual continuity between chunks\\ntwo areas to focu'},\n",
       " {'input_ids': tensor([16749,    28,   128,  1952,    13, 21655,    12,  1961, 28131, 23534,\n",
       "            344, 16749,     7,   192,   844,    12, 13479,    76,     1]),\n",
       "  'input_text': 'chunk with some degree\\nof overlap to maintain contextual continuity between chunks\\ntwo areas to focu',\n",
       "  'expected_output': 's on therefore are \\nhow the text is split how the '},\n",
       " {'input_ids': tensor([    3, 10398,  3471, 23534,   344, 16749,     7,   192,   844,    12,\n",
       "            992,    30,  2459,    33,   149,     8,  1499,    19,  5679,   149,\n",
       "              8,     1]),\n",
       "  'input_text': 'extual continuity between chunks\\ntwo areas to focus on therefore are \\nhow the text is split how the ',\n",
       "  'expected_output': 'chunk size is measured\\na very common approach is w'},\n",
       " {'input_ids': tensor([    3,     7,    30,  2459,    33,   149,     8,  1499,    19,  5679,\n",
       "            149,     8, 16749,   812,    19,  8413,     3,     9,   182,  1017,\n",
       "           1295,    19,     3,   210,     1]),\n",
       "  'input_text': 's on therefore are \\nhow the text is split how the chunk size is measured\\na very common approach is w',\n",
       "  'expected_output': 'here we predetermine the size of the text chunks \\n'},\n",
       " {'input_ids': tensor([16749,   812,    19,  8413,     3,     9,   182,  1017,  1295,    19,\n",
       "            213,    62,   554, 16372,    15,     8,   812,    13,     8,  1499,\n",
       "          16749,     7,     1]),\n",
       "  'input_text': 'chunk size is measured\\na very common approach is where we predetermine the size of the text chunks \\n',\n",
       "  'expected_output': 'additionally we can specify the overlap between ch'},\n",
       " {'input_ids': tensor([  270,    62,   554, 16372,    15,     8,   812,    13,     8,  1499,\n",
       "          16749,     7, 11700,    62,    54, 11610,     8, 21655,   344,     3,\n",
       "            524,     1]),\n",
       "  'input_text': 'here we predetermine the size of the text chunks \\nadditionally we can specify the overlap between ch',\n",
       "  'expected_output': 'unks remember overlap is\\npreferred to maintain con'},\n",
       " {'input_ids': tensor([11700,    62,    54, 11610,     8, 21655,   344, 16749,     7,  1423,\n",
       "          21655,    19,  6241,    12,  1961,   975,     1]),\n",
       "  'input_text': 'additionally we can specify the overlap between chunks remember overlap is\\npreferred to maintain con',\n",
       "  'expected_output': 'textual continuity between chunks \\nthis approach i'},\n",
       " {'input_ids': tensor([    3,  6513,     7,  1423, 21655,    19,  6241,    12,  1961, 28131,\n",
       "          23534,   344, 16749,     7,    48,  1295,     3,    23,     1]),\n",
       "  'input_text': 'unks remember overlap is\\npreferred to maintain contextual continuity between chunks \\nthis approach i',\n",
       "  'expected_output': 's simple and cheap and is therefore widely used le'},\n",
       " {'input_ids': tensor([ 1499,  3471, 23534,   344, 16749,     7,    48,  1295,    19,   650,\n",
       "             11,  2877,    11,    19,  2459,  5456,   261,    90,     1]),\n",
       "  'input_text': 'textual continuity between chunks \\nthis approach is simple and cheap and is therefore widely used le',\n",
       "  'expected_output': 'ts look at\\nsome examples  \\nkeep calm  build ai15ab'},\n",
       " {'input_ids': tensor([   3,    7,  650,   11, 2877,   11,   19, 2459, 5456,  261, 8857,  320,\n",
       "            44,  128, 4062,  453, 4447,  918,    3,    9,   23, 1808,    9,  115,\n",
       "             1]),\n",
       "  'input_text': 's simple and cheap and is therefore widely used lets look at\\nsome examples  \\nkeep calm  build ai15ab',\n",
       "  'expected_output': 'hinav kimothiindexing pipeline document splitting\\n'},\n",
       " {'input_ids': tensor([    3,    17,     7,   320,    44,   128,  4062,   453,  4447,   918,\n",
       "              3,     9,    23,  1808,     9,   115,  2907,     9,   208,     3,\n",
       "          19754,    32,  7436, 18288,    53, 12045,  1708, 28503,     1]),\n",
       "  'input_text': 'ts look at\\nsome examples  \\nkeep calm  build ai15abhinav kimothiindexing pipeline document splitting\\n',\n",
       "  'expected_output': 'split by character\\nin this approach the text is sp'},\n",
       " {'input_ids': tensor([ 3811,     9,   208,     3, 19754,    32,  7436, 18288,    53, 12045,\n",
       "           1708, 28503,  5679,    57,  1848,    16,    48,  1295,     8,  1499,\n",
       "             19,     3,     7,   102,     1]),\n",
       "  'input_text': 'hinav kimothiindexing pipeline document splitting\\nsplit by character\\nin this approach the text is sp',\n",
       "  'expected_output': 'lit based on a character and the chunk size is\\nmea'},\n",
       " {'input_ids': tensor([ 5679,    57,  1848,    16,    48,  1295,     8,  1499,    19,  5679,\n",
       "              3,   390,    30,     3,     9,  1848,    11,     8, 16749,   812,\n",
       "             19,   140,     9,     1]),\n",
       "  'input_text': 'split by character\\nin this approach the text is split based on a character and the chunk size is\\nmea',\n",
       "  'expected_output': 'sured by the number of characters\\nexample text  al'},\n",
       " {'input_ids': tensor([ 4996,     3,   390,    30,     3,     9,  1848,    11,     8, 16749,\n",
       "            812,    19,  8413,    57,     8,   381,    13,  2850,   677,  1499,\n",
       "            491,     1]),\n",
       "  'input_text': 'lit based on a character and the chunk size is\\nmeasured by the number of characters\\nexample text  al',\n",
       "  'expected_output': 'iceinwonderlandtxt the book in txt format\\nusing la'},\n",
       " {'input_ids': tensor([ 417,   26,   57,    8,  381,   13, 2850,  677, 1499,  491,  867,   77,\n",
       "           210,  106,  221, 7721,   17,  226,   17,    8,  484,   16,    3,   17,\n",
       "           226,   17, 1910,  338,   50,    1]),\n",
       "  'input_text': 'sured by the number of characters\\nexample text  aliceinwonderlandtxt the book in txt format\\nusing la',\n",
       "  'expected_output': 'ngchains charactertextsplitter\\ntexts0 \\ntitle alice'},\n",
       " {'input_ids': tensor([    3,   867,    77,   210,   106,   221,  7721,    17,   226,    17,\n",
       "              8,   484,    16,     3,    17,   226,    17,  1910,   338, 12142,\n",
       "          19836,     7,  1848,  6327,     7,  5900,    17,   449, 14877,   632,\n",
       "           2233,   491,   867,     1]),\n",
       "  'input_text': 'iceinwonderlandtxt the book in txt format\\nusing langchains charactertextsplitter\\ntexts0 \\ntitle alice',\n",
       "  'expected_output': 's adventures in wonderlandnauthor lewis carrollnnn'},\n",
       " {'input_ids': tensor([    3,  1725, 19836,     7,  1848,  6327,     7,  5900,    17,   449,\n",
       "          14877,   632,  2233,   491,   867,     7, 12560,    16,  3337,    40,\n",
       "            232,    29, 17415,    90,   210,   159,   443,  4046,    29,    29,\n",
       "             29,     1]),\n",
       "  'input_text': 'ngchains charactertextsplitter\\ntexts0 \\ntitle alices adventures in wonderlandnauthor lewis carrollnnn',\n",
       "  'expected_output': ' chapter i n down the\\nrabbithole nn alice was begi'},\n",
       " {'input_ids': tensor([    3,     7, 12560,    16,  3337,    40,   232,    29, 17415,    90,\n",
       "            210,   159,   443,  4046,    29,    29,    29,  5800,     3,    23,\n",
       "              3,    29,   323,     8, 18383,  9136,     3,    29,    29,   491,\n",
       "            867,    47,    36,   122,    23,     1]),\n",
       "  'input_text': 's adventures in wonderlandnauthor lewis carrollnnn chapter i n down the\\nrabbithole nn alice was begi',\n",
       "  'expected_output': 'nning to get very tired of sitting by her sisterno'},\n",
       " {'input_ids': tensor([ 5800,     3,    23,     3,    29,   323,     8, 18383,  9136,     3,\n",
       "             29,    29,   491,   867,    47,  1849,    12,   129,   182,  7718,\n",
       "             13,  3823,    57,   160,  4806,    29,    32,     1]),\n",
       "  'input_text': ' chapter i n down the\\nrabbithole nn alice was beginning to get very tired of sitting by her sisterno',\n",
       "  'expected_output': 'n the bank and of\\nhaving nothing to do once or twi'},\n",
       " {'input_ids': tensor([   3, 9416,   12,  129,  182, 7718,   13, 3823,   57,  160, 4806,   29,\n",
       "           106,    8, 2137,   11,   13,  578, 1327,   12,  103,  728,   42,    3,\n",
       "            17,  210,   23,    1]),\n",
       "  'input_text': 'nning to get very tired of sitting by her sisternon the bank and of\\nhaving nothing to do once or twi',\n",
       "  'expected_output': 'ce she hadnpeeped into the book her sister was rea'},\n",
       " {'input_ids': tensor([    3,    29,     8,  2137,    11,    13,   578,  1327,    12,   103,\n",
       "            728,    42,  4394,   255, 12381,   855,    15,  3138,   139,     8,\n",
       "            484,   160,  4806,    47,     3,   864,     1]),\n",
       "  'input_text': 'n the bank and of\\nhaving nothing to do once or twice she hadnpeeped into the book her sister was rea',\n",
       "  'expected_output': 'ding but it\\nhad nonpictures or conversations in it'},\n",
       " {'input_ids': tensor([  197,   255, 12381,   855,    15,  3138,   139,     8,   484,   160,\n",
       "           4806,    47,  1183,    68,    34,   141,   529,  6174, 10471,    42,\n",
       "           9029,    16,    34,     1]),\n",
       "  'input_text': 'ce she hadnpeeped into the book her sister was reading but it\\nhad nonpictures or conversations in it',\n",
       "  'expected_output': ' and what is the use of a booknthought alice witho'},\n",
       " {'input_ids': tensor([    3,    26,    53,    68,    34,   141,   529,  6174, 10471,    42,\n",
       "           9029,    16,    34,    11,   125,    19,     8,   169,    13,     3,\n",
       "              9,   484,    29, 11841,    17,   491,   867,    28,    32,     1]),\n",
       "  'input_text': 'ding but it\\nhad nonpictures or conversations in it and what is the use of a booknthought alice witho',\n",
       "  'expected_output': 'ut\\npictures or conversationnn so she was consideri'},\n",
       " {'input_ids': tensor([   11,   125,    19,     8,   169,    13,     3,     9,   484,    29,\n",
       "          11841,    17,   491,   867,   406,  1933,    42,  3634,    29,    29,\n",
       "             78,   255,    47,  1099,    23,     1]),\n",
       "  'input_text': ' and what is the use of a booknthought alice without\\npictures or conversationnn so she was consideri',\n",
       "  'expected_output': 'ng in her own mind as well as she couldnfor\\nthe ho'},\n",
       " {'input_ids': tensor([   3,   76,   17, 1933,   42, 3634,   29,   29,   78,  255,   47, 4014,\n",
       "            16,  160,  293,  809,   38,  168,   38,  255, 2654, 1161,    8, 3534,\n",
       "             1]),\n",
       "  'input_text': 'ut\\npictures or conversationnn so she was considering in her own mind as well as she couldnfor\\nthe ho',\n",
       "  'expected_output': 't day made her feel very sleepy and stupid whether'},\n",
       " {'input_ids': tensor([    3,  1725,    16,   160,   293,   809,    38,   168,    38,   255,\n",
       "           2654,  1161,     8,  1312,   239,   263,   160,   473,   182,  2085,\n",
       "             63,    11, 13721,   823,     1]),\n",
       "  'input_text': 'ng in her own mind as well as she couldnfor\\nthe hot day made her feel very sleepy and stupid whether',\n",
       "  'expected_output': 'nthe pleasure of making a daisychain\\nwould be wort'},\n",
       " {'input_ids': tensor([    3,    17,   239,   263,   160,   473,   182,  2085,    63,    11,\n",
       "          13721,   823,    29,   532,  5565,    13,   492,     3,     9,   836,\n",
       "            159,    63, 19836,   133,    36,     3,  8129,     1]),\n",
       "  'input_text': 't day made her feel very sleepy and stupid whethernthe pleasure of making a daisychain\\nwould be wort',\n",
       "  'expected_output': 'h the troublenof getting up and picking the daisie'},\n",
       " {'input_ids': tensor([    3,    29,   532,  5565,    13,   492,     3,     9,   836,   159,\n",
       "             63, 19836,   133,    36,  1494,     8,  3169,    29,   858,   652,\n",
       "             95,    11,  8915,     8,   836,    23,  2452,     1]),\n",
       "  'input_text': 'nthe pleasure of making a daisychain\\nwould be worth the troublenof getting up and picking the daisie',\n",
       "  'expected_output': 's when suddenly a whitenrabbit\\nwith pink eyes ran '},\n",
       " {'input_ids': tensor([   3,  107,    8, 3169,   29,  858,  652,   95,   11, 8915,    8,  836,\n",
       "           159,  725,  116, 8247,    3,    9,  872,   29, 7093, 2360,   28, 5571,\n",
       "          2053, 4037,    1]),\n",
       "  'input_text': 'h the troublenof getting up and picking the daisies when suddenly a whitenrabbit\\nwith pink eyes ran ',\n",
       "  'expected_output': 'close by hernn there was nothing so very remarkabl'},\n",
       " {'input_ids': tensor([    3,     7,   116,  8247,     3,     9,   872,    29,  7093,  2360,\n",
       "             28,  5571,  2053,  4037,   885,    57,   160,    29,    29,   132,\n",
       "             47,  1327,    78,   182,     3,    60,  3920, 15403,     1]),\n",
       "  'input_text': 's when suddenly a whitenrabbit\\nwith pink eyes ran close by hernn there was nothing so very remarkabl',\n",
       "  'expected_output': 'e in that nor did\\nalicenthink it so very much out '},\n",
       " {'input_ids': tensor([  885,    57,   160,    29,    29,   132,    47,  1327,    78,   182,\n",
       "           8562,    16,    24,  3701,   410,     3,     9,  2176,    35, 10337,\n",
       "             34,    78,   182,   231,    91,     1]),\n",
       "  'input_text': 'close by hernn there was nothing so very remarkable in that nor did\\nalicenthink it so very much out ',\n",
       "  'expected_output': 'of the way to hear the rabbit say tonitself oh dea'},\n",
       " {'input_ids': tensor([    3,    15,    16,    24,  3701,   410,     3,     9,  2176,    35,\n",
       "          10337,    34,    78,   182,   231,    91,    13,     8,   194,    12,\n",
       "           1616,     8, 18383,   497,    12,    29,   155,  7703,     3,    32,\n",
       "            107,    20,     9,     1]),\n",
       "  'input_text': 'e in that nor did\\nalicenthink it so very much out of the way to hear the rabbit say tonitself oh dea',\n",
       "  'expected_output': 'r oh dear i\\nshall be late when she thoughtnit over'},\n",
       " {'input_ids': tensor([   13,     8,   194,    12,  1616,     8, 18383,   497,    12,    29,\n",
       "            155,  7703,     3,    32,   107, 12537,     3,    32,   107, 12537,\n",
       "              3,    23,  1522,    36,  1480,   116,   255,   816,    29,   155,\n",
       "            147,     1]),\n",
       "  'input_text': 'of the way to hear the rabbit say tonitself oh dear oh dear i\\nshall be late when she thoughtnit over',\n",
       "  'expected_output': ' afterwards it occurred to her that she ought to\\nh'},\n",
       " {'input_ids': tensor([    3,    52,     3,    32,   107, 12537,     3,    23,  1522,    36,\n",
       "           1480,   116,   255,   816,    29,   155,   147, 15627,    34,  6935,\n",
       "             12,   160,    24,   255,  8204,    12,     3,   107,     1]),\n",
       "  'input_text': 'r oh dear i\\nshall be late when she thoughtnit over afterwards it occurred to her that she ought to\\nh',\n",
       "  'expected_output': 'avenwondered at this but at the time it all seemed'},\n",
       " {'input_ids': tensor([15627,    34,  6935,    12,   160,    24,   255,  8204,    12,    43,\n",
       "             29,   210,   106,    26,  3737,    44,    48,    68,    44,     8,\n",
       "             97,    34,    66,  3776,     1]),\n",
       "  'input_text': ' afterwards it occurred to her that she ought to\\nhavenwondered at this but at the time it all seemed',\n",
       "  'expected_output': ' quite naturalnbut when the rabbit actually\\ntook a'},\n",
       " {'input_ids': tensor([    3,     9,  1926,   210,   106,    26,  3737,    44,    48,    68,\n",
       "             44,     8,    97,    34,    66,  3776,   882,   793,    29,  2780,\n",
       "            116,     8, 18383,   700,   808,     3,     9,     1]),\n",
       "  'input_text': 'avenwondered at this but at the time it all seemed quite naturalnbut when the rabbit actually\\ntook a',\n",
       "  'expected_output': ' watch out of its waistcoatnpocket and looked at i'},\n",
       " {'input_ids': tensor([  882,   793,    29,  2780,   116,     8, 18383,   700,   808,     3,\n",
       "              9,  1605,    91,    13,   165, 13586, 18954,    29, 10496,  8044,\n",
       "             11,  2299,    44,     3,    23,     1]),\n",
       "  'input_text': ' quite naturalnbut when the rabbit actually\\ntook a watch out of its waistcoatnpocket and looked at i',\n",
       "  'expected_output': 't and then hurried on alice\\nstarted tonher feet fo'},\n",
       " {'input_ids': tensor([ 1605,    91,    13,   165, 13586, 18954,    29, 10496,  8044,    11,\n",
       "           2299,    44,    34,    11,   258,     3, 10666,  9889,    30,   491,\n",
       "            867,   708,    12,    29,   760,  1922,  5575,     1]),\n",
       "  'input_text': ' watch out of its waistcoatnpocket and looked at it and then hurried on alice\\nstarted tonher feet fo',\n",
       "  'expected_output': 'r it flashed across her mind that she had nevernbe'},\n",
       " {'input_ids': tensor([    3,    17,    11,   258,     3, 10666,  9889,    30,   491,   867,\n",
       "            708,    12,    29,   760,  1922,    21,    34,  4923,    15,    26,\n",
       "            640,   160,   809,    24,   255,   141,   470,    29,   346,     1]),\n",
       "  'input_text': 't and then hurried on alice\\nstarted tonher feet for it flashed across her mind that she had nevernbe',\n",
       "  'expected_output': 'fore seen a rabbit with\\neither a waistcoatpocket o'},\n",
       " {'input_ids': tensor([    3,    52,    34,  4923,    15,    26,   640,   160,   809,    24,\n",
       "            255,   141,   470,    29, 26116,   894,     3,     9, 18383,    28,\n",
       "            893,     3,     9, 13586, 18954, 10496,  8044,     3,    32,     1]),\n",
       "  'input_text': 'r it flashed across her mind that she had nevernbefore seen a rabbit with\\neither a waistcoatpocket o',\n",
       "  'expected_output': 'r a watch tontake out of it and burning with curio'},\n",
       " {'input_ids': tensor([   21,    15,   894,     3,     9, 18383,    28,   893,     3,     9,\n",
       "          13586, 18954, 10496,  8044,    42,     3,     9,  1605,    12,    29,\n",
       "           4914,    91,    13,    34,    11,  9706,    28,     3,  9659,    32,\n",
       "              1]),\n",
       "  'input_text': 'fore seen a rabbit with\\neither a waistcoatpocket or a watch tontake out of it and burning with curio',\n",
       "  'expected_output': 'sity she ran across\\nthenfield after it and fortuna'},\n",
       " {'input_ids': tensor([    3,    52,     3,     9,  1605,    12,    29,  4914,    91,    13,\n",
       "             34,    11,  9706,    28, 18967,   255,  4037,   640,   258,  1846,\n",
       "            227,    34,    11,    21,    17,   202,     9,     1]),\n",
       "  'input_text': 'r a watch tontake out of it and burning with curiosity she ran across\\nthenfield after it and fortuna',\n",
       "  'expected_output': 'tely was just in time to see it popndown a large r'},\n",
       " {'input_ids': tensor([    3,     7,   485,   255,  4037,   640,   258,  1846,   227,    34,\n",
       "             11,     3, 29178,    47,   131,    16,    97,    12,   217,    34,\n",
       "           2783,    29,  3035,     3,     9,   508,     3,    52,     1]),\n",
       "  'input_text': 'sity she ran across\\nthenfield after it and fortunately was just in time to see it popndown a large r',\n",
       "  'expected_output': 'abbithole under the\\nhedgenn in another moment down'},\n",
       " {'input_ids': tensor([    3,  1625,    63,    47,   131,    16,    97,    12,   217,    34,\n",
       "           2783,    29,  3035,     3,     9,   508, 18383,  9136,   365,     8,\n",
       "          18179,    29,    29,    16,   430,   798,   323,     1]),\n",
       "  'input_text': 'tely was just in time to see it popndown a large rabbithole under the\\nhedgenn in another moment down',\n",
       "  'expected_output': ' went alice after it never oncenconsidering how in'},\n",
       " {'input_ids': tensor([  703,  2360,  9136,   365,     8, 18179,    29,    29,    16,   430,\n",
       "            798,   323,   877,   491,   867,   227,    34,   470,   728,    29,\n",
       "          31750,    53,   149,    16,     1]),\n",
       "  'input_text': 'abbithole under the\\nhedgenn in another moment down went alice after it never oncenconsidering how in',\n",
       "  'expected_output': ' the world\\nshe was to get out againnn the rabbitho'},\n",
       " {'input_ids': tensor([  877,   491,   867,   227,    34,   470,   728,    29, 31750,    53,\n",
       "            149,    16,     8,   296,   255,    47,    12,   129,    91,   541,\n",
       "             29,    29,     8, 18383,   107,    32,     1]),\n",
       "  'input_text': ' went alice after it never oncenconsidering how in the world\\nshe was to get out againnn the rabbitho',\n",
       "  'expected_output': 'le went straight on like a tunnel for some waynand'},\n",
       " {'input_ids': tensor([    8,   296,   255,    47,    12,   129,    91,   541,    29,    29,\n",
       "              8, 18383,  9136,   877,  2541,    30,   114,     3,     9, 11916,\n",
       "             21,   128,   194,    29,   232,     1]),\n",
       "  'input_text': ' the world\\nshe was to get out againnn the rabbithole went straight on like a tunnel for some waynand',\n",
       "  'expected_output': '\\nthen dipped suddenly down so suddenly that alice '},\n",
       " {'input_ids': tensor([   90,   877,  2541,    30,   114,     3,     9, 11916,    21,   128,\n",
       "            194,    29,   232,   258,     3,    26, 15437,  8247,   323,    78,\n",
       "           8247,    24,   491,   867,     1]),\n",
       "  'input_text': 'le went straight on like a tunnel for some waynand\\nthen dipped suddenly down so suddenly that alice ',\n",
       "  'expected_output': 'had not anmoment to think about stopping\\nherself b'},\n",
       " {'input_ids': tensor([  258,     3,    26, 15437,  8247,   323,    78,  8247,    24,   491,\n",
       "            867,   141,    59,    46,    51,    32,   297,    12,   317,    81,\n",
       "          10847,  6257,     3,   115,     1]),\n",
       "  'input_text': '\\nthen dipped suddenly down so suddenly that alice had not anmoment to think about stopping\\nherself b',\n",
       "  'expected_output': 'efore she found herselfnfalling down a very deep w'},\n",
       " {'input_ids': tensor([  141,    59,    46,    51,    32,   297,    12,   317,    81, 10847,\n",
       "           6257,   274,   255,   435,  6257,    29,  2857,    53,   323,     3,\n",
       "              9,   182,  1659,     3,   210,     1]),\n",
       "  'input_text': 'had not anmoment to think about stopping\\nherself before she found herselfnfalling down a very deep w',\n",
       "  'expected_output': 'ell\\ntexts1\\nin another moment down went alice after'},\n",
       " {'input_ids': tensor([    3,    15,  1161,    15,   255,   435,  6257,    29,  2857,    53,\n",
       "            323,     3,     9,   182,  1659,   168, 14877,   536,    16,   430,\n",
       "            798,   323,   877,   491,   867,   227,     1]),\n",
       "  'input_text': 'efore she found herselfnfalling down a very deep well\\ntexts1\\nin another moment down went alice after',\n",
       "  'expected_output': ' it never oncenconsidering how in the world she wa'},\n",
       " {'input_ids': tensor([    3,  3820, 14877,   536,    16,   430,   798,   323,   877,   491,\n",
       "            867,   227,    34,   470,   728,    29, 31750,    53,   149,    16,\n",
       "              8,   296,   255,  8036,     1]),\n",
       "  'input_text': 'ell\\ntexts1\\nin another moment down went alice after it never oncenconsidering how in the world she wa',\n",
       "  'expected_output': 's to\\nget out againnn the rabbithole went straight '},\n",
       " {'input_ids': tensor([   34,   470,   728,    29, 31750,    53,   149,    16,     8,   296,\n",
       "            255,    47,    12,   129,    91,   541,    29,    29,     8, 18383,\n",
       "           9136,   877,  2541,     1]),\n",
       "  'input_text': ' it never oncenconsidering how in the world she was to\\nget out againnn the rabbithole went straight ',\n",
       "  'expected_output': 'on like a tunnel for some waynand then dipped\\nsudd'},\n",
       " {'input_ids': tensor([    3,     7,    12,   129,    91,   541,    29,    29,     8, 18383,\n",
       "           9136,   877,  2541,    30,   114,     3,     9, 11916,    21,   128,\n",
       "            194,    29,   232,   258,     3,    26, 15437,  8411,    26,     1]),\n",
       "  'input_text': 's to\\nget out againnn the rabbithole went straight on like a tunnel for some waynand then dipped\\nsudd',\n",
       "  'expected_output': 'enly down so suddenly that alice had not anmoment '},\n",
       " {'input_ids': tensor([   30,   114,     3,     9, 11916,    21,   128,   194,    29,   232,\n",
       "            258,     3,    26, 15437,  8247,   323,    78,  8247,    24,   491,\n",
       "            867,   141,    59,    46,    51,    32,   297,     1]),\n",
       "  'input_text': 'on like a tunnel for some waynand then dipped\\nsuddenly down so suddenly that alice had not anmoment ',\n",
       "  'expected_output': 'to think about stopping herself before\\nshe found h'},\n",
       " {'input_ids': tensor([    3,    35,   120,   323,    78,  8247,    24,   491,   867,   141,\n",
       "             59,    46,    51,    32,   297,    12,   317,    81, 10847,  6257,\n",
       "            274,   255,   435,     3,   107,     1]),\n",
       "  'input_text': 'enly down so suddenly that alice had not anmoment to think about stopping herself before\\nshe found h',\n",
       "  'expected_output': 'erselfnfalling down a very deep wellnn either the '},\n",
       " {'input_ids': tensor([   12,   317,    81, 10847,  6257,   274,   255,   435,  6257,    29,\n",
       "           2857,    53,   323,     3,     9,   182,  1659,   168,    29,    29,\n",
       "            893,     8,     1]),\n",
       "  'input_text': 'to think about stopping herself before\\nshe found herselfnfalling down a very deep wellnn either the ',\n",
       "  'expected_output': 'well was very deep or she fell very\\nslowly for she'},\n",
       " {'input_ids': tensor([   3,   49, 7703,   29, 2857,   53,  323,    3,    9,  182, 1659,  168,\n",
       "            29,   29,  893,    8,  168,   47,  182, 1659,   42,  255, 4728,  182,\n",
       "          5665,   21,  255,    1]),\n",
       "  'input_text': 'erselfnfalling down a very deep wellnn either the well was very deep or she fell very\\nslowly for she',\n",
       "  'expected_output': 'nhad plenty of time as she went down to look about'},\n",
       " {'input_ids': tensor([ 168,   47,  182, 1659,   42,  255, 4728,  182, 5665,   21,  255,   29,\n",
       "          8399, 2500,   13,   97,   38,  255,  877,  323,   12,  320,   81,    1]),\n",
       "  'input_text': 'well was very deep or she fell very\\nslowly for shenhad plenty of time as she went down to look about',\n",
       "  'expected_output': ' her and tonwonder what was\\ngoing to happen next f'},\n",
       " {'input_ids': tensor([   3,   29, 8399, 2500,   13,   97,   38,  255,  877,  323,   12,  320,\n",
       "            81,  160,   11,   12,   29,  210,  106,  588,  125,   47,  352,   12,\n",
       "          1837,  416,    3,   89,    1]),\n",
       "  'input_text': 'nhad plenty of time as she went down to look about her and tonwonder what was\\ngoing to happen next f',\n",
       "  'expected_output': 'irst she tried to lookndown and make out what she '},\n",
       " {'input_ids': tensor([ 160,   11,   12,   29,  210,  106,  588,  125,   47,  352,   12, 1837,\n",
       "           416,  166,  255, 1971,   12,  320,   29, 3035,   11,  143,   91,  125,\n",
       "           255,    1]),\n",
       "  'input_text': ' her and tonwonder what was\\ngoing to happen next first she tried to lookndown and make out what she ',\n",
       "  'expected_output': 'was coming to but it\\nwas too dark tonsee anything '},\n",
       " {'input_ids': tensor([   3,   23,   52,    7,   17,  255, 1971,   12,  320,   29, 3035,   11,\n",
       "           143,   91,  125,  255,   47, 1107,   12,   68,   34,   47,  396, 2164,\n",
       "            12,   29, 2338,  959,    1]),\n",
       "  'input_text': 'irst she tried to lookndown and make out what she was coming to but it\\nwas too dark tonsee anything ',\n",
       "  'expected_output': 'then she looked at the sides of the well andnnotic'},\n",
       " {'input_ids': tensor([  47, 1107,   12,   68,   34,   47,  396, 2164,   12,   29, 2338,  959,\n",
       "           258,  255, 2299,   44,    8, 4458,   13,    8,  168,   11,   29,   29,\n",
       "          9798,    1]),\n",
       "  'input_text': 'was coming to but it\\nwas too dark tonsee anything then she looked at the sides of the well andnnotic',\n",
       "  'expected_output': 'ed that they were\\nfilled with cupboards and booksh'},\n",
       " {'input_ids': tensor([  258,   255,  2299,    44,     8,  4458,    13,     8,   168,    11,\n",
       "             29,  2264,   867,    26,    24,    79,   130,  3353,    28, 19904,\n",
       "              7,    11,  1335,   107,     1]),\n",
       "  'input_text': 'then she looked at the sides of the well andnnoticed that they were\\nfilled with cupboards and booksh',\n",
       "  'expected_output': 'elvesnhere and there she saw maps and pictures hun'},\n",
       " {'input_ids': tensor([    3,    15,    26,    24,    79,   130,  3353,    28, 19904,     7,\n",
       "             11,  1335,    88,  8391,    29,    88,    60,    11,   132,   255,\n",
       "           1509,  8111,    11,  1933,     3,   107,   202,     1]),\n",
       "  'input_text': 'ed that they were\\nfilled with cupboards and bookshelvesnhere and there she saw maps and pictures hun',\n",
       "  'expected_output': 'g upon\\npegs shentook down a jar from one of the sh'},\n",
       " {'input_ids': tensor([   3,   15, 8391,   29,   88,   60,   11,  132,  255, 1509, 8111,   11,\n",
       "          1933,    3, 6668, 1286,  158,  122,    7,  255,   29,  235, 1825,  323,\n",
       "             3,    9,    3, 5670,   45,   80,   13,    8, 6660,    1]),\n",
       "  'input_text': 'elvesnhere and there she saw maps and pictures hung upon\\npegs shentook down a jar from one of the sh',\n",
       "  'expected_output': 'elves as she passed it wasnlabelled orange\\nmarmala'},\n",
       " {'input_ids': tensor([    3,   122,  1286,   158,   122,     7,   255,    29,   235,  1825,\n",
       "            323,     3,     9,     3,  5670,    45,    80,    13,     8, 12131,\n",
       "             38,   255,  2804,    34,  2088, 29506,  5470,  3157,  1982,     9,\n",
       "              1]),\n",
       "  'input_text': 'g upon\\npegs shentook down a jar from one of the shelves as she passed it wasnlabelled orange\\nmarmala',\n",
       "  'expected_output': 'de but to her great disappointment itnwas empty sh'},\n",
       " {'input_ids': tensor([    3,    15,  8391,    38,   255,  2804,    34,  2088, 29506,  5470,\n",
       "              3,    51, 12764, 14712,    68,    12,   160,   248, 19142,    34,\n",
       "             29,  9491,  6364,  6660,     1]),\n",
       "  'input_text': 'elves as she passed it wasnlabelled orange\\nmarmalade but to her great disappointment itnwas empty sh',\n",
       "  'expected_output': 'e did not like to drop the jar for\\nfear of killing'},\n",
       " {'input_ids': tensor([   20,    68,    12,   160,   248, 19142,    34,    29,  9491,  6364,\n",
       "            255,   410,    59,   114,    12,  2328,     8,     3,  5670,    21,\n",
       "           2971,    13,  9357,     1]),\n",
       "  'input_text': 'de but to her great disappointment itnwas empty she did not like to drop the jar for\\nfear of killing',\n",
       "  'expected_output': 'nsomebody so managed to put it into one of the cup'},\n",
       " {'input_ids': tensor([   3,   15,  410,   59,  114,   12, 2328,    8,    3, 5670,   21, 2971,\n",
       "            13, 9357,   29, 5529, 6965,   78, 3030,   12,  474,   34,  139,   80,\n",
       "            13,    8, 4119,    1]),\n",
       "  'input_text': 'e did not like to drop the jar for\\nfear of killingnsomebody so managed to put it into one of the cup',\n",
       "  'expected_output': 'boards as shenfell past itoverlapchunk 1\\nchunk 2\\nk'},\n",
       " {'input_ids': tensor([    3,    29,  5529,  6965,    78,  3030,    12,   474,    34,   139,\n",
       "             80,    13,     8, 19904,     7,    38,   255,    29,  4025,    40,\n",
       "            657,    34,  1890,  8478,   524,  6513,   209, 16749,   204,     3,\n",
       "            157,     1]),\n",
       "  'input_text': 'nsomebody so managed to put it into one of the cupboards as shenfell past itoverlapchunk 1\\nchunk 2\\nk',\n",
       "  'expected_output': 'eep calm  build ai16abhinav kimothiindexing pipeli'},\n",
       " {'input_ids': tensor([ 8126,    38,   255,    29,  4025,    40,   657,    34,  1890,  8478,\n",
       "            524,  6513,   209, 16749,   204,   453,  4447,   918,     3,     9,\n",
       "             23,  2938,     9,   115,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436, 18288,    53,  7119,    40,    23,     1]),\n",
       "  'input_text': 'boards as shenfell past itoverlapchunk 1\\nchunk 2\\nkeep calm  build ai16abhinav kimothiindexing pipeli',\n",
       "  'expected_output': 'ne document splitting\\nlets find out how many chunk'},\n",
       " {'input_ids': tensor([    3,    15,    15,   102,  4447,   918,     3,     9,    23,  2938,\n",
       "              9,   115,  2907,     9,   208,     3, 19754,    32,  7436, 18288,\n",
       "             53, 12045,  1708, 28503,  8857,   253,    91,   149,   186, 16749,\n",
       "              1]),\n",
       "  'input_text': 'eep calm  build ai16abhinav kimothiindexing pipeline document splitting\\nlets find out how many chunk',\n",
       "  'expected_output': 's were created \\ntotal number of chunks created  93'},\n",
       " {'input_ids': tensor([    3,    29,    15,  1708, 28503,  8857,   253,    91,   149,   186,\n",
       "          16749,     7,   130,   990,   792,   381,    13, 16749,     7,   990,\n",
       "              3,  4271,     1]),\n",
       "  'input_text': 'ne document splitting\\nlets find out how many chunks were created \\ntotal number of chunks created  93',\n",
       "  'expected_output': '\\nlength of the first chunk is  1777 characters\\nlen'},\n",
       " {'input_ids': tensor([    3,     7,   130,   990,   792,   381,    13, 16749,     7,   990,\n",
       "              3,  4271,  2475,    13,     8,   166, 16749,    19,  1003,  4013,\n",
       "           2850,    90,    29,     1]),\n",
       "  'input_text': 's were created \\ntotal number of chunks created  93\\nlength of the first chunk is  1777 characters\\nlen',\n",
       "  'expected_output': 'gth of the last chunk is  816 characters\\nrecursive'},\n",
       " {'input_ids': tensor([ 2475,    13,     8,   166, 16749,    19,  1003,  4013,  2850,  2475,\n",
       "             13,     8,   336, 16749,    19,   505,  2938,  2850,     3,    60,\n",
       "          15983,   757,     1]),\n",
       "  'input_text': '\\nlength of the first chunk is  1777 characters\\nlength of the last chunk is  816 characters\\nrecursive',\n",
       "  'expected_output': ' split by character\\na subtle variation to splittin'},\n",
       " {'input_ids': tensor([    3,   122,   189,    13,     8,   336, 16749,    19,   505,  2938,\n",
       "           2850,     3,    60, 15983,   757,  5679,    57,  1848,     3,     9,\n",
       "           9240, 12338,    12,  5679,    17,    77,     1]),\n",
       "  'input_text': 'gth of the last chunk is  816 characters\\nrecursive split by character\\na subtle variation to splittin',\n",
       "  'expected_output': 'g by character is recursive split the only differe'},\n",
       " {'input_ids': tensor([ 5679,    57,  1848,     3,     9,  9240, 12338,    12, 28503,    57,\n",
       "           1848,    19,     3,    60, 15983,   757,  5679,     8,   163,  7641,\n",
       "             15,     1]),\n",
       "  'input_text': ' split by character\\na subtle variation to splitting by character is recursive split the only differe',\n",
       "  'expected_output': 'nce\\nis that instead of a single character used for'},\n",
       " {'input_ids': tensor([    3,   122,    57,  1848,    19,     3,    60, 15983,   757,  5679,\n",
       "              8,   163,  1750,    19,    24,  1446,    13,     3,     9,   712,\n",
       "           1848,   261,    21,     1]),\n",
       "  'input_text': 'g by character is recursive split the only difference\\nis that instead of a single character used for',\n",
       "  'expected_output': ' splitting this technique uses a list of\\ncharacter'},\n",
       " {'input_ids': tensor([    3,  3772,    19,    24,  1446,    13,     3,     9,   712,  1848,\n",
       "            261,    21, 28503,    48,  3317,  2284,     3,     9,   570,    13,\n",
       "           1848,     1]),\n",
       "  'input_text': 'nce\\nis that instead of a single character used for splitting this technique uses a list of\\ncharacter',\n",
       "  'expected_output': 's and tries to split hierarchically till the chunk'},\n",
       " {'input_ids': tensor([28503,    48,  3317,  2284,     3,     9,   570,    13,  2850,    11,\n",
       "              3,  9000,    12,  5679,  1382,  7064,  6402,  6501,     8, 16749,\n",
       "              1]),\n",
       "  'input_text': ' splitting this technique uses a list of\\ncharacters and tries to split hierarchically till the chunk',\n",
       "  'expected_output': ' sizes are small enough\\nthis technique is generall'},\n",
       " {'input_ids': tensor([    3,     7,    11,     3,  9000,    12,  5679,  1382,  7064,  6402,\n",
       "           6501,     8, 16749,  4342,    33,   422,   631,    48,  3317,    19,\n",
       "            879,    40,     1]),\n",
       "  'input_text': 's and tries to split hierarchically till the chunk sizes are small enough\\nthis technique is generall',\n",
       "  'expected_output': 'y recommended for generic text\\nexample text  akbus'},\n",
       " {'input_ids': tensor([4342,   33,  422,  631,   48, 3317,   19, 2389, 3024,   21, 8165, 1499,\n",
       "           677, 1499,    3, 1639, 3465,    1]),\n",
       "  'input_text': ' sizes are small enough\\nthis technique is generally recommended for generic text\\nexample text  akbus',\n",
       "  'expected_output': 'ypersonintrollmtxt \\ntranscript of a youtube video '},\n",
       " {'input_ids': tensor([    3,    63,  3024,    21,  8165,  1499,   677,  1499,     3,  1639,\n",
       "           3465,    63,  6075,    77,    17,  4046,    51,    17,   226,    17,\n",
       "          20146,    13,     3,     9,    25,  9863,   671,     1]),\n",
       "  'input_text': 'y recommended for generic text\\nexample text  akbusypersonintrollmtxt \\ntranscript of a youtube video ',\n",
       "  'expected_output': 'by andrej karpathy titled 1hr talk intro to large '},\n",
       " {'input_ids': tensor([    3,    63,  6075,    77,    17,  4046,    51,    17,   226,    17,\n",
       "          20146,    13,     3,     9,    25,  9863,   671,    57,    11,    60,\n",
       "            354,     3,  4031, 24136,     3, 10920,   209,   107,    52,  1350,\n",
       "          16728,    12,   508,     1]),\n",
       "  'input_text': 'ypersonintrollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1hr talk intro to large ',\n",
       "  'expected_output': 'language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnj'},\n",
       " {'input_ids': tensor([   57,    11,    60,   354,     3,  4031, 24136,     3, 10920,   209,\n",
       "            107,    52,  1350, 16728,    12,   508,  1612,  2250,  4893,  1986,\n",
       "           4188,  9863,   287,  9237,   208,   172,   354,   157,   115,    51,\n",
       "             89,   107,    29,   354,     1]),\n",
       "  'input_text': 'by andrej karpathy titled 1hr talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnj',\n",
       "  'expected_output': 'gt9s  \\nusing langchains recursivecharactertextspli'},\n",
       " {'input_ids': tensor([ 1612,  2250,  4893,  1986,  4188,  9863,   287,  9237,   208,   172,\n",
       "            354,   157,   115,    51,    89,   107,    29,   354,   122,    17,\n",
       "           1298,     7,   338, 12142, 19836,     7,     3,    60, 15983,   757,\n",
       "          31886,  6327,     7,  5900,     1]),\n",
       "  'input_text': 'language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing langchains recursivecharactertextspli',\n",
       "  'expected_output': 'tter\\nthis is a generic text that is not formatted '},\n",
       " {'input_ids': tensor([    3,   122,    17,  1298,     7,   338, 12142, 19836,     7,     3,\n",
       "             60, 15983,   757, 31886,  6327,     7,  5900,    17,   449,    48,\n",
       "             19,     3,     9,  8165,  1499,    24,    19,    59,  1910,  1054,\n",
       "              1]),\n",
       "  'input_text': 'gt9s  \\nusing langchains recursivecharactertextsplitter\\nthis is a generic text that is not formatted ',\n",
       "  'expected_output': 'lets compare the two strategies\\ntotal number of ch'},\n",
       " {'input_ids': tensor([   3,   17,  449,   48,   19,    3,    9, 8165, 1499,   24,   19,   59,\n",
       "          1910, 1054, 8857, 4048,    8,  192, 3266,  792,  381,   13,    3,  524,\n",
       "             1]),\n",
       "  'input_text': 'tter\\nthis is a generic text that is not formatted lets compare the two strategies\\ntotal number of ch',\n",
       "  'expected_output': 'unks created  1 \\nlength of the first chunk is  643'},\n",
       " {'input_ids': tensor([ 8857,  4048,     8,   192,  3266,   792,   381,    13, 16749,     7,\n",
       "            990,   209,  2475,    13,     8,   166, 16749,    19,   431,  4906,\n",
       "              1]),\n",
       "  'input_text': 'lets compare the two strategies\\ntotal number of chunks created  1 \\nlength of the first chunk is  643',\n",
       "  'expected_output': '83 characters\\n length of the last chunk is  64383 '},\n",
       " {'input_ids': tensor([    3,  6513,     7,   990,   209,  2475,    13,     8,   166, 16749,\n",
       "             19,   431,  4906,  4591,  2850,  2475,    13,     8,   336, 16749,\n",
       "             19,   431,  4906,  4591,     1]),\n",
       "  'input_text': 'unks created  1 \\nlength of the first chunk is  64383 characters\\n length of the last chunk is  64383 ',\n",
       "  'expected_output': 'characters\\ntext splitter fails to convert the text'},\n",
       " {'input_ids': tensor([    3,  4591,  2850,  2475,    13,     8,   336, 16749,    19,   431,\n",
       "           4906,  4591,  2850,  1499,  5679,   449, 13288,    12,  5755,     8,\n",
       "           1499,     1]),\n",
       "  'input_text': '83 characters\\n length of the last chunk is  64383 characters\\ntext splitter fails to convert the text',\n",
       "  'expected_output': ' into chunks since\\nthere are no nn character prese'},\n",
       " {'input_ids': tensor([ 2850,  1499,  5679,   449, 13288,    12,  5755,     8,  1499,   139,\n",
       "          16749,     7,   437,   132,    33,   150,     3,    29,    29,  1848,\n",
       "          17497,     1]),\n",
       "  'input_text': 'characters\\ntext splitter fails to convert the text into chunks since\\nthere are no nn character prese',\n",
       "  'expected_output': 'nt in the raw transcriptwith charactertextsplitter'},\n",
       " {'input_ids': tensor([  139, 16749,     7,   437,   132,    33,   150,     3,    29,    29,\n",
       "           1848,   915,    16,     8,  5902, 20146,  4065,  1848,  6327,     7,\n",
       "           5900,    17,   449,     1]),\n",
       "  'input_text': ' into chunks since\\nthere are no nn character present in the raw transcriptwith charactertextsplitter',\n",
       "  'expected_output': '\\nkeep calm  build ai17abhinav kimothiindexing pipe'},\n",
       " {'input_ids': tensor([    3,    29,    17,    16,     8,  5902, 20146,  4065,  1848,  6327,\n",
       "              7,  5900,    17,   449,   453,  4447,   918,     3,     9,    23,\n",
       "           2517,     9,   115,  2907,     9,   208,     3, 19754,    32,  7436,\n",
       "          18288,    53,  7119,     1]),\n",
       "  'input_text': 'nt in the raw transcriptwith charactertextsplitter\\nkeep calm  build ai17abhinav kimothiindexing pipe',\n",
       "  'expected_output': 'line document splitting\\ntotal number of chunks cre'},\n",
       " {'input_ids': tensor([  453,  4447,   918,     3,     9,    23,  2517,     9,   115,  2907,\n",
       "              9,   208,     3, 19754,    32,  7436, 18288,    53, 12045,  1708,\n",
       "          28503,   792,   381,    13, 16749,     7,  3935,     1]),\n",
       "  'input_text': '\\nkeep calm  build ai17abhinav kimothiindexing pipeline document splitting\\ntotal number of chunks cre',\n",
       "  'expected_output': 'ated  40\\nlength of the first chunk is  1998 charac'},\n",
       " {'input_ids': tensor([  689,  1708, 28503,   792,   381,    13, 16749,     7,   990,  1283,\n",
       "           2475,    13,     8,   166, 16749,    19,  6260,     3,  3441,  3738,\n",
       "              1]),\n",
       "  'input_text': 'line document splitting\\ntotal number of chunks created  40\\nlength of the first chunk is  1998 charac',\n",
       "  'expected_output': 'ters\\n length of the last chunk is  1967 characters'},\n",
       " {'input_ids': tensor([    3,   920,  1283,  2475,    13,     8,   166, 16749,    19,  6260,\n",
       "           2850,  2475,    13,     8,   336, 16749,    19, 18148,  2850,     1]),\n",
       "  'input_text': 'ated  40\\nlength of the first chunk is  1998 characters\\n length of the last chunk is  1967 characters',\n",
       "  'expected_output': '\\nrecursive text splitter performs well in dealing\\n'},\n",
       " {'input_ids': tensor([    3,  4849,  2475,    13,     8,   336, 16749,    19, 18148,  2850,\n",
       "              3,    60, 15983,   757,  1499,  5679,   449,  1912,     7,   168,\n",
       "             16,  4945,     1]),\n",
       "  'input_text': 'ters\\n length of the last chunk is  1967 characters\\nrecursive text splitter performs well in dealing\\n',\n",
       "  'expected_output': 'with generic textwith recursivecharactertextsplitt'},\n",
       " {'input_ids': tensor([    3,    60, 15983,   757,  1499,  5679,   449,  1912,     7,   168,\n",
       "             16,  4945,    28,  8165,  1499,  4065,     3,    60, 15983,   757,\n",
       "          31886,  6327,     7,  5900,    17,    17,     1]),\n",
       "  'input_text': '\\nrecursive text splitter performs well in dealing\\nwith generic textwith recursivecharactertextsplitt',\n",
       "  'expected_output': 'er\\nsplit by tokens\\nfor those well versed with larg'},\n",
       " {'input_ids': tensor([   28,  8165,  1499,  4065,     3,    60, 15983,   757, 31886,  6327,\n",
       "              7,  5900,    17,   449,  5679,    57, 14145,     7,    21,   273,\n",
       "            168, 11261,    26,    28, 13332,     1]),\n",
       "  'input_text': 'with generic textwith recursivecharactertextsplitter\\nsplit by tokens\\nfor those well versed with larg',\n",
       "  'expected_output': 'e language models tokens is not a new concept\\nall '},\n",
       " {'input_ids': tensor([    3,    49,  5679,    57, 14145,     7,    21,   273,   168, 11261,\n",
       "             26,    28,   508,  1612,  2250, 14145,     7,    19,    59,     3,\n",
       "              9,   126,  2077,    66,     1]),\n",
       "  'input_text': 'er\\nsplit by tokens\\nfor those well versed with large language models tokens is not a new concept\\nall ',\n",
       "  'expected_output': 'llms have a token limit in their respective contex'},\n",
       " {'input_ids': tensor([    3,    15,  1612,  2250, 14145,     7,    19,    59,     3,     9,\n",
       "            126,  2077,    66,     3,   195,    51,     7,    43,     3,     9,\n",
       "          14145,  2006,    16,    70,  6477,  3622,   994,     1]),\n",
       "  'input_text': 'e language models tokens is not a new concept\\nall llms have a token limit in their respective contex',\n",
       "  'expected_output': 't windows which we cannot\\nexceed it is therefore a'},\n",
       " {'input_ids': tensor([    3,   195,    51,     7,    43,     3,     9, 14145,  2006,    16,\n",
       "             70,  6477,  2625,  3196,    84,    62,  1178,  8193,    34,    19,\n",
       "           2459,     3,     9,     1]),\n",
       "  'input_text': 'llms have a token limit in their respective context windows which we cannot\\nexceed it is therefore a',\n",
       "  'expected_output': ' good idea to count the tokens while creating chun'},\n",
       " {'input_ids': tensor([    3,    17,  3196,    84,    62,  1178,  8193,    34,    19,  2459,\n",
       "              3,     9,   207,   800,    12,  3476,     8, 14145,     7,   298,\n",
       "           1577,     3,   524,   202,     1]),\n",
       "  'input_text': 't windows which we cannot\\nexceed it is therefore a good idea to count the tokens while creating chun',\n",
       "  'expected_output': 'ks  all\\nllms also have their tokenizers \\ntiktoken '},\n",
       " {'input_ids': tensor([  207,   800,    12,  3476,     8, 14145,     7,   298,  1577, 16749,\n",
       "              7,    66,     3,   195,    51,     7,    92,    43,    70, 14145,\n",
       "           8585,     7,     3,  4414,   235,  2217,     1]),\n",
       "  'input_text': ' good idea to count the tokens while creating chunks  all\\nllms also have their tokenizers \\ntiktoken ',\n",
       "  'expected_output': 'tokenizer\\ntiktoken tokenizer has been created by o'},\n",
       " {'input_ids': tensor([    3,   157,     7,    66,     3,   195,    51,     7,    92,    43,\n",
       "             70, 14145,  8585,     7,     3,  4414,   235,  2217, 14145,  8585,\n",
       "              3,  4414,   235,  2217, 14145,  8585,    65,   118,   990,    57,\n",
       "              3,    32,     1]),\n",
       "  'input_text': 'ks  all\\nllms also have their tokenizers \\ntiktoken tokenizer\\ntiktoken tokenizer has been created by o',\n",
       "  'expected_output': 'penai for their family of models using\\nthis strate'},\n",
       " {'input_ids': tensor([14145,  8585,     3,  4414,   235,  2217, 14145,  8585,    65,   118,\n",
       "            990,    57,   539,     9,    23,    21,    70,   384,    13,  2250,\n",
       "            338,    48, 10133,    15,     1]),\n",
       "  'input_text': 'tokenizer\\ntiktoken tokenizer has been created by openai for their family of models using\\nthis strate',\n",
       "  'expected_output': 'gy the split still happens based on the character '},\n",
       " {'input_ids': tensor([4550,    9,   23,   21,   70,  384,   13, 2250,  338,   48, 1998,    8,\n",
       "          5679,  341, 2906,    3,  390,   30,    8, 1848,    1]),\n",
       "  'input_text': 'penai for their family of models using\\nthis strategy the split still happens based on the character ',\n",
       "  'expected_output': 'however the length\\nof the chunk is determined by t'},\n",
       " {'input_ids': tensor([    3,   122,    63,     8,  5679,   341,  2906,     3,   390,    30,\n",
       "              8,  1848,   983,     8,  2475,    13,     8, 16749,    19,  4187,\n",
       "             57,     3,    17,     1]),\n",
       "  'input_text': 'gy the split still happens based on the character however the length\\nof the chunk is determined by t',\n",
       "  'expected_output': 'he number of tokens\\nkeep calm  build ai18abhinav k'},\n",
       " {'input_ids': tensor([  983,     8,  2475,    13,     8, 16749,    19,  4187,    57,     8,\n",
       "            381,    13, 14145,     7,   453,  4447,   918,     3,     9,    23,\n",
       "           2606,     9,   115,  2907,     9,   208,     3,   157,     1]),\n",
       "  'input_text': 'however the length\\nof the chunk is determined by the number of tokens\\nkeep calm  build ai18abhinav k',\n",
       "  'expected_output': 'imothiindexing pipeline document splitting\\ntotal n'},\n",
       " {'input_ids': tensor([    3,    88,   381,    13, 14145,     7,   453,  4447,   918,     3,\n",
       "              9,    23,  2606,     9,   115,  2907,     9,   208,     3, 19754,\n",
       "             32,  7436, 18288,    53, 12045,  1708, 28503,   792,     3,    29,\n",
       "              1]),\n",
       "  'input_text': 'he number of tokens\\nkeep calm  build ai18abhinav kimothiindexing pipeline document splitting\\ntotal n',\n",
       "  'expected_output': 'umber of chunks created  14 \\ntotal number of token'},\n",
       " {'input_ids': tensor([  256,    32,  7436, 18288,    53, 12045,  1708, 28503,   792,   381,\n",
       "             13, 16749,     7,   990,   968,   792,   381,    13, 14145,     1]),\n",
       "  'input_text': 'imothiindexing pipeline document splitting\\ntotal number of chunks created  14 \\ntotal number of token',\n",
       "  'expected_output': 's in the document  12865 tokens \\nlength of the fir'},\n",
       " {'input_ids': tensor([  561,  1152,    13, 16749,     7,   990,   968,   792,   381,    13,\n",
       "          14145,     7,    16,     8,  1708,   586,  3840,   755, 14145,     7,\n",
       "           2475,    13,     8,     3, 14581,     1]),\n",
       "  'input_text': 'umber of chunks created  14 \\ntotal number of tokens in the document  12865 tokens \\nlength of the fir',\n",
       "  'expected_output': 'st chunk is  1014 tokens\\nlength of the last chunk '},\n",
       " {'input_ids': tensor([    3,     7,    16,     8,  1708,   586,  3840,   755, 14145,     7,\n",
       "           2475,    13,     8,   166, 16749,    19,   335,  2534, 14145,     7,\n",
       "           2475,    13,     8,   336, 16749,     1]),\n",
       "  'input_text': 's in the document  12865 tokens \\nlength of the first chunk is  1014 tokens\\nlength of the last chunk ',\n",
       "  'expected_output': 'is  1014 tokens\\ntokenizers are helpful in creating'},\n",
       " {'input_ids': tensor([    3,     7,    17, 16749,    19,   335,  2534, 14145,     7,  2475,\n",
       "             13,     8,   336, 16749,    19,   335,  2534, 14145,     7, 14145,\n",
       "           8585,     7,    33,  2690,    16,  1577,     1]),\n",
       "  'input_text': 'st chunk is  1014 tokens\\nlength of the last chunk is  1014 tokens\\ntokenizers are helpful in creating',\n",
       "  'expected_output': ' chunks that sit\\nwell in the context window of an '},\n",
       " {'input_ids': tensor([   19,   335,  2534, 14145,     7, 14145,  8585,     7,    33,  2690,\n",
       "             16,  1577, 16749,     7,    24,  2561,   168,    16,     8,  2625,\n",
       "           2034,    13,    46,     1]),\n",
       "  'input_text': 'is  1014 tokens\\ntokenizers are helpful in creating chunks that sit\\nwell in the context window of an ',\n",
       "  'expected_output': 'llmexample text  akbusypersonintrollmtxt \\ntranscri'},\n",
       " {'input_ids': tensor([16749,     7,    24,  2561,   168,    16,     8,  2625,  2034,    13,\n",
       "             46,     3,   195,    51,   994,     9,  9208,  1499,     3,  1639,\n",
       "           3465,    63,  6075,    77,    17,  4046,    51,    17,   226,    17,\n",
       "           3017,  2685,     1]),\n",
       "  'input_text': ' chunks that sit\\nwell in the context window of an llmexample text  akbusypersonintrollmtxt \\ntranscri',\n",
       "  'expected_output': 'pt of a youtube video by andrej karpathy titled 1h'},\n",
       " {'input_ids': tensor([    3,   195,    51,   994,     9,  9208,  1499,     3,  1639,  3465,\n",
       "             63,  6075,    77,    17,  4046,    51,    17,   226,    17, 20146,\n",
       "             13,     3,     9,    25,  9863,   671,    57,    11,    60,   354,\n",
       "              3,  4031, 24136,     3, 10920,   209,   107,     1]),\n",
       "  'input_text': 'llmexample text  akbusypersonintrollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1h',\n",
       "  'expected_output': 'r talk intro to large language\\nmodels  httpswwwyou'},\n",
       " {'input_ids': tensor([    3,   102,    17,    13,     3,     9,    25,  9863,   671,    57,\n",
       "             11,    60,   354,     3,  4031, 24136,     3, 10920,   209,   107,\n",
       "             52,  1350, 16728,    12,   508,  1612,  2250,  4893,  1986,  4188,\n",
       "              1]),\n",
       "  'input_text': 'pt of a youtube video by andrej karpathy titled 1hr talk intro to large language\\nmodels  httpswwwyou',\n",
       "  'expected_output': 'tubecomwatchvzjkbmfhnjgt9s  \\nusing langchains toke'},\n",
       " {'input_ids': tensor([    3,    52,  1350, 16728,    12,   508,  1612,  2250,  4893,  1986,\n",
       "           4188,  9863,   287,  9237,   208,   172,   354,   157,   115,    51,\n",
       "             89,   107,    29,   354,   122,    17,  1298,     7,   338, 12142,\n",
       "          19836,     7,    12,  1050,     1]),\n",
       "  'input_text': 'r talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing langchains toke',\n",
       "  'expected_output': 'ntextsplitter\\nhugging face tokenizer\\nhugging face '},\n",
       " {'input_ids': tensor([ 8017,   287,  9237,   208,   172,   354,   157,   115,    51,    89,\n",
       "            107,    29,   354,   122,    17,  1298,     7,   338, 12142, 19836,\n",
       "              7, 14145,  6327,     7,  5900,    17,   449, 18233,  3896,   522,\n",
       "          14145,  8585, 18233,  3896,   522,     1]),\n",
       "  'input_text': 'tubecomwatchvzjkbmfhnjgt9s  \\nusing langchains tokentextsplitter\\nhugging face tokenizer\\nhugging face ',\n",
       "  'expected_output': 'has become the goto platform for anyone building a'},\n",
       " {'input_ids': tensor([    3,    29,  6327,     7,  5900,    17,   449, 18233,  3896,   522,\n",
       "          14145,  8585, 18233,  3896,   522,    65,   582,     8,   530,    32,\n",
       "           1585,    21,  1321,   740,     3,     9,     1]),\n",
       "  'input_text': 'ntextsplitter\\nhugging face tokenizer\\nhugging face has become the goto platform for anyone building a',\n",
       "  'expected_output': 'pps using llms\\nor even other models all models ava'},\n",
       " {'input_ids': tensor([  65,  582,    8,  530,   32, 1585,   21, 1321,  740, 4050,  338,    3,\n",
       "           195,   51,    7,   42,  237,  119, 2250,   66, 2250,    3, 8644,    1]),\n",
       "  'input_text': 'has become the goto platform for anyone building apps using llms\\nor even other models all models ava',\n",
       "  'expected_output': 'ilable via hugging face are also accompanied\\nby th'},\n",
       " {'input_ids': tensor([    3,  1572,     7,   338,     3,   195,    51,     7,    42,   237,\n",
       "            119,  2250,    66,  2250,   347,  1009, 18233,  3896,   522,    33,\n",
       "             92,     3, 10102,    57,     3,   189,     1]),\n",
       "  'input_text': 'pps using llms\\nor even other models all models available via hugging face are also accompanied\\nby th',\n",
       "  'expected_output': 'eir tokenizers\\nkeep calm  build ai19abhinav kimoth'},\n",
       " {'input_ids': tensor([    3,   173,   179,  1009, 18233,  3896,   522,    33,    92,     3,\n",
       "          10102,    57,    70, 14145,  8585,     7,   453,  4447,   918,     3,\n",
       "              9,    23,  2294,     9,   115,  2907,     9,   208,     3, 19754,\n",
       "             32,   189,     1]),\n",
       "  'input_text': 'ilable via hugging face are also accompanied\\nby their tokenizers\\nkeep calm  build ai19abhinav kimoth',\n",
       "  'expected_output': 'iindexing pipeline document splitting\\ntexts0 \\nhi e'},\n",
       " {'input_ids': tensor([    3,    15,    23,    52, 14145,  8585,     7,   453,  4447,   918,\n",
       "              3,     9,    23,  2294,     9,   115,  2907,     9,   208,     3,\n",
       "          19754,    32,  7436, 18288,    53, 12045,  1708, 28503, 14877,   632,\n",
       "           7102,     3,    15,     1]),\n",
       "  'input_text': 'eir tokenizers\\nkeep calm  build ai19abhinav kimothiindexing pipeline document splitting\\ntexts0 \\nhi e',\n",
       "  'expected_output': 'veryone so recently i gave a 30minute talk on larg'},\n",
       " {'input_ids': tensor([    3,    23, 18288,    53, 12045,  1708, 28503, 14877,   632,  7102,\n",
       "            921,    78,  1310,     3,    23,  1891,     3,     9,   604,  6890,\n",
       "           1350,    30, 13332,     1]),\n",
       "  'input_text': 'iindexing pipeline document splitting\\ntexts0 \\nhi everyone so recently i gave a 30minute talk on larg',\n",
       "  'expected_output': 'e language\\nmodels just kind of like an intro talk '},\n",
       " {'input_ids': tensor([  182,   782,    78,  1310,     3,    23,  1891,     3,     9,   604,\n",
       "           6890,  1350,    30,   508,  1612,  2250,   131,   773,    13,   114,\n",
       "             46, 16728,  1350,     1]),\n",
       "  'input_text': 'veryone so recently i gave a 30minute talk on large language\\nmodels just kind of like an intro talk ',\n",
       "  'expected_output': 'um unfortunately that talk\\nwas not recorded but a '},\n",
       " {'input_ids': tensor([    3,    15,  1612,  2250,   131,   773,    13,   114,    46, 16728,\n",
       "           1350,   561, 12050,    24,  1350,    47,    59,  4381,    68,     3,\n",
       "              9,     1]),\n",
       "  'input_text': 'e language\\nmodels just kind of like an intro talk um unfortunately that talk\\nwas not recorded but a ',\n",
       "  'expected_output': 'lot of people came to me after the talk and\\nthey t'},\n",
       " {'input_ids': tensor([  561, 12050,    24,  1350,    47,    59,  4381,    68,     3,     9,\n",
       "            418,    13,   151,   764,    12,   140,   227,     8,  1350,    11,\n",
       "             79,     3,    17,     1]),\n",
       "  'input_text': 'um unfortunately that talk\\nwas not recorded but a lot of people came to me after the talk and\\nthey t',\n",
       "  'expected_output': 'old me that uh they really liked the talk so i wou'},\n",
       " {'input_ids': tensor([ 418,   13,  151,  764,   12,  140,  227,    8, 1350,   11,   79, 1219,\n",
       "           140,   24,    3,   76,  107,   79,  310, 6528,    8, 1350,   78,    3,\n",
       "            23, 2275,   76,    1]),\n",
       "  'input_text': 'lot of people came to me after the talk and\\nthey told me that uh they really liked the talk so i wou',\n",
       "  'expected_output': 'ld just i\\nthought i would just rerecord it and bas'},\n",
       " {'input_ids': tensor([ 625,  140,   24,    3,   76,  107,   79,  310, 6528,    8, 1350,   78,\n",
       "             3,   23,  133,  131,    3,   23,  816,    3,   23,  133,  131,    3,\n",
       "            60,   60, 7621,   34,   11, 3905,    1]),\n",
       "  'input_text': 'old me that uh they really liked the talk so i would just i\\nthought i would just rerecord it and bas',\n",
       "  'expected_output': 'ically put it up on\\nyoutube so here we go the busy'},\n",
       " {'input_ids': tensor([   3,   40,   26,  131,    3,   23,  816,    3,   23,  133,  131,    3,\n",
       "            60,   60, 7621,   34,   11, 6171,  474,   34,   95,   30,   25, 9863,\n",
       "            78,  270,   62,  281,    8, 3164,    1]),\n",
       "  'input_text': 'ld just i\\nthought i would just rerecord it and basically put it up on\\nyoutube so here we go the busy',\n",
       "  'expected_output': ' persons intro to large language\\nmodels director s'},\n",
       " {'input_ids': tensor([    3,  6402,   474,    34,    95,    30,    25,  9863,    78,   270,\n",
       "             62,   281,     8,  3164,  7609, 16728,    12,   508,  1612,  2250,\n",
       "           2090,     3,     7,     1]),\n",
       "  'input_text': 'ically put it up on\\nyoutube so here we go the busy persons intro to large language\\nmodels director s',\n",
       "  'expected_output': 'cott okay so lets begin first of all what is a lar'},\n",
       " {'input_ids': tensor([ 7609, 16728,    12,   508,  1612,  2250,  2090,     3,     7, 10405,\n",
       "           8957,    78,  8857,  1731,   166,    13,    66,   125,    19,     3,\n",
       "              9,    50,    52,     1]),\n",
       "  'input_text': ' persons intro to large language\\nmodels director scott okay so lets begin first of all what is a lar',\n",
       "  'expected_output': 'ge\\nlanguage model \\ntexts1\\nreally well a large lang'},\n",
       " {'input_ids': tensor([    3, 10405,  8957,    78,  8857,  1731,   166,    13,    66,   125,\n",
       "             19,     3,     9,   508,  1612,   825, 14877,   536,   310,   168,\n",
       "              3,     9,   508, 12142,     1]),\n",
       "  'input_text': 'cott okay so lets begin first of all what is a large\\nlanguage model \\ntexts1\\nreally well a large lang',\n",
       "  'expected_output': 'uage model is just two files right um there\\nbe two'},\n",
       " {'input_ids': tensor([  873,  1612,   825, 14877,   536,   310,   168,     3,     9,   508,\n",
       "           1612,   825,    19,   131,   192,  2073,   269,   561,   132,    36,\n",
       "            192,     1]),\n",
       "  'input_text': 'ge\\nlanguage model \\ntexts1\\nreally well a large language model is just two files right um there\\nbe two',\n",
       "  'expected_output': ' files in this hypothetical directory so for examp'},\n",
       " {'input_ids': tensor([    3,    76,   545,   825,    19,   131,   192,  2073,   269,   561,\n",
       "            132,    36,   192,  2073,    16,    48, 31754,  8174,    78,    21,\n",
       "           3631,   102,     1]),\n",
       "  'input_text': 'uage model is just two files right um there\\nbe two files in this hypothetical directory so for examp',\n",
       "  'expected_output': 'le work with\\nthe specific example of the llama 270'},\n",
       " {'input_ids': tensor([ 2073,    16,    48, 31754,  8174,    78,    21,   677,   161,    28,\n",
       "              8,   806,   677,    13,     8,     3,   195,   265,     9,     3,\n",
       "          17485,     1]),\n",
       "  'input_text': ' files in this hypothetical directory so for example work with\\nthe specific example of the llama 270',\n",
       "  'expected_output': 'b model this is a large\\nlanguage model released by'},\n",
       " {'input_ids': tensor([   90,   161,    28,     8,   806,   677,    13,     8,     3,   195,\n",
       "            265,     9,     3, 17485,   115,   825,    48,    19,     3,     9,\n",
       "            508,  1612,   825,  1883,    57,     1]),\n",
       "  'input_text': 'le work with\\nthe specific example of the llama 270b model this is a large\\nlanguage model released by',\n",
       "  'expected_output': ' meta ai and this is basically the llama\\nseries of'},\n",
       " {'input_ids': tensor([    3,   115,   825,    48,    19,     3,     9,   508,  1612,   825,\n",
       "           1883,    57, 10531,     3,     9,    23,    11,    48,    19,  6171,\n",
       "              8,     3,   195,   265,     9,   939,    13,     1]),\n",
       "  'input_text': 'b model this is a large\\nlanguage model released by meta ai and this is basically the llama\\nseries of',\n",
       "  'expected_output': ' language models the second iteration of it and th'},\n",
       " {'input_ids': tensor([10531,     3,     9,    23,    11,    48,    19,  6171,     8,     3,\n",
       "            195,   265,     9,   939,    13,  1612,  2250,     8,   511,    34,\n",
       "             49,   257,    13,    34,    11,     3,   189,     1]),\n",
       "  'input_text': ' meta ai and this is basically the llama\\nseries of language models the second iteration of it and th',\n",
       "  'expected_output': 'is is the\\n70 billion parameter model of uh of this'},\n",
       " {'input_ids': tensor([ 1612,  2250,     8,   511,    34,    49,   257,    13,    34,    11,\n",
       "             48,    19,     8,  2861,  2108, 15577,   825,    13,     3,    76,\n",
       "            107,    13,    48,     1]),\n",
       "  'input_text': ' language models the second iteration of it and this is the\\n70 billion parameter model of uh of this',\n",
       "  'expected_output': ' series so theres multiple\\nmodels uh belonging to '},\n",
       " {'input_ids': tensor([   19,    19,     8,  2861,  2108, 15577,   825,    13,     3,    76,\n",
       "            107,    13,    48,   939,    78,   132,     7,  1317,  2250,     3,\n",
       "             76,   107, 12770,    12,     1]),\n",
       "  'input_text': 'is is the\\n70 billion parameter model of uh of this series so theres multiple\\nmodels uh belonging to ',\n",
       "  'expected_output': 'the lama 2 series uh 7 billion um 13 billion\\n34 bi'},\n",
       " {'input_ids': tensor([  939,    78,   132,     7,  1317,  2250,     3,    76,   107, 12770,\n",
       "             12,     8,    50,    51,     9,   204,   939,     3,    76,   107,\n",
       "            489,  2108,   561,  1179,  2108,  6154,  2647,     1]),\n",
       "  'input_text': ' series so theres multiple\\nmodels uh belonging to the lama 2 series uh 7 billion um 13 billion\\n34 bi',\n",
       "  'expected_output': 'llion and 70 billion is the the no overlap as spec'},\n",
       " {'input_ids': tensor([    8,    50,    51,     9,   204,   939,     3,    76,   107,   489,\n",
       "           2108,   561,  1179,  2108,  6154,  2108,    11,  2861,  2108,    19,\n",
       "              8,     8,   150, 21655,    38,     3,  7576,     1]),\n",
       "  'input_text': 'the lama 2 series uh 7 billion um 13 billion\\n34 billion and 70 billion is the the no overlap as spec',\n",
       "  'expected_output': 'ifiedchunk 1\\nchunk 2example text  akbusypersonintr'},\n",
       " {'input_ids': tensor([    3,    40,  7325,    11,  2861,  2108,    19,     8,     8,   150,\n",
       "          21655,    38,  7173,   524,  6513,   209, 16749,   204,   994,     9,\n",
       "           9208,  1499,     3,  1639,  3465,    63,  6075, 20322,     1]),\n",
       "  'input_text': 'llion and 70 billion is the the no overlap as specifiedchunk 1\\nchunk 2example text  akbusypersonintr',\n",
       "  'expected_output': 'ollmtxt \\ntranscript of a youtube video by andrej k'},\n",
       " {'input_ids': tensor([    3,  3676,   524,  6513,   209, 16749,   204,   994,     9,  9208,\n",
       "           1499,     3,  1639,  3465,    63,  6075,    77,    17,  4046,    51,\n",
       "             17,   226,    17, 20146,    13,     3,     9,    25,  9863,   671,\n",
       "             57,    11,    60,   354,     3,   157,     1]),\n",
       "  'input_text': 'ifiedchunk 1\\nchunk 2example text  akbusypersonintrollmtxt \\ntranscript of a youtube video by andrej k',\n",
       "  'expected_output': 'arpathy titled 1hr talk intro to large language\\nmo'},\n",
       " {'input_ids': tensor([    3,    32,   195,    51,    17,   226,    17, 20146,    13,     3,\n",
       "              9,    25,  9863,   671,    57,    11,    60,   354,     3,  4031,\n",
       "          24136,     3, 10920,   209,   107,    52,  1350, 16728,    12,   508,\n",
       "           1612,  2288,     1]),\n",
       "  'input_text': 'ollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1hr talk intro to large language\\nmo',\n",
       "  'expected_output': 'dels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusin'},\n",
       " {'input_ids': tensor([ 1584, 24136,     3, 10920,   209,   107,    52,  1350, 16728,    12,\n",
       "            508,  1612,  2250,  4893,  1986,  4188,  9863,   287,  9237,   208,\n",
       "            172,   354,   157,   115,    51,    89,   107,    29,   354,   122,\n",
       "             17,  1298,     7,   178,    77,     1]),\n",
       "  'input_text': 'arpathy titled 1hr talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusin',\n",
       "  'expected_output': 'g transformers and langchains recursivecharacterte'},\n",
       " {'input_ids': tensor([   20,    40,     7,  4893,  1986,  4188,  9863,   287,  9237,   208,\n",
       "            172,   354,   157,   115,    51,    89,   107,    29,   354,   122,\n",
       "             17,  1298,     7,   338, 19903,     7,    11, 12142, 19836,     7,\n",
       "              3,    60, 15983,   757, 31886,    17,    15,     1]),\n",
       "  'input_text': 'dels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing transformers and langchains recursivecharacterte',\n",
       "  'expected_output': 'xtsplitter\\nexample tokenizer  gpt2tokenizerfast\\nht'},\n",
       " {'input_ids': tensor([    3,   122, 19903,     7,    11, 12142, 19836,     7,     3,    60,\n",
       "          15983,   757, 31886,  6327,     7,  5900,    17,   449,   677, 14145,\n",
       "           8585,     3,   122,   102,    17,   357,   235,  2217,  8585, 11584,\n",
       "              3,   107,    17,     1]),\n",
       "  'input_text': 'g transformers and langchains recursivecharactertextsplitter\\nexample tokenizer  gpt2tokenizerfast\\nht',\n",
       "  'expected_output': 'tpshuggingfacecodocstransformerstokenizersummary\\nd'},\n",
       " {'input_ids': tensor([    3,   226,    17,     7,  5900,    17,   449,   677, 14145,  8585,\n",
       "              3,   122,   102,    17,   357,   235,  2217,  8585, 11584,  4893,\n",
       "            107, 13917,    53,  4861,   509,  7171,     7,  7031,  2032,   277,\n",
       "            235,  2217,  8585,  4078,    51,  1208,     3,    26,     1]),\n",
       "  'input_text': 'xtsplitter\\nexample tokenizer  gpt2tokenizerfast\\nhttpshuggingfacecodocstransformerstokenizersummary\\nd',\n",
       "  'expected_output': 'o take a look at hugging face documents on tokeniz'},\n",
       " {'input_ids': tensor([    3,    17,   102, 14279,  4102,    53,  4861,   509,  7171,     7,\n",
       "           7031,  2032,   277,   235,  2217,  8585,  4078,    51,  1208,   103,\n",
       "            240,     3,     9,   320,    44, 18233,  3896,   522,  2691,    30,\n",
       "          14145,    23,   172,     1]),\n",
       "  'input_text': 'tpshuggingfacecodocstransformerstokenizersummary\\ndo take a look at hugging face documents on tokeniz',\n",
       "  'expected_output': 'ers\\nkeep calm  build ai20abhinav kimothiindexing p'},\n",
       " {'input_ids': tensor([    3,    32,   240,     3,     9,   320,    44, 18233,  3896,   522,\n",
       "           2691,    30, 14145,  8585,     7,   453,  4447,   918,     3,     9,\n",
       "             23,  1755,     9,   115,  2907,     9,   208,     3, 19754,    32,\n",
       "           7436, 18288,    53,     3,   102,     1]),\n",
       "  'input_text': 'o take a look at hugging face documents on tokenizers\\nkeep calm  build ai20abhinav kimothiindexing p',\n",
       "  'expected_output': 'ipeline document splitting\\nother tokenizer\\nother l'},\n",
       " {'input_ids': tensor([    3,   277,   453,  4447,   918,     3,     9,    23,  1755,     9,\n",
       "            115,  2907,     9,   208,     3, 19754,    32,  7436, 18288,    53,\n",
       "          12045,  1708, 28503,   119, 14145,  8585,   119,     3,    40,     1]),\n",
       "  'input_text': 'ers\\nkeep calm  build ai20abhinav kimothiindexing pipeline document splitting\\nother tokenizer\\nother l',\n",
       "  'expected_output': 'ibraries like spacy nltk and sentencetransformers '},\n",
       " {'input_ids': tensor([    3,    23,   855,   747,  1708, 28503,   119, 14145,  8585,   119,\n",
       "          12256,   114,     3,     7,  5379,    63,     3,    29,    40,    17,\n",
       "            157,    11,  7142,  7031,  2032,   277,     1]),\n",
       "  'input_text': 'ipeline document splitting\\nother tokenizer\\nother libraries like spacy nltk and sentencetransformers ',\n",
       "  'expected_output': 'also provide splitters\\nspecialized chunking\\nchunki'},\n",
       " {'input_ids': tensor([    3,    23,  1939,  2593,   114,     3,     7,  5379,    63,     3,\n",
       "             29,    40,    17,   157,    11,  7142,  7031,  2032,   277,    92,\n",
       "            370,  5679,  4849,     3,  8689, 16749,    53, 16749,    23,     1]),\n",
       "  'input_text': 'ibraries like spacy nltk and sentencetransformers also provide splitters\\nspecialized chunking\\nchunki',\n",
       "  'expected_output': 'ng often aims to keep text with common context tog'},\n",
       " {'input_ids': tensor([   92,   370,  5679,  4849,     3,  8689, 16749,    53, 16749,    53,\n",
       "            557,     3,  8345,    12,   453,  1499,    28,  1017,  2625,    12,\n",
       "            122,     1]),\n",
       "  'input_text': 'also provide splitters\\nspecialized chunking\\nchunking often aims to keep text with common context tog',\n",
       "  'expected_output': 'ether with this in\\nmind we might want to specifica'},\n",
       " {'input_ids': tensor([   3, 1725,  557,    3, 8345,   12,  453, 1499,   28, 1017, 2625,  544,\n",
       "            28,   48,   16,  809,   62,  429,  241,   12,  806,    9,    1]),\n",
       "  'input_text': 'ng often aims to keep text with common context together with this in\\nmind we might want to specifica',\n",
       "  'expected_output': 'lly honour the structure of the document itself\\nfo'},\n",
       " {'input_ids': tensor([    3, 16764,    28,    48,    16,   809,    62,   429,   241,    12,\n",
       "           3346, 14950,     8,  1809,    13,     8,  1708,  1402,  5575,     1]),\n",
       "  'input_text': 'ether with this in\\nmind we might want to specifically honour the structure of the document itself\\nfo',\n",
       "  'expected_output': 'r example html markdown latex or even code\\nexample'},\n",
       " {'input_ids': tensor([    3,  6073, 14950,     8,  1809,    13,     8,  1708,  1402,    21,\n",
       "            677,     3, 10500,  3946,  3035,  1480,   226,    42,   237,  1081,\n",
       "            677,     1]),\n",
       "  'input_text': 'lly honour the structure of the document itself\\nfor example html markdown latex or even code\\nexample',\n",
       "  'expected_output': '  httpsmediumcomp29a7e8610843\\nexample html  contex'},\n",
       " {'input_ids': tensor([    3,    52,   677,     3, 10500,  3946,  3035,  1480,   226,    42,\n",
       "            237,  1081,   677,  4893,  5700,   440,  7699,  3166,     9,   940,\n",
       "             15,  3840, 16169,  4906,   677,     3, 10500,  3622,   994,     1]),\n",
       "  'input_text': 'r example html markdown latex or even code\\nexample  httpsmediumcomp29a7e8610843\\nexample html  contex',\n",
       "  'expected_output': 't is key the significance of rag in language model'},\n",
       " {'input_ids': tensor([ 4893,  5700,   440,  7699,  3166,     9,   940,    15,  3840, 16169,\n",
       "           4906,   677,     3, 10500,  2625,    19,   843,     8, 11978,    13,\n",
       "              3,  6151,    16,  1612,   825,     1]),\n",
       "  'input_text': '  httpsmediumcomp29a7e8610843\\nexample html  context is key the significance of rag in language model',\n",
       "  'expected_output': 's\\na blog on medium  httpsmediumcomp29a7e8610843 \\nu'},\n",
       " {'input_ids': tensor([    3,    17,    19,   843,     8, 11978,    13,     3,  6151,    16,\n",
       "           1612,  2250,     3,     9,   875,    30,  2768,  4893,  5700,   440,\n",
       "           7699,  3166,     9,   940,    15,  3840, 16169,  4906,     3,    76,\n",
       "              1]),\n",
       "  'input_text': 't is key the significance of rag in language models\\na blog on medium  httpsmediumcomp29a7e8610843 \\nu',\n",
       "  'expected_output': 'sing langchains htmlheadertextsplitter  recursivec'},\n",
       " {'input_ids': tensor([    3,     7,     3,     9,   875,    30,  2768,  4893,  5700,   440,\n",
       "           7699,  3166,     9,   940,    15,  3840, 16169,  4906,   338, 12142,\n",
       "          19836,     7,     3, 10500,  3313,    49,  6327,     7,  5900,    17,\n",
       "            449,     3,    60, 15983,   757,    75,     1]),\n",
       "  'input_text': 's\\na blog on medium  httpsmediumcomp29a7e8610843 \\nusing langchains htmlheadertextsplitter  recursivec',\n",
       "  'expected_output': 'haractertextsplitter\\n all langchain splitters \\nkee'},\n",
       " {'input_ids': tensor([10159, 12142, 19836,     7,     3, 10500,  3313,    49,  6327,     7,\n",
       "           5900,    17,   449,     3,    60, 15983,   757, 31886,  6327,     7,\n",
       "           5900,    17,   449,    66, 12142, 19836,  5679,  4849,     3,  1050,\n",
       "             15,     1]),\n",
       "  'input_text': 'sing langchains htmlheadertextsplitter  recursivecharactertextsplitter\\n all langchain splitters \\nkee',\n",
       "  'expected_output': 'p calm  build ai21abhinav kimothiindexing pipeline'},\n",
       " {'input_ids': tensor([    3,  3272,  2708,    49,  6327,     7,  5900,    17,   449,    66,\n",
       "          12142, 19836,  5679,  4849,   453,  4447,   918,     3,     9,    23,\n",
       "           2658,     9,   115,  2907,     9,   208,     3, 19754,    32,  7436,\n",
       "          18288,    53, 12045,     1]),\n",
       "  'input_text': 'haractertextsplitter\\n all langchain splitters \\nkeep calm  build ai21abhinav kimothiindexing pipeline',\n",
       "  'expected_output': ' document splitting\\nthings to keep in mind\\nensure '},\n",
       " {'input_ids': tensor([    3,   102,  4447,   918,     3,     9,    23,  2658,     9,   115,\n",
       "           2907,     9,   208,     3, 19754,    32,  7436, 18288,    53, 12045,\n",
       "           1708, 28503,   378,    12,   453,    16,   809,   766,     1]),\n",
       "  'input_text': 'p calm  build ai21abhinav kimothiindexing pipeline document splitting\\nthings to keep in mind\\nensure ',\n",
       "  'expected_output': 'data quality by preprocessing it before determinin'},\n",
       " {'input_ids': tensor([ 1708, 28503,   378,    12,   453,    16,   809,   766,   331,   463,\n",
       "             57,   554, 15056,    53,    34,   274,     3, 16372,    77,     1]),\n",
       "  'input_text': ' document splitting\\nthings to keep in mind\\nensure data quality by preprocessing it before determinin',\n",
       "  'expected_output': 'g the optimal chunk\\nsize examples include removing'},\n",
       " {'input_ids': tensor([  331,   463,    57,   554, 15056,    53,    34,   274,     3, 11682,\n",
       "              8,  6624, 16749,   812,  4062,   560,     3,  8499,     1]),\n",
       "  'input_text': 'data quality by preprocessing it before determining the optimal chunk\\nsize examples include removing',\n",
       "  'expected_output': ' html tags or eliminating specific elements\\nthat c'},\n",
       " {'input_ids': tensor([    3,   122,     8,  6624, 16749,   812,  4062,   560,     3,  8499,\n",
       "              3, 10500, 12391,    42, 17323,   806,  2479,    24,     3,    75,\n",
       "              1]),\n",
       "  'input_text': 'g the optimal chunk\\nsize examples include removing html tags or eliminating specific elements\\nthat c',\n",
       "  'expected_output': 'ontribute noise particularly when data is sourced '},\n",
       " {'input_ids': tensor([    3, 10500, 12391,    42, 17323,   806,  2479,    24,  4139,  4661,\n",
       "           1989,   116,   331,    19,     3, 15551,     1]),\n",
       "  'input_text': ' html tags or eliminating specific elements\\nthat contribute noise particularly when data is sourced ',\n",
       "  'expected_output': 'from the web\\nconsider factors such as content natu'},\n",
       " {'input_ids': tensor([   30,  5135,    17,    15,  4661,  1989,   116,   331,    19,     3,\n",
       "          15551,    45,     8,   765,  1099,  2580,   224,    38,   738,     3,\n",
       "             29,   144,    76,     1]),\n",
       "  'input_text': 'ontribute noise particularly when data is sourced from the web\\nconsider factors such as content natu',\n",
       "  'expected_output': 're eg short messages or lengthy\\ndocuments embeddin'},\n",
       " {'input_ids': tensor([   45,     8,   765,  1099,  2580,   224,    38,   738,  1405,     3,\n",
       "             15,   122,   710,  4175,    42, 17574,  2691, 25078,  2644,     1]),\n",
       "  'input_text': 'from the web\\nconsider factors such as content nature eg short messages or lengthy\\ndocuments embeddin',\n",
       "  'expected_output': 'g model characteristics and capabilities like toke'},\n",
       " {'input_ids': tensor([    3,    60,     3,    15,   122,   710,  4175,    42, 17574,  2691,\n",
       "          25078,    26,    53,   825,  6803,    11,  5644,   114,    12,  1050,\n",
       "              1]),\n",
       "  'input_text': 're eg short messages or lengthy\\ndocuments embedding model characteristics and capabilities like toke',\n",
       "  'expected_output': 'n\\nlimits in choosing chunk sizes aim for a balance'},\n",
       " {'input_ids': tensor([    3,   122,   825,  6803,    11,  5644,   114, 14145,  6790,    16,\n",
       "           4622, 16749,  4342,  2674,    21,     3,     9,  2109,     1]),\n",
       "  'input_text': 'g model characteristics and capabilities like token\\nlimits in choosing chunk sizes aim for a balance',\n",
       "  'expected_output': ' between preserving context\\nand maintaining accura'},\n",
       " {'input_ids': tensor([    3,    29,  6790,    16,  4622, 16749,  4342,  2674,    21,     3,\n",
       "              9,  2109,   344,     3, 22140,  2625,    11,  6011,     3,  6004,\n",
       "           2414,     1]),\n",
       "  'input_text': 'n\\nlimits in choosing chunk sizes aim for a balance between preserving context\\nand maintaining accura',\n",
       "  'expected_output': 'cy \\ntest different chunk sizes create embeddings f'},\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "if os.path.exists('./google-bert_bert-base-cased'):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('./google-bert_bert-base-cased')\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "    model.save_pretrained('./google-bert_bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForSequenceClassification' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "modelQa = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "qa = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FUT_Novil\\AppData\\Local\\anaconda3\\envs\\hfl\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FUT_Novil\\.cache\\huggingface\\hub\\models--google-bert--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'RAG is a tuning technique that uses a computer to perform a precise tuning of'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe('What are the metrics use to measuer the performace of T5 + Picard Model') #[{'generated_text': 'metric'}]\n",
    "# pipe('What is RAG?') # [{'generated_text': 'racial agglomeration'}]\n",
    "pipe('what are the differences between RAG and Fine tuning?') #[{'generated_text': 'RAG is a tuning technique that uses a computer to perform a precise tuning of'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
