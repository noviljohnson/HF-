{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer #, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"   \n",
    "\n",
    "do_ocr = True\n",
    "if do_ocr:\n",
    "    llmsherpa_api_url = llmsherpa_api_url + \"&applyOcr=yes\"\n",
    "\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc_obj = pdf_reader.read_pdf(file_path)\n",
    "    text_data = ''\n",
    "    for chunk in doc_obj.chunks():\n",
    "        text_data += chunk.to_text()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer_flan = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "# model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# # Process PDF data from books and user manuals\n",
    "# pdf_data_path = \"./test_file\"\n",
    "# text_data = \"\"\n",
    "\n",
    "# for filename in os.listdir(pdf_data_path):\n",
    "#     if filename.endswith(\".pdf\"):\n",
    "#         with open(os.path.join(pdf_data_path, filename), \"rb\") as file:\n",
    "#             pdf_text = extract_text_from_pdf(file)\n",
    "#             text_data += pdf_text\n",
    "\n",
    "# # Tokenize the text data\n",
    "# tokenized_text = tokenizer_flan(text_data, return_tensors=\"pt\")\n",
    "\n",
    "# # Create a TextDataset from the tokenized text\n",
    "# dataset = TextDataset(tokenized_text, tokenizer=tokenizer_flan)\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./output\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# # Define Trainer for unsupervised fine-tuning\n",
    "# trainer = Trainer(\n",
    "#     model=model_flan,\n",
    "#     args=training_args,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer_flan),\n",
    "#     train_dataset=dataset,\n",
    "# )\n",
    "\n",
    "# # Perform unsupervised fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "# 1. Text Extraction\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_data_path = './test_file'\n",
    "text_data = ''\n",
    "for filename in os.listdir(pdf_data_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        text_data += extract_text_from_pdf(os.path.join(pdf_data_path, filename))\n",
    "\n",
    "# 2. Text Cleaning and Preprocessing\n",
    "text_data = text_data.lower().strip()\n",
    "text_data = ''.join(char for char in text_data if char.isalnum() or char.isspace())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 4. Tokenization and Masking\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base', mask_token='[MASK]')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Dataset Formatting\n",
    "dataset = []\n",
    "for i in range(0, len(text_data) - 100, 50):\n",
    "    input_text = text_data[i:i+100]\n",
    "    expected_output = text_data[i+100:i+150]\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    dataset.append({\n",
    "        'input_ids': input_ids.squeeze(),\n",
    "        'input_text': input_text,\n",
    "        'expected_output': expected_output\n",
    "    })\n",
    "\n",
    "\n",
    "# # 3. Dataset Formatting\n",
    "# dataset = []\n",
    "# for i in range(0, len(text_data) - 100, 50):\n",
    "#     input_text = text_data[i:i+100]\n",
    "#     expected_output = text_data[i+100:i+150]\n",
    "    \n",
    "#     # Tokenize the input text\n",
    "#     input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "#     dataset.append({\n",
    "#         'input_ids': input_ids.squeeze(),\n",
    "#         'labels': tokenizer.encode(expected_output, return_tensors='pt').squeeze()\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Load the Pre-Trained Model\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 6. Define the Fine-Tuning Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./flan_t5_fine_tuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1530 [14:40<?, ?it/s]\n",
      "                                                  \n",
      " 33%|███▎      | 500/1530 [02:20<04:45,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5671, 'grad_norm': 10.753814697265625, 'learning_rate': 3.366013071895425e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 900/1530 [04:09<02:56,  3.57it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Perform Unsupervised Fine-Tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_text': 'proceedings of the 2021 conference on empirical methods in natural language processing  pages 989599',\n",
       "  'expected_output': '01\\nnovember 711 2021 c\\r2021 association for comput'},\n",
       " {'input_text': 'thods in natural language processing  pages 98959901\\nnovember 711 2021 c\\r2021 association for comput',\n",
       "  'expected_output': 'ational linguistics9895picard \\nparsing incremental'},\n",
       " {'input_text': '01\\nnovember 711 2021 c\\r2021 association for computational linguistics9895picard \\nparsing incremental',\n",
       "  'expected_output': 'ly for constrained autoregressive decoding\\nfrom la'},\n",
       " {'input_text': 'ational linguistics9895picard \\nparsing incrementally for constrained autoregressive decoding\\nfrom la',\n",
       "  'expected_output': 'nguage models\\ntorsten scholak andnathan schucher a'},\n",
       " {'input_text': 'ly for constrained autoregressive decoding\\nfrom language models\\ntorsten scholak andnathan schucher a',\n",
       "  'expected_output': 'nddzmitry bahdanau\\nelementai a servicenow company\\n'},\n",
       " {'input_text': 'nguage models\\ntorsten scholak andnathan schucher anddzmitry bahdanau\\nelementai a servicenow company\\n',\n",
       "  'expected_output': 'torstenscholakdzmitrybahdanauservicenowcom\\nabstrac'},\n",
       " {'input_text': 'nddzmitry bahdanau\\nelementai a servicenow company\\ntorstenscholakdzmitrybahdanauservicenowcom\\nabstrac',\n",
       "  'expected_output': 't\\nlarge pretrained language models for textual\\ndat'},\n",
       " {'input_text': 'torstenscholakdzmitrybahdanauservicenowcom\\nabstract\\nlarge pretrained language models for textual\\ndat',\n",
       "  'expected_output': 'a have an unconstrained output space at\\neach decod'},\n",
       " {'input_text': 't\\nlarge pretrained language models for textual\\ndata have an unconstrained output space at\\neach decod',\n",
       "  'expected_output': 'ing step they can produce any of\\n10000s of subword'},\n",
       " {'input_text': 'a have an unconstrained output space at\\neach decoding step they can produce any of\\n10000s of subword',\n",
       "  'expected_output': ' tokens when ﬁnetuned\\nto target constrained formal'},\n",
       " {'input_text': 'ing step they can produce any of\\n10000s of subword tokens when ﬁnetuned\\nto target constrained formal',\n",
       "  'expected_output': ' languages like\\nsql these models often generate in'},\n",
       " {'input_text': ' tokens when ﬁnetuned\\nto target constrained formal languages like\\nsql these models often generate in',\n",
       "  'expected_output': 'valid code\\nrendering it unusable we propose p icar'},\n",
       " {'input_text': ' languages like\\nsql these models often generate invalid code\\nrendering it unusable we propose p icar',\n",
       "  'expected_output': 'd1\\na method for constraining autoregressive de\\ncod'},\n",
       " {'input_text': 'valid code\\nrendering it unusable we propose p icard1\\na method for constraining autoregressive de\\ncod',\n",
       "  'expected_output': 'ers of language models through incremen\\ntal parsin'},\n",
       " {'input_text': 'd1\\na method for constraining autoregressive de\\ncoders of language models through incremen\\ntal parsin',\n",
       "  'expected_output': 'g p icard helps to ﬁnd valid output\\nsequences by r'},\n",
       " {'input_text': 'ers of language models through incremen\\ntal parsing p icard helps to ﬁnd valid output\\nsequences by r',\n",
       "  'expected_output': 'ejecting inadmissible tokens at\\neach decoding step'},\n",
       " {'input_text': 'g p icard helps to ﬁnd valid output\\nsequences by rejecting inadmissible tokens at\\neach decoding step',\n",
       "  'expected_output': ' on the challenging spider\\nand cosql texttosql tra'},\n",
       " {'input_text': 'ejecting inadmissible tokens at\\neach decoding step on the challenging spider\\nand cosql texttosql tra',\n",
       "  'expected_output': 'nslation tasks we\\nshow that p icard transforms ﬁne'},\n",
       " {'input_text': ' on the challenging spider\\nand cosql texttosql translation tasks we\\nshow that p icard transforms ﬁne',\n",
       "  'expected_output': 'tuned t5\\nmodels with passable performance into sta'},\n",
       " {'input_text': 'nslation tasks we\\nshow that p icard transforms ﬁnetuned t5\\nmodels with passable performance into sta',\n",
       "  'expected_output': 'te\\noftheart solutions\\n1 introduction\\nwhile there h'},\n",
       " {'input_text': 'tuned t5\\nmodels with passable performance into state\\noftheart solutions\\n1 introduction\\nwhile there h',\n",
       "  'expected_output': 'ave been many successes in applying\\nlarge pretrain'},\n",
       " {'input_text': 'te\\noftheart solutions\\n1 introduction\\nwhile there have been many successes in applying\\nlarge pretrain',\n",
       "  'expected_output': 'ed language models to downstream\\ntasks our ability'},\n",
       " {'input_text': 'ave been many successes in applying\\nlarge pretrained language models to downstream\\ntasks our ability',\n",
       "  'expected_output': ' to control and constrain the out\\nput of these mod'},\n",
       " {'input_text': 'ed language models to downstream\\ntasks our ability to control and constrain the out\\nput of these mod',\n",
       "  'expected_output': 'els is still very limited many\\nenterprise applicat'},\n",
       " {'input_text': ' to control and constrain the out\\nput of these models is still very limited many\\nenterprise applicat',\n",
       "  'expected_output': 'ions are out of reach because\\nthey require a degre'},\n",
       " {'input_text': 'els is still very limited many\\nenterprise applications are out of reach because\\nthey require a degre',\n",
       "  'expected_output': 'e of rigour and exactitude that\\nlanguage models ar'},\n",
       " {'input_text': 'ions are out of reach because\\nthey require a degree of rigour and exactitude that\\nlanguage models ar',\n",
       "  'expected_output': 'e not able to deliver yet if the\\ntarget is a forma'},\n",
       " {'input_text': 'e of rigour and exactitude that\\nlanguage models are not able to deliver yet if the\\ntarget is a forma',\n",
       "  'expected_output': 'l language like sql then we would\\nlike the model t'},\n",
       " {'input_text': 'e not able to deliver yet if the\\ntarget is a formal language like sql then we would\\nlike the model t',\n",
       "  'expected_output': 'o adhere exactly and provably to the\\nsql speciﬁcat'},\n",
       " {'input_text': 'l language like sql then we would\\nlike the model to adhere exactly and provably to the\\nsql speciﬁcat',\n",
       "  'expected_output': 'ion with all its lexical grammatical\\nlogical and s'},\n",
       " {'input_text': 'o adhere exactly and provably to the\\nsql speciﬁcation with all its lexical grammatical\\nlogical and s',\n",
       "  'expected_output': 'emantical constraints unfortunately\\nwith pretraini'},\n",
       " {'input_text': 'ion with all its lexical grammatical\\nlogical and semantical constraints unfortunately\\nwith pretraini',\n",
       "  'expected_output': 'ng alone language models may not\\nsatisfy these cor'},\n",
       " {'input_text': 'emantical constraints unfortunately\\nwith pretraining alone language models may not\\nsatisfy these cor',\n",
       "  'expected_output': 'rectness requirements\\nfor texttosql translation th'},\n",
       " {'input_text': 'ng alone language models may not\\nsatisfy these correctness requirements\\nfor texttosql translation th',\n",
       "  'expected_output': 'e most widespread\\nsolution to constrained decoding'},\n",
       " {'input_text': 'rectness requirements\\nfor texttosql translation the most widespread\\nsolution to constrained decoding',\n",
       "  'expected_output': ' is to make invalid\\nsql unrepresentable for a whil'},\n",
       " {'input_text': 'e most widespread\\nsolution to constrained decoding is to make invalid\\nsql unrepresentable for a whil',\n",
       "  'expected_output': 'e now it has been\\npossible to restrict autoregress'},\n",
       " {'input_text': ' is to make invalid\\nsql unrepresentable for a while now it has been\\npossible to restrict autoregress',\n",
       "  'expected_output': 'ive decoding to only\\nthose token sequences that co'},\n",
       " {'input_text': 'e now it has been\\npossible to restrict autoregressive decoding to only\\nthose token sequences that co',\n",
       "  'expected_output': 'rrectly parse to sql\\nabstract syntax trees yin and'},\n",
       " {'input_text': 'ive decoding to only\\nthose token sequences that correctly parse to sql\\nabstract syntax trees yin and',\n",
       "  'expected_output': ' neubig 2018 lin\\net al 2019 wang et al 2020 more r'},\n",
       " {'input_text': 'rrectly parse to sql\\nabstract syntax trees yin and neubig 2018 lin\\net al 2019 wang et al 2020 more r',\n",
       "  'expected_output': 'ecently\\nsemiautoregressive improvements to this pa'},\n",
       " {'input_text': ' neubig 2018 lin\\net al 2019 wang et al 2020 more recently\\nsemiautoregressive improvements to this pa',\n",
       "  'expected_output': 'rsing\\n1the picard code is available at httpsgithub'},\n",
       " {'input_text': 'ecently\\nsemiautoregressive improvements to this parsing\\n1the picard code is available at httpsgithub',\n",
       "  'expected_output': '\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n05005506'},\n",
       " {'input_text': 'rsing\\n1the picard code is available at httpsgithub\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n05005506',\n",
       "  'expected_output': '0065070075\\nexact match accuracy\\nt5base\\n t5large\\n t'},\n",
       " {'input_text': '\\ncomelementaipicard \\n1 2 4 8 16\\nbeam size\\n050055060065070075\\nexact match accuracy\\nt5base\\n t5large\\n t',\n",
       "  'expected_output': '53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch '},\n",
       " {'input_text': '0065070075\\nexact match accuracy\\nt5base\\n t5large\\n t53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch ',\n",
       "  'expected_output': 'accuracy of the highest\\nscoring prediction as a fu'},\n",
       " {'input_text': '53b\\nnone\\n top2\\n top4\\n top8\\nfigure 1 exactsetmatch accuracy of the highest\\nscoring prediction as a fu',\n",
       "  'expected_output': 'nction of beam size on the spi\\nder texttosql devel'},\n",
       " {'input_text': 'accuracy of the highest\\nscoring prediction as a function of beam size on the spi\\nder texttosql devel',\n",
       "  'expected_output': 'opment set with p icard turned\\non token prediction'},\n",
       " {'input_text': 'nction of beam size on the spi\\nder texttosql development set with p icard turned\\non token prediction',\n",
       "  'expected_output': 's had to pass p icard checking at\\nevery decoding s'},\n",
       " {'input_text': 'opment set with p icard turned\\non token predictions had to pass p icard checking at\\nevery decoding s',\n",
       "  'expected_output': 'tep only the top2 4 and 8 token\\npredictions of eac'},\n",
       " {'input_text': 's had to pass p icard checking at\\nevery decoding step only the top2 4 and 8 token\\npredictions of eac',\n",
       "  'expected_output': 'h hypothesis were considered in the\\nbeam search wi'},\n",
       " {'input_text': 'tep only the top2 4 and 8 token\\npredictions of each hypothesis were considered in the\\nbeam search wi',\n",
       "  'expected_output': 'th p icard turned off none all token\\npredictions w'},\n",
       " {'input_text': 'h hypothesis were considered in the\\nbeam search with p icard turned off none all token\\npredictions w',\n",
       "  'expected_output': 'ere considered and none were checked\\nthe models t5'},\n",
       " {'input_text': 'th p icard turned off none all token\\npredictions were considered and none were checked\\nthe models t5',\n",
       "  'expected_output': 'base large and 3b did not have\\naccess to any datab'},\n",
       " {'input_text': 'ere considered and none were checked\\nthe models t5base large and 3b did not have\\naccess to any datab',\n",
       "  'expected_output': 'ase content only to the database\\nschemas\\nparadigm '},\n",
       " {'input_text': 'base large and 3b did not have\\naccess to any database content only to the database\\nschemas\\nparadigm ',\n",
       "  'expected_output': 'have been proposed rubin and berant\\n2021 however w'},\n",
       " {'input_text': 'ase content only to the database\\nschemas\\nparadigm have been proposed rubin and berant\\n2021 however w',\n",
       "  'expected_output': 'hile effective these approaches\\nhave in common tha'},\n",
       " {'input_text': 'have been proposed rubin and berant\\n2021 however while effective these approaches\\nhave in common tha',\n",
       "  'expected_output': 't they are achieved at the ex\\npense of using a cus'},\n",
       " {'input_text': 'hile effective these approaches\\nhave in common that they are achieved at the ex\\npense of using a cus',\n",
       "  'expected_output': 'tom vocabulary of special con\\ntrol tokens or a cus'},\n",
       " {'input_text': 't they are achieved at the ex\\npense of using a custom vocabulary of special con\\ntrol tokens or a cus',\n",
       "  'expected_output': 'tom model architecture or both\\nunfortunately this '},\n",
       " {'input_text': 'tom vocabulary of special con\\ntrol tokens or a custom model architecture or both\\nunfortunately this ',\n",
       "  'expected_output': 'makes them incompatible with\\ngeneric pretrained la'},\n",
       " {'input_text': 'tom model architecture or both\\nunfortunately this makes them incompatible with\\ngeneric pretrained la',\n",
       "  'expected_output': 'nguage model decoders a\\nless invasive and more com'},\n",
       " {'input_text': 'makes them incompatible with\\ngeneric pretrained language model decoders a\\nless invasive and more com',\n",
       "  'expected_output': 'patible approach is to\\nnot constrain the generatio'},\n",
       " {'input_text': 'nguage model decoders a\\nless invasive and more compatible approach is to\\nnot constrain the generatio',\n",
       "  'expected_output': 'n process but instead to\\nﬁlter ﬁnalized beam hypot'},\n",
       " {'input_text': 'patible approach is to\\nnot constrain the generation process but instead to\\nﬁlter ﬁnalized beam hypot',\n",
       "  'expected_output': 'heses by validity suhr\\net al 2020 lin et al 2020 y'},\n",
       " {'input_text': 'n process but instead to\\nﬁlter ﬁnalized beam hypotheses by validity suhr\\net al 2020 lin et al 2020 y',\n",
       "  'expected_output': 'et such ﬁltering is\\nat the expense of a very large'},\n",
       " {'input_text': 'heses by validity suhr\\net al 2020 lin et al 2020 yet such ﬁltering is\\nat the expense of a very large',\n",
       "  'expected_output': ' beam size\\nwe address the expenses of these approa'},\n",
       " {'input_text': 'et such ﬁltering is\\nat the expense of a very large beam size\\nwe address the expenses of these approa',\n",
       "  'expected_output': 'ches\\nwith a novel incremental parsing method for c'},\n",
       " {'input_text': ' beam size\\nwe address the expenses of these approaches\\nwith a novel incremental parsing method for c',\n",
       "  'expected_output': 'on\\nstrained decoding called picard  which stands\\nf'},\n",
       " {'input_text': 'ches\\nwith a novel incremental parsing method for con\\nstrained decoding called picard  which stands\\nf',\n",
       "  'expected_output': 'or parsing incrementally for constrained auto9896\\n'},\n",
       " {'input_text': 'on\\nstrained decoding called picard  which stands\\nfor parsing incrementally for constrained auto9896\\n',\n",
       "  'expected_output': 'figure 2 illustration of constrained beam search w'},\n",
       " {'input_text': 'or parsing incrementally for constrained auto9896\\nfigure 2 illustration of constrained beam search w',\n",
       "  'expected_output': 'ith\\nbeam size 2and p icard  each vertical column r'},\n",
       " {'input_text': 'figure 2 illustration of constrained beam search with\\nbeam size 2and p icard  each vertical column r',\n",
       "  'expected_output': 'epre\\nsents three token predictions for a hypothesi'},\n",
       " {'input_text': 'ith\\nbeam size 2and p icard  each vertical column repre\\nsents three token predictions for a hypothesi',\n",
       "  'expected_output': 's from top\\nto bottom in descending order by probab'},\n",
       " {'input_text': 'epre\\nsents three token predictions for a hypothesis from top\\nto bottom in descending order by probab',\n",
       "  'expected_output': 'ility in this\\nexample p icard is conﬁgured to only'},\n",
       " {'input_text': 's from top\\nto bottom in descending order by probability in this\\nexample p icard is conﬁgured to only',\n",
       "  'expected_output': ' check the top\\n2 highest ones the rest is automati'},\n",
       " {'input_text': 'ility in this\\nexample p icard is conﬁgured to only check the top\\n2 highest ones the rest is automati',\n",
       "  'expected_output': 'cally dismissed by\\nsetting their score to 1 tokens'},\n",
       " {'input_text': ' check the top\\n2 highest ones the rest is automatically dismissed by\\nsetting their score to 1 tokens',\n",
       "  'expected_output': ' rejected by p icard\\nred are also assigned a score'},\n",
       " {'input_text': 'cally dismissed by\\nsetting their score to 1 tokens rejected by p icard\\nred are also assigned a score',\n",
       "  'expected_output': ' of 1 accepted\\ntokens green  keep their original s'},\n",
       " {'input_text': ' rejected by p icard\\nred are also assigned a score of 1 accepted\\ntokens green  keep their original s',\n",
       "  'expected_output': 'core\\nregressive decoding picard is compatible with'},\n",
       " {'input_text': ' of 1 accepted\\ntokens green  keep their original score\\nregressive decoding picard is compatible with',\n",
       "  'expected_output': '\\nany existing autoregressive language model de\\ncod'},\n",
       " {'input_text': 'core\\nregressive decoding picard is compatible with\\nany existing autoregressive language model de\\ncod',\n",
       "  'expected_output': 'er and vocabularyincluding but not limited\\nto thos'},\n",
       " {'input_text': '\\nany existing autoregressive language model de\\ncoder and vocabularyincluding but not limited\\nto thos',\n",
       "  'expected_output': 'e of large pretrained transformersand it\\ndoes not '},\n",
       " {'input_text': 'er and vocabularyincluding but not limited\\nto those of large pretrained transformersand it\\ndoes not ',\n",
       "  'expected_output': 'require very large beam sizes picard is\\nentirely a'},\n",
       " {'input_text': 'e of large pretrained transformersand it\\ndoes not require very large beam sizes picard is\\nentirely a',\n",
       "  'expected_output': 'bsent from pretraining or ﬁnetuning of\\nthe model a'},\n",
       " {'input_text': 'require very large beam sizes picard is\\nentirely absent from pretraining or ﬁnetuning of\\nthe model a',\n",
       "  'expected_output': 'nd can be easily and optionally enabled\\nat inferen'},\n",
       " {'input_text': 'bsent from pretraining or ﬁnetuning of\\nthe model and can be easily and optionally enabled\\nat inferen',\n",
       "  'expected_output': 'ce time picard operates directly on the\\noutput of '},\n",
       " {'input_text': 'nd can be easily and optionally enabled\\nat inference time picard operates directly on the\\noutput of ',\n",
       "  'expected_output': 'the language model which in the case\\nof texttosql '},\n",
       " {'input_text': 'ce time picard operates directly on the\\noutput of the language model which in the case\\nof texttosql ',\n",
       "  'expected_output': 'translation is the readable surface\\nform of the sq'},\n",
       " {'input_text': 'the language model which in the case\\nof texttosql translation is the readable surface\\nform of the sq',\n",
       "  'expected_output': 'l code\\nin our experiments we ﬁnd that picard can\\ns'},\n",
       " {'input_text': 'translation is the readable surface\\nform of the sql code\\nin our experiments we ﬁnd that picard can\\ns',\n",
       "  'expected_output': 'igniﬁcantly improve the performance of a large\\npre'},\n",
       " {'input_text': 'l code\\nin our experiments we ﬁnd that picard can\\nsigniﬁcantly improve the performance of a large\\npre',\n",
       "  'expected_output': 'trained language model raffel et al 2020\\nafter it '},\n",
       " {'input_text': 'igniﬁcantly improve the performance of a large\\npretrained language model raffel et al 2020\\nafter it ',\n",
       "  'expected_output': 'is ﬁnetuned on the texttosql task on\\nthe spider te'},\n",
       " {'input_text': 'trained language model raffel et al 2020\\nafter it is ﬁnetuned on the texttosql task on\\nthe spider te',\n",
       "  'expected_output': 'xttosql dataset yu et al 2018 we\\nﬁnd that a t5base'},\n",
       " {'input_text': 'is ﬁnetuned on the texttosql task on\\nthe spider texttosql dataset yu et al 2018 we\\nﬁnd that a t5base',\n",
       "  'expected_output': ' model with picard can out\\nperform a t5large model'},\n",
       " {'input_text': 'xttosql dataset yu et al 2018 we\\nﬁnd that a t5base model with picard can out\\nperform a t5large model',\n",
       "  'expected_output': ' without it and likewise\\nfor a t5large and a t53b '},\n",
       " {'input_text': ' model with picard can out\\nperform a t5large model without it and likewise\\nfor a t5large and a t53b ',\n",
       "  'expected_output': 'model signiﬁcantly\\nwith the help of picard  a t53b'},\n",
       " {'input_text': ' without it and likewise\\nfor a t5large and a t53b model signiﬁcantly\\nwith the help of picard  a t53b',\n",
       "  'expected_output': ' model can be\\nraised to stateoftheart performance '},\n",
       " {'input_text': 'model signiﬁcantly\\nwith the help of picard  a t53b model can be\\nraised to stateoftheart performance ',\n",
       "  'expected_output': 'on the spider\\nand cosql datasets yu et al 2019\\n2 t'},\n",
       " {'input_text': ' model can be\\nraised to stateoftheart performance on the spider\\nand cosql datasets yu et al 2019\\n2 t',\n",
       "  'expected_output': 'he p icard method\\npicard warps model prediction sc'},\n",
       " {'input_text': 'on the spider\\nand cosql datasets yu et al 2019\\n2 the p icard method\\npicard warps model prediction sc',\n",
       "  'expected_output': 'ores and inte\\ngrates trivially with existing algor'},\n",
       " {'input_text': 'he p icard method\\npicard warps model prediction scores and inte\\ngrates trivially with existing algor',\n",
       "  'expected_output': 'ithms for greedy\\nand beam search used in autoregre'},\n",
       " {'input_text': 'ores and inte\\ngrates trivially with existing algorithms for greedy\\nand beam search used in autoregre',\n",
       "  'expected_output': 'ssive decoding\\nfrom language models its arguments '},\n",
       " {'input_text': 'ithms for greedy\\nand beam search used in autoregressive decoding\\nfrom language models its arguments ',\n",
       "  'expected_output': 'are the token\\nids of the current hypothesis and fo'},\n",
       " {'input_text': 'ssive decoding\\nfrom language models its arguments are the token\\nids of the current hypothesis and fo',\n",
       "  'expected_output': 'r each vocabu\\nlary token the logsoftmax scores pre'},\n",
       " {'input_text': 'are the token\\nids of the current hypothesis and for each vocabu\\nlary token the logsoftmax scores pre',\n",
       "  'expected_output': 'dicted by the\\nmodels language modeling head picard'},\n",
       " {'input_text': 'r each vocabu\\nlary token the logsoftmax scores predicted by the\\nmodels language modeling head picard',\n",
       "  'expected_output': ' also hasaccess to sql schema information in parti'},\n",
       " {'input_text': 'dicted by the\\nmodels language modeling head picard also hasaccess to sql schema information in parti',\n",
       "  'expected_output': 'cular\\ninformation about the names of tables and co'},\n",
       " {'input_text': ' also hasaccess to sql schema information in particular\\ninformation about the names of tables and co',\n",
       "  'expected_output': 'lumns\\nand about which column resides in which tabl'},\n",
       " {'input_text': 'cular\\ninformation about the names of tables and columns\\nand about which column resides in which tabl',\n",
       "  'expected_output': 'e\\nat each generation step picard ﬁrst restricts\\npr'},\n",
       " {'input_text': 'lumns\\nand about which column resides in which table\\nat each generation step picard ﬁrst restricts\\npr',\n",
       "  'expected_output': 'ediction to the top khighest probability tokens\\nan'},\n",
       " {'input_text': 'e\\nat each generation step picard ﬁrst restricts\\nprediction to the top khighest probability tokens\\nan',\n",
       "  'expected_output': 'd then assigns a score of 1 to those that fail\\npic'},\n",
       " {'input_text': 'ediction to the top khighest probability tokens\\nand then assigns a score of 1 to those that fail\\npic',\n",
       "  'expected_output': 'ard s numerous checks see figure 2 these\\nchecks ar'},\n",
       " {'input_text': 'd then assigns a score of 1 to those that fail\\npicard s numerous checks see figure 2 these\\nchecks ar',\n",
       "  'expected_output': 'e enabled by fast incremental parsing\\nosullivan an'},\n",
       " {'input_text': 'ard s numerous checks see figure 2 these\\nchecks are enabled by fast incremental parsing\\nosullivan an',\n",
       "  'expected_output': 'd gamari 2021 based on monadic\\ncombinators leijen '},\n",
       " {'input_text': 'e enabled by fast incremental parsing\\nosullivan and gamari 2021 based on monadic\\ncombinators leijen ',\n",
       "  'expected_output': 'and meijer 2001 there are\\nfour picard mode setting'},\n",
       " {'input_text': 'd gamari 2021 based on monadic\\ncombinators leijen and meijer 2001 there are\\nfour picard mode setting',\n",
       "  'expected_output': 's that control their com\\nprehensiveness off no che'},\n",
       " {'input_text': 'and meijer 2001 there are\\nfour picard mode settings that control their com\\nprehensiveness off no che',\n",
       "  'expected_output': 'cking lexing parsing\\nwithout guards and parsing wi'},\n",
       " {'input_text': 's that control their com\\nprehensiveness off no checking lexing parsing\\nwithout guards and parsing wi',\n",
       "  'expected_output': 'th guardsthe high\\nest mode a prediction that passe'},\n",
       " {'input_text': 'cking lexing parsing\\nwithout guards and parsing with guardsthe high\\nest mode a prediction that passe',\n",
       "  'expected_output': 's a higher mode\\nwill always pass a lower mode but '},\n",
       " {'input_text': 'th guardsthe high\\nest mode a prediction that passes a higher mode\\nwill always pass a lower mode but ',\n",
       "  'expected_output': 'not necessarily\\nvice versa\\n21 lexing\\nin lexing mod'},\n",
       " {'input_text': 's a higher mode\\nwill always pass a lower mode but not necessarily\\nvice versa\\n21 lexing\\nin lexing mod',\n",
       "  'expected_output': 'e picard checks the output on\\na lexical level only'},\n",
       " {'input_text': 'not necessarily\\nvice versa\\n21 lexing\\nin lexing mode picard checks the output on\\na lexical level only',\n",
       "  'expected_output': ' it attempts to convert the\\npartial detokenized mo'},\n",
       " {'input_text': 'e picard checks the output on\\na lexical level only it attempts to convert the\\npartial detokenized mo',\n",
       "  'expected_output': 'del output to a whitespace\\ndelimited sequence of i'},\n",
       " {'input_text': ' it attempts to convert the\\npartial detokenized model output to a whitespace\\ndelimited sequence of i',\n",
       "  'expected_output': 'ndividual sql keywords\\nlikeselect  punctuation lik'},\n",
       " {'input_text': 'del output to a whitespace\\ndelimited sequence of individual sql keywords\\nlikeselect  punctuation lik',\n",
       "  'expected_output': 'e  operators like\\nand literals like string and num'},\n",
       " {'input_text': 'ndividual sql keywords\\nlikeselect  punctuation like  operators like\\nand literals like string and num',\n",
       "  'expected_output': 'ber values in\\nsql conditions and identiﬁers like a'},\n",
       " {'input_text': 'e  operators like\\nand literals like string and number values in\\nsql conditions and identiﬁers like a',\n",
       "  'expected_output': 'liases tables\\nand columnswithout being sensitive t'},\n",
       " {'input_text': 'ber values in\\nsql conditions and identiﬁers like aliases tables\\nand columnswithout being sensitive t',\n",
       "  'expected_output': 'o the order\\nin which these lexical items appear by'},\n",
       " {'input_text': 'liases tables\\nand columnswithout being sensitive to the order\\nin which these lexical items appear by',\n",
       "  'expected_output': ' making it\\nsopicard can detect spelling errors in '},\n",
       " {'input_text': 'o the order\\nin which these lexical items appear by making it\\nsopicard can detect spelling errors in ',\n",
       "  'expected_output': 'keywords\\nor reject table and column names that are'},\n",
       " {'input_text': ' making it\\nsopicard can detect spelling errors in keywords\\nor reject table and column names that are',\n",
       "  'expected_output': ' invalid\\nfor the given sql schema for instance con'},\n",
       " {'input_text': 'keywords\\nor reject table and column names that are invalid\\nfor the given sql schema for instance con',\n",
       "  'expected_output': 'sider\\nthe question what are the email cell phone\\na'},\n",
       " {'input_text': ' invalid\\nfor the given sql schema for instance consider\\nthe question what are the email cell phone\\na',\n",
       "  'expected_output': 'nd home phone of each professional from\\nspiders de'},\n",
       " {'input_text': 'sider\\nthe question what are the email cell phone\\nand home phone of each professional from\\nspiders de',\n",
       "  'expected_output': 'velopment set on the dogkennels\\ndatabase our ﬁnetu'},\n",
       " {'input_text': 'nd home phone of each professional from\\nspiders development set on the dogkennels\\ndatabase our ﬁnetu',\n",
       "  'expected_output': 'ned t5large model predicts\\nselect emailaddress cel'},\n",
       " {'input_text': 'velopment set on the dogkennels\\ndatabase our ﬁnetuned t5large model predicts\\nselect emailaddress cel',\n",
       "  'expected_output': 'lphone\\nhomephone from professionals while\\nthe grou'},\n",
       " {'input_text': 'ned t5large model predicts\\nselect emailaddress cellphone\\nhomephone from professionals while\\nthe grou',\n",
       "  'expected_output': 'nd truth selects cellnumber instead of\\nthe invalid'},\n",
       " {'input_text': 'lphone\\nhomephone from professionals while\\nthe ground truth selects cellnumber instead of\\nthe invalid',\n",
       "  'expected_output': ' cellphone column this mistake is\\ncaught and avoid'},\n",
       " {'input_text': 'nd truth selects cellnumber instead of\\nthe invalid cellphone column this mistake is\\ncaught and avoid',\n",
       "  'expected_output': 'ed by p icard in lexing mode\\n22 parsing without gu'},\n",
       " {'input_text': ' cellphone column this mistake is\\ncaught and avoided by p icard in lexing mode\\n22 parsing without gu',\n",
       "  'expected_output': 'ards\\nin the lowest parsing mode above lexingreferr'},\n",
       " {'input_text': 'ed by p icard in lexing mode\\n22 parsing without guards\\nin the lowest parsing mode above lexingreferr',\n",
       "  'expected_output': 'ed\\nto as parsing without guards picard checks the\\n'},\n",
       " {'input_text': 'ards\\nin the lowest parsing mode above lexingreferred\\nto as parsing without guards picard checks the\\n',\n",
       "  'expected_output': 'output on a grammatical level picard attempts to\\np'},\n",
       " {'input_text': 'ed\\nto as parsing without guards picard checks the\\noutput on a grammatical level picard attempts to\\np',\n",
       "  'expected_output': 'arse the detokenized model output to a data struc\\n'},\n",
       " {'input_text': 'output on a grammatical level picard attempts to\\nparse the detokenized model output to a data struc\\n',\n",
       "  'expected_output': 'ture that represents the abstract syntax tree ast\\n'},\n",
       " {'input_text': 'arse the detokenized model output to a data struc\\nture that represents the abstract syntax tree ast\\n',\n",
       "  'expected_output': 'of the predicted sql query contrary to lexing\\nmode'},\n",
       " {'input_text': 'ture that represents the abstract syntax tree ast\\nof the predicted sql query contrary to lexing\\nmode',\n",
       "  'expected_output': ' the order in which keywords and clauses ap\\npear n'},\n",
       " {'input_text': 'of the predicted sql query contrary to lexing\\nmode the order in which keywords and clauses ap\\npear n',\n",
       "  'expected_output': 'ow matters picard can reject invalid query\\nstructu'},\n",
       " {'input_text': ' the order in which keywords and clauses ap\\npear now matters picard can reject invalid query\\nstructu',\n",
       "  'expected_output': 'res eg ﬁnd missing from clauses or incor\\nrect orde'},\n",
       " {'input_text': 'ow matters picard can reject invalid query\\nstructures eg ﬁnd missing from clauses or incor\\nrect orde',\n",
       "  'expected_output': 'rs of clauses and keywords it can also\\ndetect a ra'},\n",
       " {'input_text': 'res eg ﬁnd missing from clauses or incor\\nrect orders of clauses and keywords it can also\\ndetect a ra',\n",
       "  'expected_output': 'nge of issues with compositions of sql9897expressi'},\n",
       " {'input_text': 'rs of clauses and keywords it can also\\ndetect a range of issues with compositions of sql9897expressi',\n",
       "  'expected_output': 'ons number one if picard matches on\\natidcid patter'},\n",
       " {'input_text': 'nge of issues with compositions of sql9897expressions number one if picard matches on\\natidcid patter',\n",
       "  'expected_output': 'n but the table with the id tid\\ndoes not contain a'},\n",
       " {'input_text': 'ons number one if picard matches on\\natidcid pattern but the table with the id tid\\ndoes not contain a',\n",
       "  'expected_output': ' column with id cid then that\\nparse is rejected se'},\n",
       " {'input_text': 'n but the table with the id tid\\ndoes not contain a column with id cid then that\\nparse is rejected se',\n",
       "  'expected_output': 'condly if picard ﬁrst matches\\non an aliascid patte'},\n",
       " {'input_text': ' column with id cid then that\\nparse is rejected secondly if picard ﬁrst matches\\non an aliascid patte',\n",
       "  'expected_output': 'rn and then later matches\\non the tid as alias patt'},\n",
       " {'input_text': 'condly if picard ﬁrst matches\\non an aliascid pattern and then later matches\\non the tid as alias patt',\n",
       "  'expected_output': 'ern but tid does not\\ncontain cid then that parse i'},\n",
       " {'input_text': 'rn and then later matches\\non the tid as alias pattern but tid does not\\ncontain cid then that parse i',\n",
       "  'expected_output': 's also rejected an\\nequivalent rule also exists for'},\n",
       " {'input_text': 'ern but tid does not\\ncontain cid then that parse is also rejected an\\nequivalent rule also exists for',\n",
       "  'expected_output': ' subqueries bound to\\ntable aliases lastly picard p'},\n",
       " {'input_text': 's also rejected an\\nequivalent rule also exists for subqueries bound to\\ntable aliases lastly picard p',\n",
       "  'expected_output': 'rohibits duplicate\\nbinding of a table alias in the'},\n",
       " {'input_text': ' subqueries bound to\\ntable aliases lastly picard prohibits duplicate\\nbinding of a table alias in the',\n",
       "  'expected_output': ' same select scope\\nbut permits shadowing of aliase'},\n",
       " {'input_text': 'rohibits duplicate\\nbinding of a table alias in the same select scope\\nbut permits shadowing of aliase',\n",
       "  'expected_output': 's deﬁned in a sur\\nrounding scope this can happen i'},\n",
       " {'input_text': ' same select scope\\nbut permits shadowing of aliases deﬁned in a sur\\nrounding scope this can happen i',\n",
       "  'expected_output': 'n nested sql\\nqueries\\n23 parsing with guards\\nin its'},\n",
       " {'input_text': 's deﬁned in a sur\\nrounding scope this can happen in nested sql\\nqueries\\n23 parsing with guards\\nin its',\n",
       "  'expected_output': ' highest parsing mode picard engages\\nin additional'},\n",
       " {'input_text': 'n nested sql\\nqueries\\n23 parsing with guards\\nin its highest parsing mode picard engages\\nin additional',\n",
       "  'expected_output': ' analysescalled guardswhile as\\nsembling the sql as'},\n",
       " {'input_text': ' highest parsing mode picard engages\\nin additional analysescalled guardswhile as\\nsembling the sql as',\n",
       "  'expected_output': 't if picard matches on\\ntidcid oraliascid  then gua'},\n",
       " {'input_text': ' analysescalled guardswhile as\\nsembling the sql ast if picard matches on\\ntidcid oraliascid  then gua',\n",
       "  'expected_output': 'rds require\\nthat the table tid or the alias alias '},\n",
       " {'input_text': 't if picard matches on\\ntidcid oraliascid  then guards require\\nthat the table tid or the alias alias ',\n",
       "  'expected_output': ' respectively\\nis eventually brought into scope by '},\n",
       " {'input_text': 'rds require\\nthat the table tid or the alias alias  respectively\\nis eventually brought into scope by ',\n",
       "  'expected_output': 'adding it to the\\nfrom clause moreover the alias al'},\n",
       " {'input_text': ' respectively\\nis eventually brought into scope by adding it to the\\nfrom clause moreover the alias al',\n",
       "  'expected_output': 'ias is con\\nstrained to resolve to a table or a sub'},\n",
       " {'input_text': 'adding it to the\\nfrom clause moreover the alias alias is con\\nstrained to resolve to a table or a sub',\n",
       "  'expected_output': 'query that has\\nthe column cid in it if picard matc'},\n",
       " {'input_text': 'ias is con\\nstrained to resolve to a table or a subquery that has\\nthe column cid in it if picard matc',\n",
       "  'expected_output': 'hes on the\\npattern cid then another guard requires'},\n",
       " {'input_text': 'query that has\\nthe column cid in it if picard matches on the\\npattern cid then another guard requires',\n",
       "  'expected_output': ' that ex\\nactly one table is eventually brought int'},\n",
       " {'input_text': 'hes on the\\npattern cid then another guard requires that ex\\nactly one table is eventually brought int',\n",
       "  'expected_output': 'o scope that\\ncontains a column with that id these '},\n",
       " {'input_text': ' that ex\\nactly one table is eventually brought into scope that\\ncontains a column with that id these ',\n",
       "  'expected_output': 'guards are\\nenforced eagerly in order to fail fast '},\n",
       " {'input_text': 'o scope that\\ncontains a column with that id these guards are\\nenforced eagerly in order to fail fast ',\n",
       "  'expected_output': 'and to eject\\ninvalid hypotheses from the beam at t'},\n",
       " {'input_text': 'guards are\\nenforced eagerly in order to fail fast and to eject\\ninvalid hypotheses from the beam at t',\n",
       "  'expected_output': 'he earliest\\npossible time the ﬁrst time this is ha'},\n",
       " {'input_text': 'and to eject\\ninvalid hypotheses from the beam at the earliest\\npossible time the ﬁrst time this is ha',\n",
       "  'expected_output': 'ppening is\\nafter parsing the from clause\\nonly with'},\n",
       " {'input_text': 'he earliest\\npossible time the ﬁrst time this is happening is\\nafter parsing the from clause\\nonly with',\n",
       "  'expected_output': ' these guards picard is able to\\nreject a wrong pre'},\n",
       " {'input_text': 'ppening is\\nafter parsing the from clause\\nonly with these guards picard is able to\\nreject a wrong pre',\n",
       "  'expected_output': 'diction from our ﬁnetuned\\nt5large model like selec'},\n",
       " {'input_text': ' these guards picard is able to\\nreject a wrong prediction from our ﬁnetuned\\nt5large model like selec',\n",
       "  'expected_output': 't maker model\\nfrom carmakers for the question what'},\n",
       " {'input_text': 'diction from our ﬁnetuned\\nt5large model like select maker model\\nfrom carmakers for the question what',\n",
       "  'expected_output': ' are\\nthe makers and models here the correct table '},\n",
       " {'input_text': 't maker model\\nfrom carmakers for the question what are\\nthe makers and models here the correct table ',\n",
       "  'expected_output': 'to\\nuse would have been modellist  since it is the\\n'},\n",
       " {'input_text': ' are\\nthe makers and models here the correct table to\\nuse would have been modellist  since it is the\\n',\n",
       "  'expected_output': 'only one in spiders car1 schema that contains\\nboth'},\n",
       " {'input_text': 'to\\nuse would have been modellist  since it is the\\nonly one in spiders car1 schema that contains\\nboth',\n",
       "  'expected_output': ' a maker and a model column\\nadditional checks and '},\n",
       " {'input_text': 'only one in spiders car1 schema that contains\\nboth a maker and a model column\\nadditional checks and ',\n",
       "  'expected_output': 'guards are conceivable\\nfor instance checking that '},\n",
       " {'input_text': ' a maker and a model column\\nadditional checks and guards are conceivable\\nfor instance checking that ',\n",
       "  'expected_output': 'only expressions of\\nthe same type are compared or '},\n",
       " {'input_text': 'guards are conceivable\\nfor instance checking that only expressions of\\nthe same type are compared or ',\n",
       "  'expected_output': 'that column types\\nselected by union except  orinte'},\n",
       " {'input_text': 'only expressions of\\nthe same type are compared or that column types\\nselected by union except  orinte',\n",
       "  'expected_output': 'rsect\\nqueries match we leave these additional chec'},\n",
       " {'input_text': 'that column types\\nselected by union except  orintersect\\nqueries match we leave these additional chec',\n",
       "  'expected_output': 'ks to\\nfuture work\\n3 experiments\\nour experiments ar'},\n",
       " {'input_text': 'rsect\\nqueries match we leave these additional checks to\\nfuture work\\n3 experiments\\nour experiments ar',\n",
       "  'expected_output': 'e mainly focused on spider\\nyu et al 2018 a large m'},\n",
       " {'input_text': 'ks to\\nfuture work\\n3 experiments\\nour experiments are mainly focused on spider\\nyu et al 2018 a large m',\n",
       "  'expected_output': 'ultidomain and cross\\ndatabase dataset for texttosq'},\n",
       " {'input_text': 'e mainly focused on spider\\nyu et al 2018 a large multidomain and cross\\ndatabase dataset for texttosq',\n",
       "  'expected_output': 'l parsing we train\\non the 7000 examples in the spi'},\n",
       " {'input_text': 'ultidomain and cross\\ndatabase dataset for texttosql parsing we train\\non the 7000 examples in the spi',\n",
       "  'expected_output': 'der training set andevaluate on spiders developmen'},\n",
       " {'input_text': 'l parsing we train\\non the 7000 examples in the spider training set andevaluate on spiders developmen',\n",
       "  'expected_output': 't set and its hid\\nden test set we also report resu'},\n",
       " {'input_text': 'der training set andevaluate on spiders development set and its hid\\nden test set we also report resu',\n",
       "  'expected_output': 'lts on the cosql\\nsqlgrounded dialog state tracking'},\n",
       " {'input_text': 't set and its hid\\nden test set we also report results on the cosql\\nsqlgrounded dialog state tracking',\n",
       "  'expected_output': ' task yu et al\\n2019 where we predict a sql query f'},\n",
       " {'input_text': 'lts on the cosql\\nsqlgrounded dialog state tracking task yu et al\\n2019 where we predict a sql query f',\n",
       "  'expected_output': 'or each\\nquestion given previous questions in an in'},\n",
       " {'input_text': ' task yu et al\\n2019 where we predict a sql query for each\\nquestion given previous questions in an in',\n",
       "  'expected_output': 'teraction\\ncontext for this task we train on both t'},\n",
       " {'input_text': 'or each\\nquestion given previous questions in an interaction\\ncontext for this task we train on both t',\n",
       "  'expected_output': 'he spider\\ntexttosql training data and the cosql di'},\n",
       " {'input_text': 'teraction\\ncontext for this task we train on both the spider\\ntexttosql training data and the cosql di',\n",
       "  'expected_output': 'alog\\nstate tracking training data and evaluate on '},\n",
       " {'input_text': 'he spider\\ntexttosql training data and the cosql dialog\\nstate tracking training data and evaluate on ',\n",
       "  'expected_output': 'the\\ncosql development and test sets\\nspider and cos'},\n",
       " {'input_text': 'alog\\nstate tracking training data and evaluate on the\\ncosql development and test sets\\nspider and cos',\n",
       "  'expected_output': 'ql are both zeroshot settings\\nthere is no overlap '},\n",
       " {'input_text': 'the\\ncosql development and test sets\\nspider and cosql are both zeroshot settings\\nthere is no overlap ',\n",
       "  'expected_output': 'between questions or databases\\nbetween the respect'},\n",
       " {'input_text': 'ql are both zeroshot settings\\nthere is no overlap between questions or databases\\nbetween the respect',\n",
       "  'expected_output': 'ive training development and\\ntest sets\\non spider w'},\n",
       " {'input_text': 'between questions or databases\\nbetween the respective training development and\\ntest sets\\non spider w',\n",
       "  'expected_output': 'e determine model performance\\nbased on three metri'},\n",
       " {'input_text': 'ive training development and\\ntest sets\\non spider we determine model performance\\nbased on three metri',\n",
       "  'expected_output': 'cs exactsetmatch accuracy\\nexecution accuracy and t'},\n",
       " {'input_text': 'e determine model performance\\nbased on three metrics exactsetmatch accuracy\\nexecution accuracy and t',\n",
       "  'expected_output': 'estsuite execution accu\\nracy zhong et al 2020 exac'},\n",
       " {'input_text': 'cs exactsetmatch accuracy\\nexecution accuracy and testsuite execution accu\\nracy zhong et al 2020 exac',\n",
       "  'expected_output': 'tsetmatch accu\\nracy compares the predicted and the'},\n",
       " {'input_text': 'estsuite execution accu\\nracy zhong et al 2020 exactsetmatch accu\\nracy compares the predicted and the',\n",
       "  'expected_output': ' groundtruth\\nsql query by parsing both into a norm'},\n",
       " {'input_text': 'tsetmatch accu\\nracy compares the predicted and the groundtruth\\nsql query by parsing both into a norm',\n",
       "  'expected_output': 'alized data\\nstructure this comparison is not sensi'},\n",
       " {'input_text': ' groundtruth\\nsql query by parsing both into a normalized data\\nstructure this comparison is not sensi',\n",
       "  'expected_output': 'tive to lit\\neral query values and can decrease und'},\n",
       " {'input_text': 'alized data\\nstructure this comparison is not sensitive to lit\\neral query values and can decrease und',\n",
       "  'expected_output': 'er semantic\\npreserving sql query rewriting executi'},\n",
       " {'input_text': 'tive to lit\\neral query values and can decrease under semantic\\npreserving sql query rewriting executi',\n",
       "  'expected_output': 'on accu\\nracy compares the results of executing the'},\n",
       " {'input_text': 'er semantic\\npreserving sql query rewriting execution accu\\nracy compares the results of executing the',\n",
       "  'expected_output': ' predicted\\nand groundtruth sql queries on the data'},\n",
       " {'input_text': 'on accu\\nracy compares the results of executing the predicted\\nand groundtruth sql queries on the data',\n",
       "  'expected_output': 'base con\\ntents shipped with the spider dataset thi'},\n",
       " {'input_text': ' predicted\\nand groundtruth sql queries on the database con\\ntents shipped with the spider dataset thi',\n",
       "  'expected_output': 's metric is\\nsensitive to literal query values but '},\n",
       " {'input_text': 'base con\\ntents shipped with the spider dataset this metric is\\nsensitive to literal query values but ',\n",
       "  'expected_output': 'suffers from a\\nhigh false positive rate zhong et a'},\n",
       " {'input_text': 's metric is\\nsensitive to literal query values but suffers from a\\nhigh false positive rate zhong et a',\n",
       "  'expected_output': 'l 2020 lastly\\ntestsuite execution accuracy extends'},\n",
       " {'input_text': 'suffers from a\\nhigh false positive rate zhong et al 2020 lastly\\ntestsuite execution accuracy extends',\n",
       "  'expected_output': ' execution to\\nmultiple database instances per sql '},\n",
       " {'input_text': 'l 2020 lastly\\ntestsuite execution accuracy extends execution to\\nmultiple database instances per sql ',\n",
       "  'expected_output': 'schema the\\ncontents of these instances are optimiz'},\n",
       " {'input_text': ' execution to\\nmultiple database instances per sql schema the\\ncontents of these instances are optimiz',\n",
       "  'expected_output': 'ed to lower\\nthe number of false positives and to p'},\n",
       " {'input_text': 'schema the\\ncontents of these instances are optimized to lower\\nthe number of false positives and to p',\n",
       "  'expected_output': 'rovide the\\nbest approximation of semantic accuracy'},\n",
       " {'input_text': 'ed to lower\\nthe number of false positives and to provide the\\nbest approximation of semantic accuracy',\n",
       "  'expected_output': '\\non cosql we measure model performance in\\nterms of'},\n",
       " {'input_text': 'rovide the\\nbest approximation of semantic accuracy\\non cosql we measure model performance in\\nterms of',\n",
       "  'expected_output': ' the question match accuracy and the inter\\naction '},\n",
       " {'input_text': '\\non cosql we measure model performance in\\nterms of the question match accuracy and the inter\\naction ',\n",
       "  'expected_output': 'match accuracy both metrics are based on\\nexactsetm'},\n",
       " {'input_text': ' the question match accuracy and the inter\\naction match accuracy both metrics are based on\\nexactsetm',\n",
       "  'expected_output': 'atch accuracy interaction match accu\\nracy is the j'},\n",
       " {'input_text': 'match accuracy both metrics are based on\\nexactsetmatch accuracy interaction match accu\\nracy is the j',\n",
       "  'expected_output': 'oint accuracy over all questions in an\\ninteraction'},\n",
       " {'input_text': 'atch accuracy interaction match accu\\nracy is the joint accuracy over all questions in an\\ninteraction',\n",
       "  'expected_output': '\\nwe are encouraged by results by shaw et al\\n2021 w'},\n",
       " {'input_text': 'oint accuracy over all questions in an\\ninteraction\\nwe are encouraged by results by shaw et al\\n2021 w',\n",
       "  'expected_output': 'ho showed that a pretrained t5base\\nor t53b model c'},\n",
       " {'input_text': '\\nwe are encouraged by results by shaw et al\\n2021 who showed that a pretrained t5base\\nor t53b model c',\n",
       "  'expected_output': 'an not only learn the textto\\nsql task but also gen'},\n",
       " {'input_text': 'ho showed that a pretrained t5base\\nor t53b model can not only learn the textto\\nsql task but also gen',\n",
       "  'expected_output': 'eralize to unseen databases\\nand even that t53b can'},\n",
       " {'input_text': 'an not only learn the textto\\nsql task but also generalize to unseen databases\\nand even that t53b can',\n",
       "  'expected_output': ' be competitive with the\\nthenstateoftheart choi et'},\n",
       " {'input_text': 'eralize to unseen databases\\nand even that t53b can be competitive with the\\nthenstateoftheart choi et',\n",
       "  'expected_output': ' al 2021 wang et al\\n2020all without modiﬁcations t'},\n",
       " {'input_text': ' be competitive with the\\nthenstateoftheart choi et al 2021 wang et al\\n2020all without modiﬁcations t',\n",
       "  'expected_output': 'o the model we\\ntherefore use t5 as the baseline fo'},\n",
       " {'input_text': ' al 2021 wang et al\\n2020all without modiﬁcations to the model we\\ntherefore use t5 as the baseline fo',\n",
       "  'expected_output': 'r all our experi\\nments\\nin order to allow for gener'},\n",
       " {'input_text': 'o the model we\\ntherefore use t5 as the baseline for all our experi\\nments\\nin order to allow for gener',\n",
       "  'expected_output': 'alization to unseen\\ndatabases we encode the schema'},\n",
       " {'input_text': 'r all our experi\\nments\\nin order to allow for generalization to unseen\\ndatabases we encode the schema',\n",
       "  'expected_output': ' together with the\\nquestions we use the same seria'},\n",
       " {'input_text': 'alization to unseen\\ndatabases we encode the schema together with the\\nquestions we use the same seria',\n",
       "  'expected_output': 'lization scheme\\nused by shaw et al 2021 in experim'},\n",
       " {'input_text': ' together with the\\nquestions we use the same serialization scheme\\nused by shaw et al 2021 in experim',\n",
       "  'expected_output': 'ents using9898database content we detect and attac'},\n",
       " {'input_text': 'lization scheme\\nused by shaw et al 2021 in experiments using9898database content we detect and attac',\n",
       "  'expected_output': 'h the database\\nvalues to the column names in a fas'},\n",
       " {'input_text': 'ents using9898database content we detect and attach the database\\nvalues to the column names in a fas',\n",
       "  'expected_output': 'hion similar to\\nthe bridge model by lin et al 2020'},\n",
       " {'input_text': 'h the database\\nvalues to the column names in a fashion similar to\\nthe bridge model by lin et al 2020',\n",
       "  'expected_output': ' when\\nﬁnetuning for the cosql dialog state trackin'},\n",
       " {'input_text': 'hion similar to\\nthe bridge model by lin et al 2020 when\\nﬁnetuning for the cosql dialog state trackin',\n",
       "  'expected_output': 'g\\ntask we append the previous questions in the in\\n'},\n",
       " {'input_text': ' when\\nﬁnetuning for the cosql dialog state tracking\\ntask we append the previous questions in the in\\n',\n",
       "  'expected_output': 'teraction in reverse chronological order to the in'},\n",
       " {'input_text': 'g\\ntask we append the previous questions in the in\\nteraction in reverse chronological order to the in',\n",
       "  'expected_output': '\\nput inputs exceeding the 512token limit of t5 are'},\n",
       " {'input_text': 'teraction in reverse chronological order to the in\\nput inputs exceeding the 512token limit of t5 are',\n",
       "  'expected_output': '\\ntruncated the target is the sql from the spider\\na'},\n",
       " {'input_text': '\\nput inputs exceeding the 512token limit of t5 are\\ntruncated the target is the sql from the spider\\na',\n",
       "  'expected_output': 'ndor cosql training sets unmodiﬁed except for\\na co'},\n",
       " {'input_text': '\\ntruncated the target is the sql from the spider\\nandor cosql training sets unmodiﬁed except for\\na co',\n",
       "  'expected_output': 'nversion of keywords and identiﬁers to lower\\ncase '},\n",
       " {'input_text': 'ndor cosql training sets unmodiﬁed except for\\na conversion of keywords and identiﬁers to lower\\ncase ',\n",
       "  'expected_output': 'we ﬁnetune t5 for up to 3072 epochs using\\nadafacto'},\n",
       " {'input_text': 'nversion of keywords and identiﬁers to lower\\ncase we ﬁnetune t5 for up to 3072 epochs using\\nadafacto',\n",
       "  'expected_output': 'r shazeer and stern 2018 a batch size\\nof2048  and '},\n",
       " {'input_text': 'we ﬁnetune t5 for up to 3072 epochs using\\nadafactor shazeer and stern 2018 a batch size\\nof2048  and ',\n",
       "  'expected_output': 'a learning rate of 104\\nresults our ﬁndings on the '},\n",
       " {'input_text': 'r shazeer and stern 2018 a batch size\\nof2048  and a learning rate of 104\\nresults our ﬁndings on the ',\n",
       "  'expected_output': 'spider dataset are\\nsummarized in table 1 and figur'},\n",
       " {'input_text': 'a learning rate of 104\\nresults our ﬁndings on the spider dataset are\\nsummarized in table 1 and figur',\n",
       "  'expected_output': 'e 1 our repro\\nductions of shaw et al 2021s results'},\n",
       " {'input_text': 'spider dataset are\\nsummarized in table 1 and figure 1 our repro\\nductions of shaw et al 2021s results',\n",
       "  'expected_output': ' with t5\\ncannot compete with the current state of '},\n",
       " {'input_text': 'e 1 our repro\\nductions of shaw et al 2021s results with t5\\ncannot compete with the current state of ',\n",
       "  'expected_output': 'the art on\\nspider the issue is that these models p'},\n",
       " {'input_text': ' with t5\\ncannot compete with the current state of the art on\\nspider the issue is that these models p',\n",
       "  'expected_output': 'redict a\\nlot of invalid sql for instance 12 of the'},\n",
       " {'input_text': 'the art on\\nspider the issue is that these models predict a\\nlot of invalid sql for instance 12 of the',\n",
       "  'expected_output': ' sql\\nqueries generated by the t53b model on spider'},\n",
       " {'input_text': 'redict a\\nlot of invalid sql for instance 12 of the sql\\nqueries generated by the t53b model on spider',\n",
       "  'expected_output': 's\\ndevelopment set result in an execution error how'},\n",
       " {'input_text': ' sql\\nqueries generated by the t53b model on spiders\\ndevelopment set result in an execution error how',\n",
       "  'expected_output': '\\never when these same models are augmented with\\npi'},\n",
       " {'input_text': 's\\ndevelopment set result in an execution error how\\never when these same models are augmented with\\npi',\n",
       "  'expected_output': 'card  we ﬁnd substantial improvements first\\ninvali'},\n",
       " {'input_text': '\\never when these same models are augmented with\\npicard  we ﬁnd substantial improvements first\\ninvali',\n",
       "  'expected_output': 'd sql predictions become rare for t53b\\nwith picard'},\n",
       " {'input_text': 'card  we ﬁnd substantial improvements first\\ninvalid sql predictions become rare for t53b\\nwith picard',\n",
       "  'expected_output': '  only 2 of the predictions are un\\nusable in these'},\n",
       " {'input_text': 'd sql predictions become rare for t53b\\nwith picard  only 2 of the predictions are un\\nusable in these',\n",
       "  'expected_output': ' cases beam search exited without\\nﬁnding a valid s'},\n",
       " {'input_text': '  only 2 of the predictions are un\\nusable in these cases beam search exited without\\nﬁnding a valid s',\n",
       "  'expected_output': 'ql prediction second and most\\nsigniﬁcantly by usin'},\n",
       " {'input_text': ' cases beam search exited without\\nﬁnding a valid sql prediction second and most\\nsigniﬁcantly by usin',\n",
       "  'expected_output': 'g picard  the t53b model is\\nlifted to stateofthear'},\n",
       " {'input_text': 'ql prediction second and most\\nsigniﬁcantly by using picard  the t53b model is\\nlifted to stateofthear',\n",
       "  'expected_output': 't performance we measure\\nan exactsetmatch accuracy'},\n",
       " {'input_text': 'g picard  the t53b model is\\nlifted to stateoftheart performance we measure\\nan exactsetmatch accuracy',\n",
       "  'expected_output': ' of 755 on the devel\\nopment set and 719 on the tes'},\n",
       " {'input_text': 't performance we measure\\nan exactsetmatch accuracy of 755 on the devel\\nopment set and 719 on the tes',\n",
       "  'expected_output': 't set the execution\\naccuracy results are 793 and 7'},\n",
       " {'input_text': ' of 755 on the devel\\nopment set and 719 on the test set the execution\\naccuracy results are 793 and 7',\n",
       "  'expected_output': '51 respectively\\nthese numbers are on par or higher'},\n",
       " {'input_text': 't set the execution\\naccuracy results are 793 and 751 respectively\\nthese numbers are on par or higher',\n",
       "  'expected_output': ' than those of\\nthe closest competitor lgesql  elec'},\n",
       " {'input_text': '51 respectively\\nthese numbers are on par or higher than those of\\nthe closest competitor lgesql  elec',\n",
       "  'expected_output': 'tra\\ncao et al 2021 see table 1 furthermore we\\nachi'},\n",
       " {'input_text': ' than those of\\nthe closest competitor lgesql  electra\\ncao et al 2021 see table 1 furthermore we\\nachi',\n",
       "  'expected_output': 'eve a testsuite execution accuracy of 719\\non spide'},\n",
       " {'input_text': 'tra\\ncao et al 2021 see table 1 furthermore we\\nachieve a testsuite execution accuracy of 719\\non spide',\n",
       "  'expected_output': 'rs development set\\nour ﬁndings on the cosql dialog'},\n",
       " {'input_text': 'eve a testsuite execution accuracy of 719\\non spiders development set\\nour ﬁndings on the cosql dialog',\n",
       "  'expected_output': ' state tracking\\ndataset see table 2 are similar to'},\n",
       " {'input_text': 'rs development set\\nour ﬁndings on the cosql dialog state tracking\\ndataset see table 2 are similar to',\n",
       "  'expected_output': ' those for spider\\npicard signiﬁcantly improves the'},\n",
       " {'input_text': ' state tracking\\ndataset see table 2 are similar to those for spider\\npicard signiﬁcantly improves the',\n",
       "  'expected_output': ' performance\\nand our ﬁnetuned t53b model achieves '},\n",
       " {'input_text': ' those for spider\\npicard signiﬁcantly improves the performance\\nand our ﬁnetuned t53b model achieves ',\n",
       "  'expected_output': 'stateof\\ntheart performance\\npicard is not only impr'},\n",
       " {'input_text': ' performance\\nand our ﬁnetuned t53b model achieves stateof\\ntheart performance\\npicard is not only impr',\n",
       "  'expected_output': 'oving performance it\\nis also fast during evaluatio'},\n",
       " {'input_text': 'stateof\\ntheart performance\\npicard is not only improving performance it\\nis also fast during evaluatio',\n",
       "  'expected_output': 'n of the t53b model\\non spider the decoding speed w'},\n",
       " {'input_text': 'oving performance it\\nis also fast during evaluation of the t53b model\\non spider the decoding speed w',\n",
       "  'expected_output': 'ith beam size 4\\non an nvidia a100sxm440gb gpu was '},\n",
       " {'input_text': 'n of the t53b model\\non spider the decoding speed with beam size 4\\non an nvidia a100sxm440gb gpu was ',\n",
       "  'expected_output': 'on\\naverage 25 seconds per sample without picard\\nan'},\n",
       " {'input_text': 'ith beam size 4\\non an nvidia a100sxm440gb gpu was on\\naverage 25 seconds per sample without picard\\nan',\n",
       "  'expected_output': 'd 31 seconds per sample with p icard \\nbeam size fi'},\n",
       " {'input_text': 'on\\naverage 25 seconds per sample without picard\\nand 31 seconds per sample with p icard \\nbeam size fi',\n",
       "  'expected_output': 'gure 1 shows results on spider with\\nout and with p'},\n",
       " {'input_text': 'd 31 seconds per sample with p icard \\nbeam size figure 1 shows results on spider with\\nout and with p',\n",
       "  'expected_output': 'icard when parsing with guards\\n1 2 4 8\\nbeam size\\n0'},\n",
       " {'input_text': 'gure 1 shows results on spider with\\nout and with picard when parsing with guards\\n1 2 4 8\\nbeam size\\n0',\n",
       "  'expected_output': '60062064066068070\\nexact match accuracy\\nturned off\\n'},\n",
       " {'input_text': 'icard when parsing with guards\\n1 2 4 8\\nbeam size\\n060062064066068070\\nexact match accuracy\\nturned off\\n',\n",
       "  'expected_output': 'lexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁna'},\n",
       " {'input_text': '60062064066068070\\nexact match accuracy\\nturned off\\nlexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁna',\n",
       "  'expected_output': 'lizing\\nincremental\\nfigure 3 exactsetmatch accuracy'},\n",
       " {'input_text': 'lexing\\nparsing wo guards\\nparsing w guards\\nnone\\nﬁnalizing\\nincremental\\nfigure 3 exactsetmatch accuracy',\n",
       "  'expected_output': ' on the spider de\\nvelopment set as a function of b'},\n",
       " {'input_text': 'lizing\\nincremental\\nfigure 3 exactsetmatch accuracy on the spider de\\nvelopment set as a function of b',\n",
       "  'expected_output': 'eam size for top 4\\npicard on t5large schema only a'},\n",
       " {'input_text': ' on the spider de\\nvelopment set as a function of beam size for top 4\\npicard on t5large schema only a',\n",
       "  'expected_output': 'nd for different\\noperation modes turned off lexing'},\n",
       " {'input_text': 'eam size for top 4\\npicard on t5large schema only and for different\\noperation modes turned off lexing',\n",
       "  'expected_output': ' parsing without\\nguards and parsing with guards in'},\n",
       " {'input_text': 'nd for different\\noperation modes turned off lexing parsing without\\nguards and parsing with guards in',\n",
       "  'expected_output': ' each mode p i\\ncard is either used incrementally a'},\n",
       " {'input_text': ' parsing without\\nguards and parsing with guards in each mode p i\\ncard is either used incrementally a',\n",
       "  'expected_output': 't each step or only\\nwhen ﬁnalizing a hypothesis\\nfo'},\n",
       " {'input_text': ' each mode p i\\ncard is either used incrementally at each step or only\\nwhen ﬁnalizing a hypothesis\\nfo',\n",
       "  'expected_output': 'r different beam sizes and sizes of t5 for\\neach mo'},\n",
       " {'input_text': 't each step or only\\nwhen ﬁnalizing a hypothesis\\nfor different beam sizes and sizes of t5 for\\neach mo',\n",
       "  'expected_output': 'del size picard increases performance\\nwith increas'},\n",
       " {'input_text': 'r different beam sizes and sizes of t5 for\\neach model size picard increases performance\\nwith increas',\n",
       "  'expected_output': 'ing beam size these increases are\\nthe strongest fo'},\n",
       " {'input_text': 'del size picard increases performance\\nwith increasing beam size these increases are\\nthe strongest fo',\n",
       "  'expected_output': 'r the step from beam size 1to2\\nless pronounced fro'},\n",
       " {'input_text': 'ing beam size these increases are\\nthe strongest for the step from beam size 1to2\\nless pronounced fro',\n",
       "  'expected_output': 'm 2to4 and then saturating\\nfor beam sizes above 4 '},\n",
       " {'input_text': 'r the step from beam size 1to2\\nless pronounced from 2to4 and then saturating\\nfor beam sizes above 4 ',\n",
       "  'expected_output': 'even with greedy search\\nbeam size 1picard allows f'},\n",
       " {'input_text': 'm 2to4 and then saturating\\nfor beam sizes above 4 even with greedy search\\nbeam size 1picard allows f',\n",
       "  'expected_output': 'or some modest\\nimprovements note that without pica'},\n",
       " {'input_text': 'even with greedy search\\nbeam size 1picard allows for some modest\\nimprovements note that without pica',\n",
       "  'expected_output': 'rd  these\\nmodels do not beneﬁt from beam search th'},\n",
       " {'input_text': 'or some modest\\nimprovements note that without picard  these\\nmodels do not beneﬁt from beam search th',\n",
       "  'expected_output': 'e num\\nberk of highestprobability tokens that are p'},\n",
       " {'input_text': 'rd  these\\nmodels do not beneﬁt from beam search the num\\nberk of highestprobability tokens that are p',\n",
       "  'expected_output': 'ro\\ncessed by picard at each decoding step has a\\nmo'},\n",
       " {'input_text': 'e num\\nberk of highestprobability tokens that are pro\\ncessed by picard at each decoding step has a\\nmo',\n",
       "  'expected_output': 'dest to negligible impact on performance it is\\nthe'},\n",
       " {'input_text': 'ro\\ncessed by picard at each decoding step has a\\nmodest to negligible impact on performance it is\\nthe',\n",
       "  'expected_output': ' largest for t5base smaller for t5large and\\nalmost'},\n",
       " {'input_text': 'dest to negligible impact on performance it is\\nthe largest for t5base smaller for t5large and\\nalmost',\n",
       "  'expected_output': ' undetectable for t53b we do not study\\nthe case k '},\n",
       " {'input_text': ' largest for t5base smaller for t5large and\\nalmost undetectable for t53b we do not study\\nthe case k ',\n",
       "  'expected_output': '1 because it reduces the beam search\\nto constraine'},\n",
       " {'input_text': ' undetectable for t53b we do not study\\nthe case k 1 because it reduces the beam search\\nto constraine',\n",
       "  'expected_output': 'd greedy search\\nablations in figure 3 we have cond'},\n",
       " {'input_text': '1 because it reduces the beam search\\nto constrained greedy search\\nablations in figure 3 we have cond',\n",
       "  'expected_output': 'ensed our\\nablation analysis for picard  we show re'},\n",
       " {'input_text': 'd greedy search\\nablations in figure 3 we have condensed our\\nablation analysis for picard  we show re',\n",
       "  'expected_output': 'sults for\\nour t5large model in all four picard che'},\n",
       " {'input_text': 'ensed our\\nablation analysis for picard  we show results for\\nour t5large model in all four picard che',\n",
       "  'expected_output': 'cking\\nmodes and for four different beam sizes on t'},\n",
       " {'input_text': 'sults for\\nour t5large model in all four picard checking\\nmodes and for four different beam sizes on t',\n",
       "  'expected_output': 'he spi\\nder development set when checking increment'},\n",
       " {'input_text': 'cking\\nmodes and for four different beam sizes on the spi\\nder development set when checking increment',\n",
       "  'expected_output': 'ally\\nat each decoding step lexing shows a small im'},\n",
       " {'input_text': 'he spi\\nder development set when checking incrementally\\nat each decoding step lexing shows a small im',\n",
       "  'expected_output': '\\nprovement over the unconstrained t5 model the\\nres'},\n",
       " {'input_text': 'ally\\nat each decoding step lexing shows a small im\\nprovement over the unconstrained t5 model the\\nres',\n",
       "  'expected_output': 'ults without picard and with picard in lex\\ning mod'},\n",
       " {'input_text': '\\nprovement over the unconstrained t5 model the\\nresults without picard and with picard in lex\\ning mod',\n",
       "  'expected_output': 'e are largely independent of the beam size\\nthis is'},\n",
       " {'input_text': 'ults without picard and with picard in lex\\ning mode are largely independent of the beam size\\nthis is',\n",
       "  'expected_output': ' different when picard is switched into\\nthe more s'},\n",
       " {'input_text': 'e are largely independent of the beam size\\nthis is different when picard is switched into\\nthe more s',\n",
       "  'expected_output': 'ophisticated parsing modes both with\\nand without g'},\n",
       " {'input_text': ' different when picard is switched into\\nthe more sophisticated parsing modes both with\\nand without g',\n",
       "  'expected_output': 'uards improvements from picard\\nincrease rapidly fo'},\n",
       " {'input_text': 'ophisticated parsing modes both with\\nand without guards improvements from picard\\nincrease rapidly fo',\n",
       "  'expected_output': 'r increasing beam sizes where\\nparsing with guards '},\n",
       " {'input_text': 'uards improvements from picard\\nincrease rapidly for increasing beam sizes where\\nparsing with guards ',\n",
       "  'expected_output': 'clearly has a strong lead over9899development test'},\n",
       " {'input_text': 'r increasing beam sizes where\\nparsing with guards clearly has a strong lead over9899development test',\n",
       "  'expected_output': '\\nsystem em ex em ex\\nbridge v2  bert ensembleylin e'},\n",
       " {'input_text': 'clearly has a strong lead over9899development test\\nsystem em ex em ex\\nbridge v2  bert ensembleylin e',\n",
       "  'expected_output': 't al 2020 711 703 675 683\\nsmbop  g rappayrubin and'},\n",
       " {'input_text': '\\nsystem em ex em ex\\nbridge v2  bert ensembleylin et al 2020 711 703 675 683\\nsmbop  g rappayrubin and',\n",
       "  'expected_output': ' berant 2021 747 750 695 711\\nratsql  gapyshi et al'},\n",
       " {'input_text': 't al 2020 711 703 675 683\\nsmbop  g rappayrubin and berant 2021 747 750 695 711\\nratsql  gapyshi et al',\n",
       "  'expected_output': ' 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al '},\n",
       " {'input_text': ' berant 2021 747 750 695 711\\nratsql  gapyshi et al 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al ',\n",
       "  'expected_output': '2021 750  709 \\nlgesql  electraycao et al 2021 751 '},\n",
       " {'input_text': ' 2021 718  697 \\ndtfixup sqlsp  r obert ayxu et al 2021 750  709 \\nlgesql  electraycao et al 2021 751 ',\n",
       "  'expected_output': ' 720 \\nt5base shaw et al 2021 571   \\nt53b shaw et a'},\n",
       " {'input_text': '2021 750  709 \\nlgesql  electraycao et al 2021 751  720 \\nt5base shaw et al 2021 571   \\nt53b shaw et a',\n",
       "  'expected_output': 'l 2021 700   \\nt5base ours 572 579  \\nt5basep icard '},\n",
       " {'input_text': ' 720 \\nt5base shaw et al 2021 571   \\nt53b shaw et al 2021 700   \\nt5base ours 572 579  \\nt5basep icard ',\n",
       "  'expected_output': '658 684  \\nt5large 653 672  \\nt5largep icard 691 729'},\n",
       " {'input_text': 'l 2021 700   \\nt5base ours 572 579  \\nt5basep icard 658 684  \\nt5large 653 672  \\nt5largep icard 691 729',\n",
       "  'expected_output': '  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by'},\n",
       " {'input_text': '658 684  \\nt5large 653 672  \\nt5largep icard 691 729  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by',\n",
       "  'expected_output': '715 744 680 701\\nt53bp icardy755 793 719 751\\ntable '},\n",
       " {'input_text': '  \\nt53b ours 699 714  \\nt53bp icard 741 763  \\nt53by715 744 680 701\\nt53bp icardy755 793 719 751\\ntable ',\n",
       "  'expected_output': '1 our results bottom and relevant prior art top on'},\n",
       " {'input_text': '715 744 680 701\\nt53bp icardy755 793 719 751\\ntable 1 our results bottom and relevant prior art top on',\n",
       "  'expected_output': ' the spider texttosql task shown are the exactset\\n'},\n",
       " {'input_text': '1 our results bottom and relevant prior art top on the spider texttosql task shown are the exactset\\n',\n",
       "  'expected_output': 'match accuracy em and execution accuracy ex percen'},\n",
       " {'input_text': ' the spider texttosql task shown are the exactset\\nmatch accuracy em and execution accuracy ex percen',\n",
       "  'expected_output': 'tages on spiders development and test sets our res'},\n",
       " {'input_text': 'match accuracy em and execution accuracy ex percentages on spiders development and test sets our res',\n",
       "  'expected_output': 'ults\\nare for a beam of size 4 and p icard is parsi'},\n",
       " {'input_text': 'tages on spiders development and test sets our results\\nare for a beam of size 4 and p icard is parsi',\n",
       "  'expected_output': 'ng with guards for the top2 token predictions a da'},\n",
       " {'input_text': 'ults\\nare for a beam of size 4 and p icard is parsing with guards for the top2 token predictions a da',\n",
       "  'expected_output': 'gger  indicates\\nuse of database content otherwise '},\n",
       " {'input_text': 'ng with guards for the top2 token predictions a dagger  indicates\\nuse of database content otherwise ',\n",
       "  'expected_output': 'schema only\\ndevelopment test\\nsystem qm im qm im\\nra'},\n",
       " {'input_text': 'gger  indicates\\nuse of database content otherwise schema only\\ndevelopment test\\nsystem qm im qm im\\nra',\n",
       "  'expected_output': 'tsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538'},\n",
       " {'input_text': 'schema only\\ndevelopment test\\nsystem qm im qm im\\nratsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538',\n",
       "  'expected_output': ' 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 o'},\n",
       " {'input_text': 'tsql  sc oreyu et al 2021 521 220 516 212\\nt53b 538 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 o',\n",
       "  'expected_output': 'ur results bottom and relevant prior art top on th'},\n",
       " {'input_text': ' 218 514 217\\nt53bp icard 569 242 546 237\\ntable 2 our results bottom and relevant prior art top on th',\n",
       "  'expected_output': 'e cosql dialog state tracking task shown are the\\nq'},\n",
       " {'input_text': 'ur results bottom and relevant prior art top on the cosql dialog state tracking task shown are the\\nq',\n",
       "  'expected_output': 'uestion match accuracy qm and interaction match ac'},\n",
       " {'input_text': 'e cosql dialog state tracking task shown are the\\nquestion match accuracy qm and interaction match ac',\n",
       "  'expected_output': 'curacy im percentages on cosqls development and\\nte'},\n",
       " {'input_text': 'uestion match accuracy qm and interaction match accuracy im percentages on cosqls development and\\nte',\n",
       "  'expected_output': 'st sets our results are for a beam of size 4 and p'},\n",
       " {'input_text': 'curacy im percentages on cosqls development and\\ntest sets our results are for a beam of size 4 and p',\n",
       "  'expected_output': ' icard is parsing with guards for the top2 token p'},\n",
       " {'input_text': 'st sets our results are for a beam of size 4 and p icard is parsing with guards for the top2 token p',\n",
       "  'expected_output': 'redictions\\nparsing without them\\nin order to compar'},\n",
       " {'input_text': ' icard is parsing with guards for the top2 token predictions\\nparsing without them\\nin order to compar',\n",
       "  'expected_output': 'e picard with the ﬁltering\\nbyvalidity approach of '},\n",
       " {'input_text': 'redictions\\nparsing without them\\nin order to compare picard with the ﬁltering\\nbyvalidity approach of ',\n",
       "  'expected_output': 'suhr et al 2020 and lin\\net al 2020 we have studied'},\n",
       " {'input_text': 'e picard with the ﬁltering\\nbyvalidity approach of suhr et al 2020 and lin\\net al 2020 we have studied',\n",
       "  'expected_output': ' also what happens\\nwhen picard is only checking hy'},\n",
       " {'input_text': 'suhr et al 2020 and lin\\net al 2020 we have studied also what happens\\nwhen picard is only checking hy',\n",
       "  'expected_output': 'potheses when\\nthe model predicts their ﬁnalization'},\n",
       " {'input_text': ' also what happens\\nwhen picard is only checking hypotheses when\\nthe model predicts their ﬁnalization',\n",
       "  'expected_output': ' with the endof\\nsequence token2in this restrained '},\n",
       " {'input_text': 'potheses when\\nthe model predicts their ﬁnalization with the endof\\nsequence token2in this restrained ',\n",
       "  'expected_output': 'mode picard\\nis still effective but much less so co'},\n",
       " {'input_text': ' with the endof\\nsequence token2in this restrained mode picard\\nis still effective but much less so co',\n",
       "  'expected_output': 'mpared to\\nnormal incremental operation the gap bet'},\n",
       " {'input_text': 'mode picard\\nis still effective but much less so compared to\\nnormal incremental operation the gap bet',\n",
       "  'expected_output': 'ween\\nthese two modes of operation only begins to s'},\n",
       " {'input_text': 'mpared to\\nnormal incremental operation the gap between\\nthese two modes of operation only begins to s',\n",
       "  'expected_output': 'hrink\\nfor large beam sizes this is understandable '},\n",
       " {'input_text': 'ween\\nthese two modes of operation only begins to shrink\\nfor large beam sizes this is understandable ',\n",
       "  'expected_output': 'since\\nlin et al 2020 used beam sizes of at least 1'},\n",
       " {'input_text': 'hrink\\nfor large beam sizes this is understandable since\\nlin et al 2020 used beam sizes of at least 1',\n",
       "  'expected_output': '6\\nand up to 64to reach optimal results with ﬁlteri'},\n",
       " {'input_text': 'since\\nlin et al 2020 used beam sizes of at least 16\\nand up to 64to reach optimal results with ﬁlteri',\n",
       "  'expected_output': 'ng\\nwhile suhr et al 2020 used a beam of size 100\\n4'},\n",
       " {'input_text': '6\\nand up to 64to reach optimal results with ﬁltering\\nwhile suhr et al 2020 used a beam of size 100\\n4',\n",
       "  'expected_output': ' conclusion\\nwe propose and evaluate a new method p'},\n",
       " {'input_text': 'ng\\nwhile suhr et al 2020 used a beam of size 100\\n4 conclusion\\nwe propose and evaluate a new method p',\n",
       "  'expected_output': 'icard \\nfor simple and effective constrained decodi'},\n",
       " {'input_text': ' conclusion\\nwe propose and evaluate a new method picard \\nfor simple and effective constrained decodi',\n",
       "  'expected_output': 'ng with\\n2this is not exactly equivalent to ﬁlterin'},\n",
       " {'input_text': 'icard \\nfor simple and effective constrained decoding with\\n2this is not exactly equivalent to ﬁlterin',\n",
       "  'expected_output': 'g a completely\\nﬁnalized beam because the hypothese'},\n",
       " {'input_text': 'ng with\\n2this is not exactly equivalent to ﬁltering a completely\\nﬁnalized beam because the hypothese',\n",
       "  'expected_output': 's rejected by picard\\nnever enter it and never take'},\n",
       " {'input_text': 'g a completely\\nﬁnalized beam because the hypotheses rejected by picard\\nnever enter it and never take',\n",
       "  'expected_output': ' up any spacelarge pretrained language models on b'},\n",
       " {'input_text': 's rejected by picard\\nnever enter it and never take up any spacelarge pretrained language models on b',\n",
       "  'expected_output': 'oth the\\nspider crossdomain and crossdatabase textt'},\n",
       " {'input_text': ' up any spacelarge pretrained language models on both the\\nspider crossdomain and crossdatabase textt',\n",
       "  'expected_output': 'o\\nsql dataset and the cosql sqlgrounded dialog\\nsta'},\n",
       " {'input_text': 'oth the\\nspider crossdomain and crossdatabase textto\\nsql dataset and the cosql sqlgrounded dialog\\nsta',\n",
       "  'expected_output': 'te tracking dataset we ﬁnd that the picard de\\ncodi'},\n",
       " {'input_text': 'o\\nsql dataset and the cosql sqlgrounded dialog\\nstate tracking dataset we ﬁnd that the picard de\\ncodi',\n",
       "  'expected_output': 'ng method not only signiﬁcantly improves the\\nperfo'},\n",
       " {'input_text': 'te tracking dataset we ﬁnd that the picard de\\ncoding method not only signiﬁcantly improves the\\nperfo',\n",
       "  'expected_output': 'rmance of ﬁnetuned but otherwise unmodi\\nﬁed t5 mod'},\n",
       " {'input_text': 'ng method not only signiﬁcantly improves the\\nperformance of ﬁnetuned but otherwise unmodi\\nﬁed t5 mod',\n",
       "  'expected_output': 'els it also lifts a t53b model to state\\noftheart r'},\n",
       " {'input_text': 'rmance of ﬁnetuned but otherwise unmodi\\nﬁed t5 models it also lifts a t53b model to state\\noftheart r',\n",
       "  'expected_output': 'esults on the established exactmatch\\nand execution'},\n",
       " {'input_text': 'els it also lifts a t53b model to state\\noftheart results on the established exactmatch\\nand execution',\n",
       "  'expected_output': ' accuracy metrics\\nacknowledgements\\nwe thank lee za'},\n",
       " {'input_text': 'esults on the established exactmatch\\nand execution accuracy metrics\\nacknowledgements\\nwe thank lee za',\n",
       "  'expected_output': 'mparo for his contributions to the\\nexperiments on '},\n",
       " {'input_text': ' accuracy metrics\\nacknowledgements\\nwe thank lee zamparo for his contributions to the\\nexperiments on ',\n",
       "  'expected_output': 'the cosql dataset further we\\nwould like to thank p'},\n",
       " {'input_text': 'mparo for his contributions to the\\nexperiments on the cosql dataset further we\\nwould like to thank p',\n",
       "  'expected_output': 'ete shaw for his input on\\nthe reproduction of the '},\n",
       " {'input_text': 'the cosql dataset further we\\nwould like to thank pete shaw for his input on\\nthe reproduction of the ',\n",
       "  'expected_output': 't5 results on spider we\\nwould also like to extend '},\n",
       " {'input_text': 'ete shaw for his input on\\nthe reproduction of the t5 results on spider we\\nwould also like to extend ',\n",
       "  'expected_output': 'our gratitude to tao yu\\nand yusen zhang for their '},\n",
       " {'input_text': 't5 results on spider we\\nwould also like to extend our gratitude to tao yu\\nand yusen zhang for their ',\n",
       "  'expected_output': 'efforts in evaluating our\\nmodel on the test split '},\n",
       " {'input_text': 'our gratitude to tao yu\\nand yusen zhang for their efforts in evaluating our\\nmodel on the test split ',\n",
       "  'expected_output': 'of the spider and cosql\\ndatasets finally we thank '},\n",
       " {'input_text': 'efforts in evaluating our\\nmodel on the test split of the spider and cosql\\ndatasets finally we thank ',\n",
       "  'expected_output': 'our anonymous review\\ners for their time and valuab'},\n",
       " {'input_text': 'of the spider and cosql\\ndatasets finally we thank our anonymous review\\ners for their time and valuab',\n",
       "  'expected_output': 'le suggestions9900references\\nruisheng cao lu chen '},\n",
       " {'input_text': 'our anonymous review\\ners for their time and valuable suggestions9900references\\nruisheng cao lu chen ',\n",
       "  'expected_output': 'zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql'},\n",
       " {'input_text': 'le suggestions9900references\\nruisheng cao lu chen zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql',\n",
       "  'expected_output': ' line graph en\\nhanced texttosql model with mixed l'},\n",
       " {'input_text': 'zhi chen yanbin zhao\\nsu zhu and kai yu 2021 lgesql line graph en\\nhanced texttosql model with mixed l',\n",
       "  'expected_output': 'ocal and non\\nlocal relations in proceedings of the'},\n",
       " {'input_text': ' line graph en\\nhanced texttosql model with mixed local and non\\nlocal relations in proceedings of the',\n",
       "  'expected_output': ' 59th annual\\nmeeting of the association for comput'},\n",
       " {'input_text': 'ocal and non\\nlocal relations in proceedings of the 59th annual\\nmeeting of the association for comput',\n",
       "  'expected_output': 'ational lin\\nguistics and the 11th international jo'},\n",
       " {'input_text': ' 59th annual\\nmeeting of the association for computational lin\\nguistics and the 11th international jo',\n",
       "  'expected_output': 'int conference\\non natural language processing volu'},\n",
       " {'input_text': 'ational lin\\nguistics and the 11th international joint conference\\non natural language processing volu',\n",
       "  'expected_output': 'me 1 long\\npapers  pages 25412555 online associatio'},\n",
       " {'input_text': 'int conference\\non natural language processing volume 1 long\\npapers  pages 25412555 online associatio',\n",
       "  'expected_output': 'n for\\ncomputational linguistics\\ndonghyun choi myeo'},\n",
       " {'input_text': 'me 1 long\\npapers  pages 25412555 online association for\\ncomputational linguistics\\ndonghyun choi myeo',\n",
       "  'expected_output': 'ng cheol shin eunggyun kim\\nand dong ryeol shin 202'},\n",
       " {'input_text': 'n for\\ncomputational linguistics\\ndonghyun choi myeong cheol shin eunggyun kim\\nand dong ryeol shin 202',\n",
       "  'expected_output': '1 ryansql re\\ncursively applying sketchbased slot f'},\n",
       " {'input_text': 'ng cheol shin eunggyun kim\\nand dong ryeol shin 2021 ryansql re\\ncursively applying sketchbased slot f',\n",
       "  'expected_output': 'illings for\\ncomplex texttosql in crossdomain datab'},\n",
       " {'input_text': '1 ryansql re\\ncursively applying sketchbased slot fillings for\\ncomplex texttosql in crossdomain datab',\n",
       "  'expected_output': 'ases\\ncomputational linguistics  472309332\\ndaan lei'},\n",
       " {'input_text': 'illings for\\ncomplex texttosql in crossdomain databases\\ncomputational linguistics  472309332\\ndaan lei',\n",
       "  'expected_output': 'jen and erik meijer 2001 parsec direct\\nstyle monad'},\n",
       " {'input_text': 'ases\\ncomputational linguistics  472309332\\ndaan leijen and erik meijer 2001 parsec direct\\nstyle monad',\n",
       "  'expected_output': 'ic parser combinators for the real world\\ntechnical'},\n",
       " {'input_text': 'jen and erik meijer 2001 parsec direct\\nstyle monadic parser combinators for the real world\\ntechnical',\n",
       "  'expected_output': ' report uucs200127 user model\\ning 2007 11th intern'},\n",
       " {'input_text': 'ic parser combinators for the real world\\ntechnical report uucs200127 user model\\ning 2007 11th intern',\n",
       "  'expected_output': 'ational conference um 2007\\ncorfu greece june 2529 '},\n",
       " {'input_text': ' report uucs200127 user model\\ning 2007 11th international conference um 2007\\ncorfu greece june 2529 ',\n",
       "  'expected_output': '2007\\nkevin lin ben bogin mark neumann jonathan be\\n'},\n",
       " {'input_text': 'ational conference um 2007\\ncorfu greece june 2529 2007\\nkevin lin ben bogin mark neumann jonathan be\\n',\n",
       "  'expected_output': 'rant and matt gardner 2019 grammarbased neu\\nral te'},\n",
       " {'input_text': '2007\\nkevin lin ben bogin mark neumann jonathan be\\nrant and matt gardner 2019 grammarbased neu\\nral te',\n",
       "  'expected_output': 'xttosql generation\\nxi victoria lin richard socher '},\n",
       " {'input_text': 'rant and matt gardner 2019 grammarbased neu\\nral texttosql generation\\nxi victoria lin richard socher ',\n",
       "  'expected_output': 'and caiming xiong\\n2020 bridging textual and tabula'},\n",
       " {'input_text': 'xttosql generation\\nxi victoria lin richard socher and caiming xiong\\n2020 bridging textual and tabula',\n",
       "  'expected_output': 'r data for cross\\ndomain texttosql semantic parsing'},\n",
       " {'input_text': 'and caiming xiong\\n2020 bridging textual and tabular data for cross\\ndomain texttosql semantic parsing',\n",
       "  'expected_output': ' findings of the\\nassociation for computational lin'},\n",
       " {'input_text': 'r data for cross\\ndomain texttosql semantic parsing findings of the\\nassociation for computational lin',\n",
       "  'expected_output': 'guistics emnlp\\n2020 \\nbryan osullivan and ben gamar'},\n",
       " {'input_text': ' findings of the\\nassociation for computational linguistics emnlp\\n2020 \\nbryan osullivan and ben gamar',\n",
       "  'expected_output': 'i 2021 attopar\\nsec fast combinator parsing for byt'},\n",
       " {'input_text': 'guistics emnlp\\n2020 \\nbryan osullivan and ben gamari 2021 attopar\\nsec fast combinator parsing for byt',\n",
       "  'expected_output': 'estrings and text\\nsoftware available on the haskel'},\n",
       " {'input_text': 'i 2021 attopar\\nsec fast combinator parsing for bytestrings and text\\nsoftware available on the haskel',\n",
       "  'expected_output': 'l package reposi\\ntory\\ncolin raffel noam shazeer ad'},\n",
       " {'input_text': 'estrings and text\\nsoftware available on the haskell package reposi\\ntory\\ncolin raffel noam shazeer ad',\n",
       "  'expected_output': 'am roberts katherine\\nlee sharan narang michael mat'},\n",
       " {'input_text': 'l package reposi\\ntory\\ncolin raffel noam shazeer adam roberts katherine\\nlee sharan narang michael mat',\n",
       "  'expected_output': 'ena yanqi zhou\\nwei li and peter j liu 2020 explori'},\n",
       " {'input_text': 'am roberts katherine\\nlee sharan narang michael matena yanqi zhou\\nwei li and peter j liu 2020 explori',\n",
       "  'expected_output': 'ng the lim\\nits of transfer learning with a uniﬁed '},\n",
       " {'input_text': 'ena yanqi zhou\\nwei li and peter j liu 2020 exploring the lim\\nits of transfer learning with a uniﬁed ',\n",
       "  'expected_output': 'texttotext\\ntransformer journal of machine learning'},\n",
       " {'input_text': 'ng the lim\\nits of transfer learning with a uniﬁed texttotext\\ntransformer journal of machine learning',\n",
       "  'expected_output': ' research \\n21167\\nohad rubin and jonathan berant 20'},\n",
       " {'input_text': 'texttotext\\ntransformer journal of machine learning research \\n21167\\nohad rubin and jonathan berant 20',\n",
       "  'expected_output': '21 smbop\\nsemiautoregressive bottomup semantic pars'},\n",
       " {'input_text': ' research \\n21167\\nohad rubin and jonathan berant 2021 smbop\\nsemiautoregressive bottomup semantic pars',\n",
       "  'expected_output': 'ing in\\nproceedings of the 2021 conference of the n'},\n",
       " {'input_text': '21 smbop\\nsemiautoregressive bottomup semantic parsing in\\nproceedings of the 2021 conference of the n',\n",
       "  'expected_output': 'orth\\namerican chapter of the association for compu'},\n",
       " {'input_text': 'ing in\\nproceedings of the 2021 conference of the north\\namerican chapter of the association for compu',\n",
       "  'expected_output': 'ta\\ntional linguistics human language technologies '},\n",
       " {'input_text': 'orth\\namerican chapter of the association for computa\\ntional linguistics human language technologies ',\n",
       "  'expected_output': '\\npages 311324 online association for computa\\ntiona'},\n",
       " {'input_text': 'ta\\ntional linguistics human language technologies \\npages 311324 online association for computa\\ntiona',\n",
       "  'expected_output': 'l linguistics\\npeter shaw mingwei chang panupong pa'},\n",
       " {'input_text': '\\npages 311324 online association for computa\\ntional linguistics\\npeter shaw mingwei chang panupong pa',\n",
       "  'expected_output': 'supat and\\nkristina toutanova 2021 compositional ge'},\n",
       " {'input_text': 'l linguistics\\npeter shaw mingwei chang panupong pasupat and\\nkristina toutanova 2021 compositional ge',\n",
       "  'expected_output': 'neral\\nization and natural language variation can a'},\n",
       " {'input_text': 'supat and\\nkristina toutanova 2021 compositional general\\nization and natural language variation can a',\n",
       "  'expected_output': ' se\\nmantic parsing approach handle both in proceed'},\n",
       " {'input_text': 'neral\\nization and natural language variation can a se\\nmantic parsing approach handle both in proceed',\n",
       "  'expected_output': '\\nings of the 59th annual meeting of the associatio'},\n",
       " {'input_text': ' se\\nmantic parsing approach handle both in proceed\\nings of the 59th annual meeting of the associatio',\n",
       "  'expected_output': 'n\\nfor computational linguistics and the 11th inter'},\n",
       " {'input_text': '\\nings of the 59th annual meeting of the association\\nfor computational linguistics and the 11th inter',\n",
       "  'expected_output': 'na\\ntional joint conference on natural language pro'},\n",
       " {'input_text': 'n\\nfor computational linguistics and the 11th interna\\ntional joint conference on natural language pro',\n",
       "  'expected_output': '\\ncessing volume 1 long papers  pages 922938\\nonline'},\n",
       " {'input_text': 'na\\ntional joint conference on natural language pro\\ncessing volume 1 long papers  pages 922938\\nonline',\n",
       "  'expected_output': ' association for computational linguistics\\nnoam sh'},\n",
       " {'input_text': '\\ncessing volume 1 long papers  pages 922938\\nonline association for computational linguistics\\nnoam sh',\n",
       "  'expected_output': 'azeer and mitchell stern 2018 adafactor\\nadaptive l'},\n",
       " {'input_text': ' association for computational linguistics\\nnoam shazeer and mitchell stern 2018 adafactor\\nadaptive l',\n",
       "  'expected_output': 'earning rates with sublinear memory costininternat'},\n",
       " {'input_text': 'azeer and mitchell stern 2018 adafactor\\nadaptive learning rates with sublinear memory costininternat',\n",
       "  'expected_output': 'ional conference on machine learning \\npages 459646'},\n",
       " {'input_text': 'earning rates with sublinear memory costininternational conference on machine learning \\npages 459646',\n",
       "  'expected_output': '04 pmlr\\npeng shi patrick ng zhiguo wang henghui zh'},\n",
       " {'input_text': 'ional conference on machine learning \\npages 45964604 pmlr\\npeng shi patrick ng zhiguo wang henghui zh',\n",
       "  'expected_output': 'u\\nalexander hanbo li jun wang cicero nogueira\\ndos '},\n",
       " {'input_text': '04 pmlr\\npeng shi patrick ng zhiguo wang henghui zhu\\nalexander hanbo li jun wang cicero nogueira\\ndos ',\n",
       "  'expected_output': 'santos and bing xiang 2021 learning con\\ntextual re'},\n",
       " {'input_text': 'u\\nalexander hanbo li jun wang cicero nogueira\\ndos santos and bing xiang 2021 learning con\\ntextual re',\n",
       "  'expected_output': 'presentations for semantic parsing with\\ngeneration'},\n",
       " {'input_text': 'santos and bing xiang 2021 learning con\\ntextual representations for semantic parsing with\\ngeneration',\n",
       "  'expected_output': 'augmented pretraining in proceedings\\nof the aaai c'},\n",
       " {'input_text': 'presentations for semantic parsing with\\ngenerationaugmented pretraining in proceedings\\nof the aaai c',\n",
       "  'expected_output': 'onference on artiﬁcial intelligence \\nvolume 35 pag'},\n",
       " {'input_text': 'augmented pretraining in proceedings\\nof the aaai conference on artiﬁcial intelligence \\nvolume 35 pag',\n",
       "  'expected_output': 'es 1380613814\\nalane suhr mingwei chang peter shaw '},\n",
       " {'input_text': 'onference on artiﬁcial intelligence \\nvolume 35 pages 1380613814\\nalane suhr mingwei chang peter shaw ',\n",
       "  'expected_output': 'and ken\\nton lee 2020 exploring unexplored generali'},\n",
       " {'input_text': 'es 1380613814\\nalane suhr mingwei chang peter shaw and ken\\nton lee 2020 exploring unexplored generali',\n",
       "  'expected_output': 'zation\\nchallenges for crossdatabase semantic parsi'},\n",
       " {'input_text': 'and ken\\nton lee 2020 exploring unexplored generalization\\nchallenges for crossdatabase semantic parsi',\n",
       "  'expected_output': 'ng in\\nproceedings of the 58th annual meeting of th'},\n",
       " {'input_text': 'zation\\nchallenges for crossdatabase semantic parsing in\\nproceedings of the 58th annual meeting of th',\n",
       "  'expected_output': 'e asso\\nciation for computational linguistics  page'},\n",
       " {'input_text': 'ng in\\nproceedings of the 58th annual meeting of the asso\\nciation for computational linguistics  page',\n",
       "  'expected_output': 's 8372\\n8388 online association for computational l'},\n",
       " {'input_text': 'e asso\\nciation for computational linguistics  pages 8372\\n8388 online association for computational l',\n",
       "  'expected_output': 'in\\nguistics\\nbailin wang richard shin xiaodong liu '},\n",
       " {'input_text': 's 8372\\n8388 online association for computational lin\\nguistics\\nbailin wang richard shin xiaodong liu ',\n",
       "  'expected_output': 'oleksandr\\npolozov and matthew richardson 2020 rats'},\n",
       " {'input_text': 'in\\nguistics\\nbailin wang richard shin xiaodong liu oleksandr\\npolozov and matthew richardson 2020 rats',\n",
       "  'expected_output': 'ql\\nrelationaware schema encoding and linking for\\nt'},\n",
       " {'input_text': 'oleksandr\\npolozov and matthew richardson 2020 ratsql\\nrelationaware schema encoding and linking for\\nt',\n",
       "  'expected_output': 'exttosql parsers proceedings of the 58th annual\\nme'},\n",
       " {'input_text': 'ql\\nrelationaware schema encoding and linking for\\ntexttosql parsers proceedings of the 58th annual\\nme',\n",
       "  'expected_output': 'eting of the association for computational lin\\ngui'},\n",
       " {'input_text': 'exttosql parsers proceedings of the 58th annual\\nmeeting of the association for computational lin\\ngui',\n",
       "  'expected_output': 'stics \\npeng xu dhruv kumar wei yang wenjie zi keyi'},\n",
       " {'input_text': 'eting of the association for computational lin\\nguistics \\npeng xu dhruv kumar wei yang wenjie zi keyi',\n",
       "  'expected_output': '\\ntang chenyang huang jackie chi kit cheung si\\nmon '},\n",
       " {'input_text': 'stics \\npeng xu dhruv kumar wei yang wenjie zi keyi\\ntang chenyang huang jackie chi kit cheung si\\nmon ',\n",
       "  'expected_output': 'jd prince and yanshuai cao 2021 optimiz\\ning deeper'},\n",
       " {'input_text': '\\ntang chenyang huang jackie chi kit cheung si\\nmon jd prince and yanshuai cao 2021 optimiz\\ning deeper',\n",
       "  'expected_output': ' transformers on small datasets in pro\\nceedings of'},\n",
       " {'input_text': 'jd prince and yanshuai cao 2021 optimiz\\ning deeper transformers on small datasets in pro\\nceedings of',\n",
       "  'expected_output': ' the 59th annual meeting of the associa\\ntion for c'},\n",
       " {'input_text': ' transformers on small datasets in pro\\nceedings of the 59th annual meeting of the associa\\ntion for c',\n",
       "  'expected_output': 'omputational linguistics and the 11th in\\nternation'},\n",
       " {'input_text': ' the 59th annual meeting of the associa\\ntion for computational linguistics and the 11th in\\nternation',\n",
       "  'expected_output': 'al joint conference on natural language\\nprocessing'},\n",
       " {'input_text': 'omputational linguistics and the 11th in\\nternational joint conference on natural language\\nprocessing',\n",
       "  'expected_output': ' volume 1 long papers  pages 2089\\n2102 online asso'},\n",
       " {'input_text': 'al joint conference on natural language\\nprocessing volume 1 long papers  pages 2089\\n2102 online asso',\n",
       "  'expected_output': 'ciation for computational lin\\nguistics\\npengcheng y'},\n",
       " {'input_text': ' volume 1 long papers  pages 2089\\n2102 online association for computational lin\\nguistics\\npengcheng y',\n",
       "  'expected_output': 'in and graham neubig 2018 tranx a\\ntransitionbased '},\n",
       " {'input_text': 'ciation for computational lin\\nguistics\\npengcheng yin and graham neubig 2018 tranx a\\ntransitionbased ',\n",
       "  'expected_output': 'neural abstract syntax parser for se\\nmantic parsin'},\n",
       " {'input_text': 'in and graham neubig 2018 tranx a\\ntransitionbased neural abstract syntax parser for se\\nmantic parsin',\n",
       "  'expected_output': 'g and code generation proceedings of\\nthe 2018 conf'},\n",
       " {'input_text': 'neural abstract syntax parser for se\\nmantic parsing and code generation proceedings of\\nthe 2018 conf',\n",
       "  'expected_output': 'erence on empirical methods in natu\\nral language p'},\n",
       " {'input_text': 'g and code generation proceedings of\\nthe 2018 conference on empirical methods in natu\\nral language p',\n",
       "  'expected_output': 'rocessing system demonstrations \\ntao yu rui zhang '},\n",
       " {'input_text': 'erence on empirical methods in natu\\nral language processing system demonstrations \\ntao yu rui zhang ',\n",
       "  'expected_output': 'heyang er suyi li eric xue\\nbo pang xi victoria lin'},\n",
       " {'input_text': 'rocessing system demonstrations \\ntao yu rui zhang heyang er suyi li eric xue\\nbo pang xi victoria lin',\n",
       "  'expected_output': ' yi chern tan tianze\\nshi zihan li youxuan jiang mi'},\n",
       " {'input_text': 'heyang er suyi li eric xue\\nbo pang xi victoria lin yi chern tan tianze\\nshi zihan li youxuan jiang mi',\n",
       "  'expected_output': 'chihiro yasunaga\\nsungrok shim tao chen alexander f'},\n",
       " {'input_text': ' yi chern tan tianze\\nshi zihan li youxuan jiang michihiro yasunaga\\nsungrok shim tao chen alexander f',\n",
       "  'expected_output': 'abbri zifan\\nli luyao chen yuwen zhang shreya dixit'},\n",
       " {'input_text': 'chihiro yasunaga\\nsungrok shim tao chen alexander fabbri zifan\\nli luyao chen yuwen zhang shreya dixit',\n",
       "  'expected_output': ' vin\\ncent zhang caiming xiong richard socher wal\\nt'},\n",
       " {'input_text': 'abbri zifan\\nli luyao chen yuwen zhang shreya dixit vin\\ncent zhang caiming xiong richard socher wal\\nt',\n",
       "  'expected_output': 'er lasecki and dragomir radev 2019 cosql a\\nconvers'},\n",
       " {'input_text': ' vin\\ncent zhang caiming xiong richard socher wal\\nter lasecki and dragomir radev 2019 cosql a\\nconvers',\n",
       "  'expected_output': 'ational texttosql challenge towards cross\\ndomain n'},\n",
       " {'input_text': 'er lasecki and dragomir radev 2019 cosql a\\nconversational texttosql challenge towards cross\\ndomain n',\n",
       "  'expected_output': 'atural language interfaces to databases in\\nproceed'},\n",
       " {'input_text': 'ational texttosql challenge towards cross\\ndomain natural language interfaces to databases in\\nproceed',\n",
       "  'expected_output': 'ings of the 2019 conference on empirical\\nmethods i'},\n",
       " {'input_text': 'atural language interfaces to databases in\\nproceedings of the 2019 conference on empirical\\nmethods i',\n",
       "  'expected_output': 'n natural language processing and the\\n9th internat'},\n",
       " {'input_text': 'ings of the 2019 conference on empirical\\nmethods in natural language processing and the\\n9th internat',\n",
       "  'expected_output': 'ional joint conference on natural lan\\nguage proces'},\n",
       " {'input_text': 'n natural language processing and the\\n9th international joint conference on natural lan\\nguage proces',\n",
       "  'expected_output': 'sing emnlpijcnlp  pages 1962\\n1979 hong kong china '},\n",
       " {'input_text': 'ional joint conference on natural lan\\nguage processing emnlpijcnlp  pages 1962\\n1979 hong kong china ',\n",
       "  'expected_output': 'association for computa\\ntional linguistics\\ntao yu '},\n",
       " {'input_text': 'sing emnlpijcnlp  pages 1962\\n1979 hong kong china association for computa\\ntional linguistics\\ntao yu ',\n",
       "  'expected_output': 'rui zhang alex polozov christopher meek\\nand ahmed '},\n",
       " {'input_text': 'association for computa\\ntional linguistics\\ntao yu rui zhang alex polozov christopher meek\\nand ahmed ',\n",
       "  'expected_output': 'hassan awadallah 2021 score pre\\ntraining for conte'},\n",
       " {'input_text': 'rui zhang alex polozov christopher meek\\nand ahmed hassan awadallah 2021 score pre\\ntraining for conte',\n",
       "  'expected_output': 'xt representation in conversational\\nsemantic parsi'},\n",
       " {'input_text': 'hassan awadallah 2021 score pre\\ntraining for context representation in conversational\\nsemantic parsi',\n",
       "  'expected_output': 'ng in international conference on\\nlearning represe'},\n",
       " {'input_text': 'xt representation in conversational\\nsemantic parsing in international conference on\\nlearning represe',\n",
       "  'expected_output': 'ntations 9901tao yu rui zhang kai yang michihiro y'},\n",
       " {'input_text': 'ng in international conference on\\nlearning representations 9901tao yu rui zhang kai yang michihiro y',\n",
       "  'expected_output': 'asunaga\\ndongxu wang zifan li james ma irene li qin'},\n",
       " {'input_text': 'ntations 9901tao yu rui zhang kai yang michihiro yasunaga\\ndongxu wang zifan li james ma irene li qin',\n",
       "  'expected_output': 'gn\\ning yao shanelle roman and et al 2018 spider a\\n'},\n",
       " {'input_text': 'asunaga\\ndongxu wang zifan li james ma irene li qingn\\ning yao shanelle roman and et al 2018 spider a\\n',\n",
       "  'expected_output': 'largescale humanlabeled dataset for complex and\\ncr'},\n",
       " {'input_text': 'gn\\ning yao shanelle roman and et al 2018 spider a\\nlargescale humanlabeled dataset for complex and\\ncr',\n",
       "  'expected_output': 'ossdomain semantic parsing and texttosql task\\nproc'},\n",
       " {'input_text': 'largescale humanlabeled dataset for complex and\\ncrossdomain semantic parsing and texttosql task\\nproc',\n",
       "  'expected_output': 'eedings of the 2018 conference on empirical\\nmethod'},\n",
       " {'input_text': 'ossdomain semantic parsing and texttosql task\\nproceedings of the 2018 conference on empirical\\nmethod',\n",
       "  'expected_output': 's in natural language processing \\nruiqi zhong tao '},\n",
       " {'input_text': 'eedings of the 2018 conference on empirical\\nmethods in natural language processing \\nruiqi zhong tao ',\n",
       "  'expected_output': 'yu and dan klein 2020 seman\\ntic evaluation for tex'},\n",
       " {'input_text': 's in natural language processing \\nruiqi zhong tao yu and dan klein 2020 seman\\ntic evaluation for tex',\n",
       "  'expected_output': 'ttosql with distilled test suites\\nproceedings of t'},\n",
       " {'input_text': 'yu and dan klein 2020 seman\\ntic evaluation for texttosql with distilled test suites\\nproceedings of t',\n",
       "  'expected_output': 'he 2020 conference on empirical\\nmethods in natural'},\n",
       " {'input_text': 'ttosql with distilled test suites\\nproceedings of the 2020 conference on empirical\\nmethods in natural',\n",
       "  'expected_output': ' language processing emnlp retrieval\\naugmented\\ngen'},\n",
       " {'input_text': 'he 2020 conference on empirical\\nmethods in natural language processing emnlp retrieval\\naugmented\\ngen',\n",
       "  'expected_output': 'eration\\nabhinav kimothia simple introductiontable '},\n",
       " {'input_text': ' language processing emnlp retrieval\\naugmented\\ngeneration\\nabhinav kimothia simple introductiontable ',\n",
       "  'expected_output': 'of contents\\nabhinav kimothi01 what is rag  3\\n02 ho'},\n",
       " {'input_text': 'eration\\nabhinav kimothia simple introductiontable of contents\\nabhinav kimothi01 what is rag  3\\n02 ho',\n",
       "  'expected_output': 'w does rag help  6\\n03 what are some popular rag us'},\n",
       " {'input_text': 'of contents\\nabhinav kimothi01 what is rag  3\\n02 how does rag help  6\\n03 what are some popular rag us',\n",
       "  'expected_output': 'e cases  7\\n04 rag architecture  8\\n        i indexi'},\n",
       " {'input_text': 'w does rag help  6\\n03 what are some popular rag use cases  7\\n04 rag architecture  8\\n        i indexi',\n",
       "  'expected_output': 'ng pipeline  9\\n            a data loading  10\\n    '},\n",
       " {'input_text': 'e cases  7\\n04 rag architecture  8\\n        i indexing pipeline  9\\n            a data loading  10\\n    ',\n",
       "  'expected_output': '        b document splitting  14\\n            c emb'},\n",
       " {'input_text': 'ng pipeline  9\\n            a data loading  10\\n            b document splitting  14\\n            c emb',\n",
       "  'expected_output': 'edding  23\\n            d vector stores  29\\n       '},\n",
       " {'input_text': '        b document splitting  14\\n            c embedding  23\\n            d vector stores  29\\n       ',\n",
       "  'expected_output': ' ii rag pipeline  35\\n            a retrieval  37 \\n'},\n",
       " {'input_text': 'edding  23\\n            d vector stores  29\\n        ii rag pipeline  35\\n            a retrieval  37 \\n',\n",
       "  'expected_output': '            b augmentation and generation  45\\n05 e'},\n",
       " {'input_text': ' ii rag pipeline  35\\n            a retrieval  37 \\n            b augmentation and generation  45\\n05 e',\n",
       "  'expected_output': 'valuation  46\\n06 rag vs finetuning  56\\n07 evolving'},\n",
       " {'input_text': '            b augmentation and generation  45\\n05 evaluation  46\\n06 rag vs finetuning  56\\n07 evolving',\n",
       "  'expected_output': ' rag llmops stack  59\\n08 multimodal rag  63\\n09 pro'},\n",
       " {'input_text': 'valuation  46\\n06 rag vs finetuning  56\\n07 evolving rag llmops stack  59\\n08 multimodal rag  63\\n09 pro',\n",
       "  'expected_output': 'gression of rag systems  66\\n        i naive rag  6'},\n",
       " {'input_text': ' rag llmops stack  59\\n08 multimodal rag  63\\n09 progression of rag systems  66\\n        i naive rag  6',\n",
       "  'expected_output': '6\\n        ii advanced rag  67\\n        iii multimod'},\n",
       " {'input_text': 'gression of rag systems  66\\n        i naive rag  66\\n        ii advanced rag  67\\n        iii multimod',\n",
       "  'expected_output': 'al rag  71\\n10 acknowledgements  73\\n11 resources  7'},\n",
       " {'input_text': '6\\n        ii advanced rag  67\\n        iii multimodal rag  71\\n10 acknowledgements  73\\n11 resources  7',\n",
       "  'expected_output': '4\\nkeep calm  build ai2generative ai large language'},\n",
       " {'input_text': 'al rag  71\\n10 acknowledgements  73\\n11 resources  74\\nkeep calm  build ai2generative ai large language',\n",
       "  'expected_output': ' models\\n20221106 20231105020406080100\\nabhinav kimo'},\n",
       " {'input_text': '4\\nkeep calm  build ai2generative ai large language models\\n20221106 20231105020406080100\\nabhinav kimo',\n",
       "  'expected_output': 'thiwhat is rag\\nwhat is rag\\ngoogle trends  interest'},\n",
       " {'input_text': ' models\\n20221106 20231105020406080100\\nabhinav kimothiwhat is rag\\nwhat is rag\\ngoogle trends  interest',\n",
       "  'expected_output': ' over time nov22 to nov23 30th november 2022 will '},\n",
       " {'input_text': 'thiwhat is rag\\nwhat is rag\\ngoogle trends  interest over time nov22 to nov23 30th november 2022 will ',\n",
       "  'expected_output': 'be remembered as the watershed moment in artificia'},\n",
       " {'input_text': ' over time nov22 to nov23 30th november 2022 will be remembered as the watershed moment in artificia',\n",
       "  'expected_output': 'l\\nintelligence openai released chatgpt and the wor'},\n",
       " {'input_text': 'be remembered as the watershed moment in artificial\\nintelligence openai released chatgpt and the wor',\n",
       "  'expected_output': 'ld was mesmerised interest in\\npreviously obscure t'},\n",
       " {'input_text': 'l\\nintelligence openai released chatgpt and the world was mesmerised interest in\\npreviously obscure t',\n",
       "  'expected_output': 'erms like generative ai and large language models '},\n",
       " {'input_text': 'ld was mesmerised interest in\\npreviously obscure terms like generative ai and large language models ',\n",
       "  'expected_output': 'llms\\nwas unstoppable over the following 12 months\\n'},\n",
       " {'input_text': 'erms like generative ai and large language models llms\\nwas unstoppable over the following 12 months\\n',\n",
       "  'expected_output': 'the curse of the llms\\nas usage exploded so did the'},\n",
       " {'input_text': 'llms\\nwas unstoppable over the following 12 months\\nthe curse of the llms\\nas usage exploded so did the',\n",
       "  'expected_output': ' expectations many users started using chatgpt as\\n'},\n",
       " {'input_text': 'the curse of the llms\\nas usage exploded so did the expectations many users started using chatgpt as\\n',\n",
       "  'expected_output': 'a source of information like an alternative to goo'},\n",
       " {'input_text': ' expectations many users started using chatgpt as\\na source of information like an alternative to goo',\n",
       "  'expected_output': 'gle as a result they also started\\nencountering pro'},\n",
       " {'input_text': 'a source of information like an alternative to google as a result they also started\\nencountering pro',\n",
       "  'expected_output': 'minent weaknesses of the system concerns around co'},\n",
       " {'input_text': 'gle as a result they also started\\nencountering prominent weaknesses of the system concerns around co',\n",
       "  'expected_output': 'pyright\\nprivacy security ability to do mathematica'},\n",
       " {'input_text': 'minent weaknesses of the system concerns around copyright\\nprivacy security ability to do mathematica',\n",
       "  'expected_output': 'l calculations etc aside people\\nrealised that ther'},\n",
       " {'input_text': 'pyright\\nprivacy security ability to do mathematical calculations etc aside people\\nrealised that ther',\n",
       "  'expected_output': 'e are two major limitations of large language mode'},\n",
       " {'input_text': 'l calculations etc aside people\\nrealised that there are two major limitations of large language mode',\n",
       "  'expected_output': 'ls\\na knowledge cutoff date hallucinations\\nusers lo'},\n",
       " {'input_text': 'e are two major limitations of large language models\\na knowledge cutoff date hallucinations\\nusers lo',\n",
       "  'expected_output': 'ok at llms for knowledge and wisdom yet llms\\nare s'},\n",
       " {'input_text': 'ls\\na knowledge cutoff date hallucinations\\nusers look at llms for knowledge and wisdom yet llms\\nare s',\n",
       "  'expected_output': 'ophisticated predictors of what word comes nexttra'},\n",
       " {'input_text': 'ok at llms for knowledge and wisdom yet llms\\nare sophisticated predictors of what word comes nexttra',\n",
       "  'expected_output': 'ining an llm is an expensive and\\ntimeconsuming pro'},\n",
       " {'input_text': 'ophisticated predictors of what word comes nexttraining an llm is an expensive and\\ntimeconsuming pro',\n",
       "  'expected_output': 'cess llms are\\ntrained on massive amount of data th'},\n",
       " {'input_text': 'ining an llm is an expensive and\\ntimeconsuming process llms are\\ntrained on massive amount of data th',\n",
       "  'expected_output': 'e\\ndata that llms are trained on is\\ntherefore histo'},\n",
       " {'input_text': 'cess llms are\\ntrained on massive amount of data the\\ndata that llms are trained on is\\ntherefore histo',\n",
       "  'expected_output': 'rical or dated \\neg the latest gpt4 model by openai'},\n",
       " {'input_text': 'e\\ndata that llms are trained on is\\ntherefore historical or dated \\neg the latest gpt4 model by openai',\n",
       "  'expected_output': '\\nhas knowledge only till april 2023 and\\nany event '},\n",
       " {'input_text': 'rical or dated \\neg the latest gpt4 model by openai\\nhas knowledge only till april 2023 and\\nany event ',\n",
       "  'expected_output': 'that happened post that\\ndate the information is no'},\n",
       " {'input_text': '\\nhas knowledge only till april 2023 and\\nany event that happened post that\\ndate the information is no',\n",
       "  'expected_output': 't available to\\nthe modeloften it was observed that'},\n",
       " {'input_text': 'that happened post that\\ndate the information is not available to\\nthe modeloften it was observed that',\n",
       "  'expected_output': ' llms\\nprovided responses that were factually\\nincor'},\n",
       " {'input_text': 't available to\\nthe modeloften it was observed that llms\\nprovided responses that were factually\\nincor',\n",
       "  'expected_output': 'rect despite being factually\\nincorrect the llm res'},\n",
       " {'input_text': ' llms\\nprovided responses that were factually\\nincorrect despite being factually\\nincorrect the llm res',\n",
       "  'expected_output': 'ponses\\nsounded extremely confident and\\nlegitimate '},\n",
       " {'input_text': 'rect despite being factually\\nincorrect the llm responses\\nsounded extremely confident and\\nlegitimate ',\n",
       "  'expected_output': 'this characteristic of  lying\\nwith confidence prov'},\n",
       " {'input_text': 'ponses\\nsounded extremely confident and\\nlegitimate this characteristic of  lying\\nwith confidence prov',\n",
       "  'expected_output': 'ed to be one of\\nthe biggest criticisms of chatgpt '},\n",
       " {'input_text': 'this characteristic of  lying\\nwith confidence proved to be one of\\nthe biggest criticisms of chatgpt ',\n",
       "  'expected_output': 'and\\nllm techniques in generalretrieval augmented g'},\n",
       " {'input_text': 'ed to be one of\\nthe biggest criticisms of chatgpt and\\nllm techniques in generalretrieval augmented g',\n",
       "  'expected_output': 'eneration\\nkeep calm  build ai3while model retraini'},\n",
       " {'input_text': 'and\\nllm techniques in generalretrieval augmented generation\\nkeep calm  build ai3while model retraini',\n",
       "  'expected_output': 'ngfinetuningreinforcement learning are options tha'},\n",
       " {'input_text': 'eneration\\nkeep calm  build ai3while model retrainingfinetuningreinforcement learning are options tha',\n",
       "  'expected_output': 't solve\\nthe aforementioned challenges these approa'},\n",
       " {'input_text': 'ngfinetuningreinforcement learning are options that solve\\nthe aforementioned challenges these approa',\n",
       "  'expected_output': 'ches are timeconsuming and\\ncostly in majority of t'},\n",
       " {'input_text': 't solve\\nthe aforementioned challenges these approaches are timeconsuming and\\ncostly in majority of t',\n",
       "  'expected_output': 'he usecase these costs are prohibitive \\nin may 202'},\n",
       " {'input_text': 'ches are timeconsuming and\\ncostly in majority of the usecase these costs are prohibitive \\nin may 202',\n",
       "  'expected_output': '0 researchers in their paper retrievalaugmented ge'},\n",
       " {'input_text': 'he usecase these costs are prohibitive \\nin may 2020 researchers in their paper retrievalaugmented ge',\n",
       "  'expected_output': 'neration for\\nknowledgeintensive nlp tasks explored'},\n",
       " {'input_text': '0 researchers in their paper retrievalaugmented generation for\\nknowledgeintensive nlp tasks explored',\n",
       "  'expected_output': ' models which combine pretrained\\nparametric and no'},\n",
       " {'input_text': 'neration for\\nknowledgeintensive nlp tasks explored models which combine pretrained\\nparametric and no',\n",
       "  'expected_output': 'nparametric memory for language generation \\nabhina'},\n",
       " {'input_text': ' models which combine pretrained\\nparametric and nonparametric memory for language generation \\nabhina',\n",
       "  'expected_output': 'v kimothiwhat is rag\\nthe hunger for more\\nwhile the'},\n",
       " {'input_text': 'nparametric memory for language generation \\nabhinav kimothiwhat is rag\\nthe hunger for more\\nwhile the',\n",
       "  'expected_output': ' weaknesses of llms were being discussed a paralle'},\n",
       " {'input_text': 'v kimothiwhat is rag\\nthe hunger for more\\nwhile the weaknesses of llms were being discussed a paralle',\n",
       "  'expected_output': 'l discourse around\\nproviding context to the models'},\n",
       " {'input_text': ' weaknesses of llms were being discussed a parallel discourse around\\nproviding context to the models',\n",
       "  'expected_output': ' started in essence it meant creating a chatgpt\\non'},\n",
       " {'input_text': 'l discourse around\\nproviding context to the models started in essence it meant creating a chatgpt\\non',\n",
       "  'expected_output': ' proprietary data\\nmake llms respond with uptodate '},\n",
       " {'input_text': ' started in essence it meant creating a chatgpt\\non proprietary data\\nmake llms respond with uptodate ',\n",
       "  'expected_output': 'information\\nmake llms not respond with factually i'},\n",
       " {'input_text': ' proprietary data\\nmake llms respond with uptodate information\\nmake llms not respond with factually i',\n",
       "  'expected_output': 'naccurate\\ninformation\\nmake llms aware of proprieta'},\n",
       " {'input_text': 'information\\nmake llms not respond with factually inaccurate\\ninformation\\nmake llms aware of proprieta',\n",
       "  'expected_output': 'ry informationthe challenge \\nproviding contextprov'},\n",
       " {'input_text': 'naccurate\\ninformation\\nmake llms aware of proprietary informationthe challenge \\nproviding contextprov',\n",
       "  'expected_output': 'iding llms with information not in their memory\\nke'},\n",
       " {'input_text': 'ry informationthe challenge \\nproviding contextproviding llms with information not in their memory\\nke',\n",
       "  'expected_output': 'ep calm  build ai4retriever\\nsearch fetch\\ncontext\\np'},\n",
       " {'input_text': 'iding llms with information not in their memory\\nkeep calm  build ai4retriever\\nsearch fetch\\ncontext\\np',\n",
       "  'expected_output': 'roprietary and nonproprietary informationuserpromp'},\n",
       " {'input_text': 'ep calm  build ai4retriever\\nsearch fetch\\ncontext\\nproprietary and nonproprietary informationuserpromp',\n",
       "  'expected_output': 't  prompt  context \\nllmr a g\\nr\\na\\nglookup the exter'},\n",
       " {'input_text': 'roprietary and nonproprietary informationuserprompt  prompt  context \\nllmr a g\\nr\\na\\nglookup the exter',\n",
       "  'expected_output': 'nal source to\\nretrieve the   relevant information\\n'},\n",
       " {'input_text': 't  prompt  context \\nllmr a g\\nr\\na\\nglookup the external source to\\nretrieve the   relevant information\\n',\n",
       "  'expected_output': 'add the retrieved information to\\nthe user prompt\\nu'},\n",
       " {'input_text': 'nal source to\\nretrieve the   relevant information\\nadd the retrieved information to\\nthe user prompt\\nu',\n",
       "  'expected_output': 'se llm to generate response to\\nuser prompt with th'},\n",
       " {'input_text': 'add the retrieved information to\\nthe user prompt\\nuse llm to generate response to\\nuser prompt with th',\n",
       "  'expected_output': 'e context\\nretrieved relevant information is augmen'},\n",
       " {'input_text': 'se llm to generate response to\\nuser prompt with the context\\nretrieved relevant information is augmen',\n",
       "  'expected_output': 'ted to the prompt as context\\nllm is asked to gener'},\n",
       " {'input_text': 'e context\\nretrieved relevant information is augmented to the prompt as context\\nllm is asked to gener',\n",
       "  'expected_output': 'ate response to the prompt in the context\\naugmente'},\n",
       " {'input_text': 'ted to the prompt as context\\nllm is asked to generate response to the prompt in the context\\naugmente',\n",
       "  'expected_output': 'd information\\nabhinav kimothiwhat is rag\\nso what i'},\n",
       " {'input_text': 'ate response to the prompt in the context\\naugmented information\\nabhinav kimothiwhat is rag\\nso what i',\n",
       "  'expected_output': 's rag\\nin 2023 rag has become one of the most used '},\n",
       " {'input_text': 'd information\\nabhinav kimothiwhat is rag\\nso what is rag\\nin 2023 rag has become one of the most used ',\n",
       "  'expected_output': 'technique in the domain of large\\nlanguage models\\nw'},\n",
       " {'input_text': 's rag\\nin 2023 rag has become one of the most used technique in the domain of large\\nlanguage models\\nw',\n",
       "  'expected_output': 'hat is rag\\nuser enters a promptquery\\nretriever sea'},\n",
       " {'input_text': 'technique in the domain of large\\nlanguage models\\nwhat is rag\\nuser enters a promptquery\\nretriever sea',\n",
       "  'expected_output': 'rches and fetches information relevant to the prom'},\n",
       " {'input_text': 'hat is rag\\nuser enters a promptquery\\nretriever searches and fetches information relevant to the prom',\n",
       "  'expected_output': 'pt\\neg from the internet or internet data warehouse'},\n",
       " {'input_text': 'rches and fetches information relevant to the prompt\\neg from the internet or internet data warehouse',\n",
       "  'expected_output': '\\nuser receives the response\\na naive rag workflow\\nk'},\n",
       " {'input_text': 'pt\\neg from the internet or internet data warehouse\\nuser receives the response\\na naive rag workflow\\nk',\n",
       "  'expected_output': 'eep calm  build ai5retrieverweb pages\\napis  dynami'},\n",
       " {'input_text': '\\nuser receives the response\\na naive rag workflow\\nkeep calm  build ai5retrieverweb pages\\napis  dynami',\n",
       "  'expected_output': 'c dbs\\ndocument reposwithout rag  \\nabhinav kimothih'},\n",
       " {'input_text': 'eep calm  build ai5retrieverweb pages\\napis  dynamic dbs\\ndocument reposwithout rag  \\nabhinav kimothih',\n",
       "  'expected_output': 'ow does rag help\\nhow does rag help\\nunlimited knowl'},\n",
       " {'input_text': 'c dbs\\ndocument reposwithout rag  \\nabhinav kimothihow does rag help\\nhow does rag help\\nunlimited knowl',\n",
       "  'expected_output': 'edge\\nthe retriever of an rag system can have acces'},\n",
       " {'input_text': 'ow does rag help\\nhow does rag help\\nunlimited knowledge\\nthe retriever of an rag system can have acces',\n",
       "  'expected_output': 's to external sources of information therefore\\nthe'},\n",
       " {'input_text': 'edge\\nthe retriever of an rag system can have access to external sources of information therefore\\nthe',\n",
       "  'expected_output': ' llm is not limited to its internal knowledge the '},\n",
       " {'input_text': 's to external sources of information therefore\\nthe llm is not limited to its internal knowledge the ',\n",
       "  'expected_output': 'external sources can be proprietary\\ndocuments and '},\n",
       " {'input_text': ' llm is not limited to its internal knowledge the external sources can be proprietary\\ndocuments and ',\n",
       "  'expected_output': 'data or even the internet\\nan llm has knowledge\\nonl'},\n",
       " {'input_text': 'external sources can be proprietary\\ndocuments and data or even the internet\\nan llm has knowledge\\nonl',\n",
       "  'expected_output': 'y of the data it has\\nbeen trained on\\nalso called p'},\n",
       " {'input_text': 'data or even the internet\\nan llm has knowledge\\nonly of the data it has\\nbeen trained on\\nalso called p',\n",
       "  'expected_output': 'arametric\\nmemory information\\nstored in the model\\np'},\n",
       " {'input_text': 'y of the data it has\\nbeen trained on\\nalso called parametric\\nmemory information\\nstored in the model\\np',\n",
       "  'expected_output': 'arametersdatabasesother sources\\nretriever  searche'},\n",
       " {'input_text': 'arametric\\nmemory information\\nstored in the model\\nparametersdatabasesother sources\\nretriever  searche',\n",
       "  'expected_output': 's and fetches information that the llm has not\\nnec'},\n",
       " {'input_text': 'arametersdatabasesother sources\\nretriever  searches and fetches information that the llm has not\\nnec',\n",
       "  'expected_output': 'essarily been trained on this adds to the llm memo'},\n",
       " {'input_text': 's and fetches information that the llm has not\\nnecessarily been trained on this adds to the llm memo',\n",
       "  'expected_output': 'ry and is passed as\\ncontext in the prompts also ca'},\n",
       " {'input_text': 'essarily been trained on this adds to the llm memory and is passed as\\ncontext in the prompts also ca',\n",
       "  'expected_output': 'lled nonparametric memory information\\navailable ou'},\n",
       " {'input_text': 'ry and is passed as\\ncontext in the prompts also called nonparametric memory information\\navailable ou',\n",
       "  'expected_output': 'tside the model parameters\\nexpandable to all sourc'},\n",
       " {'input_text': 'lled nonparametric memory information\\navailable outside the model parameters\\nexpandable to all sourc',\n",
       "  'expected_output': 'es\\neasier to updatemaintain\\nmuch cheaper than retr'},\n",
       " {'input_text': 'tside the model parameters\\nexpandable to all sources\\neasier to updatemaintain\\nmuch cheaper than retr',\n",
       "  'expected_output': 'ainingfinetuning\\nthe effort lies in creation of th'},\n",
       " {'input_text': 'es\\neasier to updatemaintain\\nmuch cheaper than retrainingfinetuning\\nthe effort lies in creation of th',\n",
       "  'expected_output': 'e knowledge basewith rag\\nconfidence in responses\\nw'},\n",
       " {'input_text': 'ainingfinetuning\\nthe effort lies in creation of the knowledge basewith rag\\nconfidence in responses\\nw',\n",
       "  'expected_output': 'ith the context extra information that is retrieve'},\n",
       " {'input_text': 'e knowledge basewith rag\\nconfidence in responses\\nwith the context extra information that is retrieve',\n",
       "  'expected_output': 'd made available to the llm\\nthe confidence in llm '},\n",
       " {'input_text': 'ith the context extra information that is retrieved made available to the llm\\nthe confidence in llm ',\n",
       "  'expected_output': 'responses is increased\\ncontext awareness source ci'},\n",
       " {'input_text': 'd made available to the llm\\nthe confidence in llm responses is increased\\ncontext awareness source ci',\n",
       "  'expected_output': 'tation reduced hallucinations\\nadded information\\nas'},\n",
       " {'input_text': 'responses is increased\\ncontext awareness source citation reduced hallucinations\\nadded information\\nas',\n",
       "  'expected_output': 'sists llms in\\ngenerating responses\\nthat are accura'},\n",
       " {'input_text': 'tation reduced hallucinations\\nadded information\\nassists llms in\\ngenerating responses\\nthat are accura',\n",
       "  'expected_output': 'te and\\ncontextually appropriateaccess to sources o'},\n",
       " {'input_text': 'sists llms in\\ngenerating responses\\nthat are accurate and\\ncontextually appropriateaccess to sources o',\n",
       "  'expected_output': 'f\\ninformation improves the\\ntransparency of the llm'},\n",
       " {'input_text': 'te and\\ncontextually appropriateaccess to sources of\\ninformation improves the\\ntransparency of the llm',\n",
       "  'expected_output': '\\nresponsesrag enabled llm\\nsystems are observed to\\n'},\n",
       " {'input_text': 'f\\ninformation improves the\\ntransparency of the llm\\nresponsesrag enabled llm\\nsystems are observed to\\n',\n",
       "  'expected_output': 'be less prone to\\nhallucinations than the\\nones with'},\n",
       " {'input_text': '\\nresponsesrag enabled llm\\nsystems are observed to\\nbe less prone to\\nhallucinations than the\\nones with',\n",
       "  'expected_output': 'out rag\\nkeep calm  build ai6abhinav kimothiwhat ar'},\n",
       " {'input_text': 'be less prone to\\nhallucinations than the\\nones without rag\\nkeep calm  build ai6abhinav kimothiwhat ar',\n",
       "  'expected_output': 'e some popular rag use cases\\nrag use cases \\nthe de'},\n",
       " {'input_text': 'out rag\\nkeep calm  build ai6abhinav kimothiwhat are some popular rag use cases\\nrag use cases \\nthe de',\n",
       "  'expected_output': 'velopment of rag technique is rooted in use cases '},\n",
       " {'input_text': 'e some popular rag use cases\\nrag use cases \\nthe development of rag technique is rooted in use cases ',\n",
       "  'expected_output': 'that were limited by the\\ninherent weaknesses of th'},\n",
       " {'input_text': 'velopment of rag technique is rooted in use cases that were limited by the\\ninherent weaknesses of th',\n",
       "  'expected_output': 'e llms as of today some commercial applications of'},\n",
       " {'input_text': 'that were limited by the\\ninherent weaknesses of the llms as of today some commercial applications of',\n",
       "  'expected_output': '\\nrag are in  \\nby providing access to proprietary e'},\n",
       " {'input_text': 'e llms as of today some commercial applications of\\nrag are in  \\nby providing access to proprietary e',\n",
       "  'expected_output': 'nterprise document to an llm the\\nresponses are lim'},\n",
       " {'input_text': '\\nrag are in  \\nby providing access to proprietary enterprise document to an llm the\\nresponses are lim',\n",
       "  'expected_output': 'ited to what is provided within them a retriever c'},\n",
       " {'input_text': 'nterprise document to an llm the\\nresponses are limited to what is provided within them a retriever c',\n",
       "  'expected_output': 'an\\nsearch for the most relevant documents and prov'},\n",
       " {'input_text': 'ited to what is provided within them a retriever can\\nsearch for the most relevant documents and prov',\n",
       "  'expected_output': 'ide the information to\\nthe llm check out this blog'},\n",
       " {'input_text': 'an\\nsearch for the most relevant documents and provide the information to\\nthe llm check out this blog',\n",
       "  'expected_output': ' for an exampledocument question answering systems'},\n",
       " {'input_text': 'ide the information to\\nthe llm check out this blog for an exampledocument question answering systems',\n",
       "  'expected_output': '\\nllms can be customised to productservice manuals '},\n",
       " {'input_text': ' for an exampledocument question answering systems\\nllms can be customised to productservice manuals ',\n",
       "  'expected_output': 'domain\\nknowledge guidelines etc using rag the agen'},\n",
       " {'input_text': '\\nllms can be customised to productservice manuals domain\\nknowledge guidelines etc using rag the agen',\n",
       "  'expected_output': 't can also route users to\\nmore specialised agents '},\n",
       " {'input_text': 'domain\\nknowledge guidelines etc using rag the agent can also route users to\\nmore specialised agents ',\n",
       "  'expected_output': 'depending on their query searchunify has an\\nllmrag'},\n",
       " {'input_text': 't can also route users to\\nmore specialised agents depending on their query searchunify has an\\nllmrag',\n",
       "  'expected_output': ' powered conversational agent for their users\\nimag'},\n",
       " {'input_text': 'depending on their query searchunify has an\\nllmrag powered conversational agent for their users\\nimag',\n",
       "  'expected_output': 'ine an event like a sports or a new event a retrie'},\n",
       " {'input_text': ' powered conversational agent for their users\\nimagine an event like a sports or a new event a retrie',\n",
       "  'expected_output': 'ver can connect to\\nrealtime updatesdata via apis a'},\n",
       " {'input_text': 'ine an event like a sports or a new event a retriever can connect to\\nrealtime updatesdata via apis a',\n",
       "  'expected_output': 'nd pass this information to the llm to\\ncreate a vi'},\n",
       " {'input_text': 'ver can connect to\\nrealtime updatesdata via apis and pass this information to the llm to\\ncreate a vi',\n",
       "  'expected_output': 'rtual commentator these can further be augmented w'},\n",
       " {'input_text': 'nd pass this information to the llm to\\ncreate a virtual commentator these can further be augmented w',\n",
       "  'expected_output': 'ith\\ntext to speech modelsibm leveraged the technol'},\n",
       " {'input_text': 'rtual commentator these can further be augmented with\\ntext to speech modelsibm leveraged the technol',\n",
       "  'expected_output': 'ogy for commentary\\nduring the 2023 us openconversa'},\n",
       " {'input_text': 'ith\\ntext to speech modelsibm leveraged the technology for commentary\\nduring the 2023 us openconversa',\n",
       "  'expected_output': 'tional agents\\nrealtime event commentary\\ncontent ge'},\n",
       " {'input_text': 'ogy for commentary\\nduring the 2023 us openconversational agents\\nrealtime event commentary\\ncontent ge',\n",
       "  'expected_output': 'neration\\nthe widest use of llms has probably been '},\n",
       " {'input_text': 'tional agents\\nrealtime event commentary\\ncontent generation\\nthe widest use of llms has probably been ',\n",
       "  'expected_output': 'in content generation using\\nrag the generation can'},\n",
       " {'input_text': 'neration\\nthe widest use of llms has probably been in content generation using\\nrag the generation can',\n",
       "  'expected_output': ' be personalised to readers incorporate real\\ntime '},\n",
       " {'input_text': 'in content generation using\\nrag the generation can be personalised to readers incorporate real\\ntime ',\n",
       "  'expected_output': 'trends and be contextually appropriate yarnit is a'},\n",
       " {'input_text': ' be personalised to readers incorporate real\\ntime trends and be contextually appropriate yarnit is a',\n",
       "  'expected_output': 'n ai based\\ncontent marketing platform that uses ra'},\n",
       " {'input_text': 'trends and be contextually appropriate yarnit is an ai based\\ncontent marketing platform that uses ra',\n",
       "  'expected_output': 'g for multiple tasks\\npersonalised recommendation\\nr'},\n",
       " {'input_text': 'n ai based\\ncontent marketing platform that uses rag for multiple tasks\\npersonalised recommendation\\nr',\n",
       "  'expected_output': 'ecommendation engines have been a game changes in '},\n",
       " {'input_text': 'g for multiple tasks\\npersonalised recommendation\\nrecommendation engines have been a game changes in ',\n",
       "  'expected_output': 'the digital\\neconomy llms are capable of powering t'},\n",
       " {'input_text': 'ecommendation engines have been a game changes in the digital\\neconomy llms are capable of powering t',\n",
       "  'expected_output': 'he next evolution in content\\nrecommendations check'},\n",
       " {'input_text': 'the digital\\neconomy llms are capable of powering the next evolution in content\\nrecommendations check',\n",
       "  'expected_output': ' out amans blog on the utility of llms in\\nrecommen'},\n",
       " {'input_text': 'he next evolution in content\\nrecommendations check out amans blog on the utility of llms in\\nrecommen',\n",
       "  'expected_output': 'dation systems\\nvirtual assistants\\nvirtual personal'},\n",
       " {'input_text': ' out amans blog on the utility of llms in\\nrecommendation systems\\nvirtual assistants\\nvirtual personal',\n",
       "  'expected_output': ' assistants like siri alexa and others are in plan'},\n",
       " {'input_text': 'dation systems\\nvirtual assistants\\nvirtual personal assistants like siri alexa and others are in plan',\n",
       "  'expected_output': 's to use\\nllms to enhance the experience coupled wi'},\n",
       " {'input_text': ' assistants like siri alexa and others are in plans to use\\nllms to enhance the experience coupled wi',\n",
       "  'expected_output': 'th more context on user\\nbehaviour these assistants'},\n",
       " {'input_text': 's to use\\nllms to enhance the experience coupled with more context on user\\nbehaviour these assistants',\n",
       "  'expected_output': ' can become highly personalised  \\nkeep calm  build'},\n",
       " {'input_text': 'th more context on user\\nbehaviour these assistants can become highly personalised  \\nkeep calm  build',\n",
       "  'expected_output': ' ai7promptsearch relevant\\ninformation\\nllm\\nendpoint'},\n",
       " {'input_text': ' can become highly personalised  \\nkeep calm  build ai7promptsearch relevant\\ninformation\\nllm\\nendpoint',\n",
       "  'expected_output': '      \\nknowledge sources\\nabhinav kimothirag archit'},\n",
       " {'input_text': ' ai7promptsearch relevant\\ninformation\\nllm\\nendpoint      \\nknowledge sources\\nabhinav kimothirag archit',\n",
       "  'expected_output': 'ecture\\nrag architecture \\nlets revisit the five hig'},\n",
       " {'input_text': '      \\nknowledge sources\\nabhinav kimothirag architecture\\nrag architecture \\nlets revisit the five hig',\n",
       "  'expected_output': 'h level steps of an rag enabled system\\nprompt\\nrele'},\n",
       " {'input_text': 'ecture\\nrag architecture \\nlets revisit the five high level steps of an rag enabled system\\nprompt\\nrele',\n",
       "  'expected_output': 'vant \\ncontext\\nprompt  context\\ngenerated response\\nr'},\n",
       " {'input_text': 'h level steps of an rag enabled system\\nprompt\\nrelevant \\ncontext\\nprompt  context\\ngenerated response\\nr',\n",
       "  'expected_output': 'ag system\\nuser writes a prompt or a query that is '},\n",
       " {'input_text': 'vant \\ncontext\\nprompt  context\\ngenerated response\\nrag system\\nuser writes a prompt or a query that is ',\n",
       "  'expected_output': 'passed to an orchestrator\\norchestrator sends a sea'},\n",
       " {'input_text': 'ag system\\nuser writes a prompt or a query that is passed to an orchestrator\\norchestrator sends a sea',\n",
       "  'expected_output': 'rch query to the retriever\\nretriever fetches the r'},\n",
       " {'input_text': 'passed to an orchestrator\\norchestrator sends a search query to the retriever\\nretriever fetches the r',\n",
       "  'expected_output': 'elevant information from the knowledge sources and'},\n",
       " {'input_text': 'rch query to the retriever\\nretriever fetches the relevant information from the knowledge sources and',\n",
       "  'expected_output': ' sends back\\norchestrator augments the prompt with '},\n",
       " {'input_text': 'elevant information from the knowledge sources and sends back\\norchestrator augments the prompt with ',\n",
       "  'expected_output': 'the context and sends to the llm\\nllm responds with'},\n",
       " {'input_text': ' sends back\\norchestrator augments the prompt with the context and sends to the llm\\nllm responds with',\n",
       "  'expected_output': ' the generated text which is displayed to the user'},\n",
       " {'input_text': 'the context and sends to the llm\\nllm responds with the generated text which is displayed to the user',\n",
       "  'expected_output': ' via the orchestrator\\ntwo pipelines become importa'},\n",
       " {'input_text': ' the generated text which is displayed to the user via the orchestrator\\ntwo pipelines become importa',\n",
       "  'expected_output': 'nt in setting up the rag system the first one bein'},\n",
       " {'input_text': ' via the orchestrator\\ntwo pipelines become important in setting up the rag system the first one bein',\n",
       "  'expected_output': 'g\\nsetting up the knowledge sources for efficient s'},\n",
       " {'input_text': 'nt in setting up the rag system the first one being\\nsetting up the knowledge sources for efficient s',\n",
       "  'expected_output': 'earch and retrieval and the\\nsecond one being the f'},\n",
       " {'input_text': 'g\\nsetting up the knowledge sources for efficient search and retrieval and the\\nsecond one being the f',\n",
       "  'expected_output': 'ive steps of the generation \\nindexing pipeline\\ndat'},\n",
       " {'input_text': 'earch and retrieval and the\\nsecond one being the five steps of the generation \\nindexing pipeline\\ndat',\n",
       "  'expected_output': 'a for the knowledge is ingested from the source an'},\n",
       " {'input_text': 'ive steps of the generation \\nindexing pipeline\\ndata for the knowledge is ingested from the source an',\n",
       "  'expected_output': 'd indexed this\\ninvolves steps like splitting creat'},\n",
       " {'input_text': 'a for the knowledge is ingested from the source and indexed this\\ninvolves steps like splitting creat',\n",
       "  'expected_output': 'ion of embeddings and storage of\\ndata\\nrag pipeline'},\n",
       " {'input_text': 'd indexed this\\ninvolves steps like splitting creation of embeddings and storage of\\ndata\\nrag pipeline',\n",
       "  'expected_output': '\\nthis involves the actual rag process which takes '},\n",
       " {'input_text': 'ion of embeddings and storage of\\ndata\\nrag pipeline\\nthis involves the actual rag process which takes ',\n",
       "  'expected_output': 'the user query at\\nrun time and retrieves the relev'},\n",
       " {'input_text': '\\nthis involves the actual rag process which takes the user query at\\nrun time and retrieves the relev',\n",
       "  'expected_output': 'ant data from the index then passes\\nthat to the mo'},\n",
       " {'input_text': 'the user query at\\nrun time and retrieves the relevant data from the index then passes\\nthat to the mo',\n",
       "  'expected_output': 'del\\nkeep calm  build ai8retriever\\nfetch\\nshort cont'},\n",
       " {'input_text': 'ant data from the index then passes\\nthat to the model\\nkeep calm  build ai8retriever\\nfetch\\nshort cont',\n",
       "  'expected_output': 'ext on the fly indexing\\nabhinav kimothiindexing pi'},\n",
       " {'input_text': 'del\\nkeep calm  build ai8retriever\\nfetch\\nshort context on the fly indexing\\nabhinav kimothiindexing pi',\n",
       "  'expected_output': 'peline\\nindexing pipeline \\nthe indexing pipeline se'},\n",
       " {'input_text': 'ext on the fly indexing\\nabhinav kimothiindexing pipeline\\nindexing pipeline \\nthe indexing pipeline se',\n",
       "  'expected_output': 'ts up the knowledge source for the rag system it i'},\n",
       " {'input_text': 'peline\\nindexing pipeline \\nthe indexing pipeline sets up the knowledge source for the rag system it i',\n",
       "  'expected_output': 's\\ngenerally considered an offline process however '},\n",
       " {'input_text': 'ts up the knowledge source for the rag system it is\\ngenerally considered an offline process however ',\n",
       "  'expected_output': 'information can also be\\nfetched in real time it in'},\n",
       " {'input_text': 's\\ngenerally considered an offline process however information can also be\\nfetched in real time it in',\n",
       "  'expected_output': 'volves four primary steps\\nloading splitting embedd'},\n",
       " {'input_text': 'information can also be\\nfetched in real time it involves four primary steps\\nloading splitting embedd',\n",
       "  'expected_output': 'ing storing\\nthis step involves\\nextracting\\ninformat'},\n",
       " {'input_text': 'volves four primary steps\\nloading splitting embedding storing\\nthis step involves\\nextracting\\ninformat',\n",
       "  'expected_output': 'ion from\\ndifferent\\nknowledge sources\\na loading the'},\n",
       " {'input_text': 'ing storing\\nthis step involves\\nextracting\\ninformation from\\ndifferent\\nknowledge sources\\na loading the',\n",
       "  'expected_output': 'm into\\ndocumentsthis step involves\\nsplitting\\ndocum'},\n",
       " {'input_text': 'ion from\\ndifferent\\nknowledge sources\\na loading them into\\ndocumentsthis step involves\\nsplitting\\ndocum',\n",
       "  'expected_output': 'ents into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks'},\n",
       " {'input_text': 'm into\\ndocumentsthis step involves\\nsplitting\\ndocuments into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks',\n",
       "  'expected_output': ' are easier\\nto search and to\\nuse in llm context\\nwi'},\n",
       " {'input_text': 'ents into\\nsmaller\\nmanageable\\nchunks smaller\\nchunks are easier\\nto search and to\\nuse in llm context\\nwi',\n",
       "  'expected_output': 'ndowsthis step involves\\nconverting text\\ndocuments '},\n",
       " {'input_text': ' are easier\\nto search and to\\nuse in llm context\\nwindowsthis step involves\\nconverting text\\ndocuments ',\n",
       "  'expected_output': 'into\\nnumerical vectors\\nml models are\\nmathematical\\n'},\n",
       " {'input_text': 'ndowsthis step involves\\nconverting text\\ndocuments into\\nnumerical vectors\\nml models are\\nmathematical\\n',\n",
       "  'expected_output': 'models and\\ntherefore require\\nnumerical datathis st'},\n",
       " {'input_text': 'into\\nnumerical vectors\\nml models are\\nmathematical\\nmodels and\\ntherefore require\\nnumerical datathis st',\n",
       "  'expected_output': 'ep involves\\nstoring the\\nembeddings\\nvectors vectors'},\n",
       " {'input_text': 'models and\\ntherefore require\\nnumerical datathis step involves\\nstoring the\\nembeddings\\nvectors vectors',\n",
       "  'expected_output': '\\nare typically stored\\nin vector\\ndatabases which\\nar'},\n",
       " {'input_text': 'ep involves\\nstoring the\\nembeddings\\nvectors vectors\\nare typically stored\\nin vector\\ndatabases which\\nar',\n",
       "  'expected_output': 'e best suited for\\nsearching\\noffline indexing pipel'},\n",
       " {'input_text': '\\nare typically stored\\nin vector\\ndatabases which\\nare best suited for\\nsearching\\noffline indexing pipel',\n",
       "  'expected_output': 'ines are typically used when a knowledge base\\nwith'},\n",
       " {'input_text': 'e best suited for\\nsearching\\noffline indexing pipelines are typically used when a knowledge base\\nwith',\n",
       "  'expected_output': ' large amount of data is being built for repeated '},\n",
       " {'input_text': 'ines are typically used when a knowledge base\\nwith large amount of data is being built for repeated ',\n",
       "  'expected_output': 'usage eg a\\nnumber of enterprise documents manuals '},\n",
       " {'input_text': ' large amount of data is being built for repeated usage eg a\\nnumber of enterprise documents manuals ',\n",
       "  'expected_output': 'etc \\nin cases where only a fixed small amount of o'},\n",
       " {'input_text': 'usage eg a\\nnumber of enterprise documents manuals etc \\nin cases where only a fixed small amount of o',\n",
       "  'expected_output': 'ne time data is required\\neg a 300 word blog there '},\n",
       " {'input_text': 'etc \\nin cases where only a fixed small amount of one time data is required\\neg a 300 word blog there ',\n",
       "  'expected_output': 'is no need for storing the data the blog\\ntext can '},\n",
       " {'input_text': 'ne time data is required\\neg a 300 word blog there is no need for storing the data the blog\\ntext can ',\n",
       "  'expected_output': 'either be directly passed in the llm context windo'},\n",
       " {'input_text': 'is no need for storing the data the blog\\ntext can either be directly passed in the llm context windo',\n",
       "  'expected_output': 'w or a\\ntemporary vector index can be created\\nuserp'},\n",
       " {'input_text': 'either be directly passed in the llm context window or a\\ntemporary vector index can be created\\nuserp',\n",
       "  'expected_output': 'rompt  prompt  context \\nllm response\\nno search\\nnee'},\n",
       " {'input_text': 'w or a\\ntemporary vector index can be created\\nuserprompt  prompt  context \\nllm response\\nno search\\nnee',\n",
       "  'expected_output': 'ded since\\ncontext is fixed\\nkeep calm  build ai9thi'},\n",
       " {'input_text': 'rompt  prompt  context \\nllm response\\nno search\\nneeded since\\ncontext is fixed\\nkeep calm  build ai9thi',\n",
       "  'expected_output': 's is a good time to introduce two popular framewor'},\n",
       " {'input_text': 'ded since\\ncontext is fixed\\nkeep calm  build ai9this is a good time to introduce two popular framewor',\n",
       "  'expected_output': 'ks that are being used to\\ndevelop llm powered appl'},\n",
       " {'input_text': 's is a good time to introduce two popular frameworks that are being used to\\ndevelop llm powered appl',\n",
       "  'expected_output': 'ications\\nabhinav kimothiindexing pipeline loading '},\n",
       " {'input_text': 'ks that are being used to\\ndevelop llm powered applications\\nabhinav kimothiindexing pipeline loading ',\n",
       "  'expected_output': 'data\\nloading data\\nas weve been discussing the util'},\n",
       " {'input_text': 'ications\\nabhinav kimothiindexing pipeline loading data\\nloading data\\nas weve been discussing the util',\n",
       "  'expected_output': 'ity of rag is to access data for all sorts of\\nsour'},\n",
       " {'input_text': 'data\\nloading data\\nas weve been discussing the utility of rag is to access data for all sorts of\\nsour',\n",
       "  'expected_output': 'ces these sources can be  \\nwebsites  html pages\\ndo'},\n",
       " {'input_text': 'ity of rag is to access data for all sorts of\\nsources these sources can be  \\nwebsites  html pages\\ndo',\n",
       "  'expected_output': 'cuments like word pdf etc\\ncode in python java etc\\n'},\n",
       " {'input_text': 'ces these sources can be  \\nwebsites  html pages\\ndocuments like word pdf etc\\ncode in python java etc\\n',\n",
       "  'expected_output': 'data in json csv etc\\napis\\nfile directories\\ndatabas'},\n",
       " {'input_text': 'cuments like word pdf etc\\ncode in python java etc\\ndata in json csv etc\\napis\\nfile directories\\ndatabas',\n",
       "  'expected_output': 'es\\nand many more\\nthe first step is to extract the '},\n",
       " {'input_text': 'data in json csv etc\\napis\\nfile directories\\ndatabases\\nand many more\\nthe first step is to extract the ',\n",
       "  'expected_output': 'information present in these source locations \\nuse'},\n",
       " {'input_text': 'es\\nand many more\\nthe first step is to extract the information present in these source locations \\nuse',\n",
       "  'expected_output': ' cases good for tasks that\\nrequire text search and'},\n",
       " {'input_text': 'information present in these source locations \\nuse cases good for tasks that\\nrequire text search and',\n",
       "  'expected_output': ' retrieval like\\ninformation retrieval or content\\nd'},\n",
       " {'input_text': ' cases good for tasks that\\nrequire text search and retrieval like\\ninformation retrieval or content\\nd',\n",
       "  'expected_output': 'iscovery\\nfeatures excels in data indexing and\\nlang'},\n",
       " {'input_text': ' retrieval like\\ninformation retrieval or content\\ndiscovery\\nfeatures excels in data indexing and\\nlang',\n",
       "  'expected_output': 'uage model enhancement\\nconnectors provides connect'},\n",
       " {'input_text': 'iscovery\\nfeatures excels in data indexing and\\nlanguage model enhancement\\nconnectors provides connect',\n",
       "  'expected_output': 'ors to\\naccess data from databases\\nexternal apis or'},\n",
       " {'input_text': 'uage model enhancement\\nconnectors provides connectors to\\naccess data from databases\\nexternal apis or',\n",
       "  'expected_output': ' other datasetsuse cases good for applications tha'},\n",
       " {'input_text': 'ors to\\naccess data from databases\\nexternal apis or other datasetsuse cases good for applications tha',\n",
       "  'expected_output': 't\\nneed enhanced ai capabilities like\\nlanguage unde'},\n",
       " {'input_text': ' other datasetsuse cases good for applications that\\nneed enhanced ai capabilities like\\nlanguage unde',\n",
       "  'expected_output': 'rstanding tasks and more\\nsophisticated text genera'},\n",
       " {'input_text': 't\\nneed enhanced ai capabilities like\\nlanguage understanding tasks and more\\nsophisticated text genera',\n",
       "  'expected_output': 'tion\\nfeatures stands out for its versatility\\nand a'},\n",
       " {'input_text': 'rstanding tasks and more\\nsophisticated text generation\\nfeatures stands out for its versatility\\nand a',\n",
       "  'expected_output': 'daptability in building robust\\napplications with l'},\n",
       " {'input_text': 'tion\\nfeatures stands out for its versatility\\nand adaptability in building robust\\napplications with l',\n",
       "  'expected_output': 'lms\\nagents makes creating agents using\\nlarge langu'},\n",
       " {'input_text': 'daptability in building robust\\napplications with llms\\nagents makes creating agents using\\nlarge langu',\n",
       "  'expected_output': 'age models simple through\\ntheir agents api\\nboth fr'},\n",
       " {'input_text': 'lms\\nagents makes creating agents using\\nlarge language models simple through\\ntheir agents api\\nboth fr',\n",
       "  'expected_output': 'ameworks are rapidly evolving and adding new capab'},\n",
       " {'input_text': 'age models simple through\\ntheir agents api\\nboth frameworks are rapidly evolving and adding new capab',\n",
       "  'expected_output': 'ilities every week\\nits not an eitheror situation a'},\n",
       " {'input_text': 'ameworks are rapidly evolving and adding new capabilities every week\\nits not an eitheror situation a',\n",
       "  'expected_output': 'nd you can use both together or neither \\nkeep calm'},\n",
       " {'input_text': 'ilities every week\\nits not an eitheror situation and you can use both together or neither \\nkeep calm',\n",
       "  'expected_output': '  build ai10abhinav kimothiindexing pipeline loadi'},\n",
       " {'input_text': 'nd you can use both together or neither \\nkeep calm  build ai10abhinav kimothiindexing pipeline loadi',\n",
       "  'expected_output': 'ng data\\nexample  loading a youtube video\\ntranscrip'},\n",
       " {'input_text': '  build ai10abhinav kimothiindexing pipeline loading data\\nexample  loading a youtube video\\ntranscrip',\n",
       "  'expected_output': 't using langchain loaders\\nlets begin by sourcing t'},\n",
       " {'input_text': 'ng data\\nexample  loading a youtube video\\ntranscript using langchain loaders\\nlets begin by sourcing t',\n",
       "  'expected_output': 'he transcript from this video  \\ndalle 2 explained '},\n",
       " {'input_text': 't using langchain loaders\\nlets begin by sourcing the transcript from this video  \\ndalle 2 explained ',\n",
       "  'expected_output': 'by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbel'},\n",
       " {'input_text': 'he transcript from this video  \\ndalle 2 explained by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbel',\n",
       "  'expected_output': 'ow is the code using youtubeloader from langchaind'},\n",
       " {'input_text': 'by openai \\nhttpswwwyoutubecomwatchvqtgpskkjfvg\\nbelow is the code using youtubeloader from langchaind',\n",
       "  'expected_output': 'ocumentloaders\\ndocumentpagecontenthave you ever se'},\n",
       " {'input_text': 'ow is the code using youtubeloader from langchaindocumentloaders\\ndocumentpagecontenthave you ever se',\n",
       "  'expected_output': 'en a polar bear\\nplaying bass or a robot painted li'},\n",
       " {'input_text': 'ocumentloaders\\ndocumentpagecontenthave you ever seen a polar bear\\nplaying bass or a robot painted li',\n",
       "  'expected_output': 'ke a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnan'},\n",
       " {'input_text': 'en a polar bear\\nplaying bass or a robot painted like a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnan',\n",
       "  'expected_output': 'd clever systems can work together to make new\\nthi'},\n",
       " {'input_text': 'ke a picasso didnt think so\\ndalle 2 is \\n\\n\\numansnand clever systems can work together to make new\\nthi',\n",
       "  'expected_output': 'ngs  amplifying our creative potential metadatasou'},\n",
       " {'input_text': 'd clever systems can work together to make new\\nthings  amplifying our creative potential metadatasou',\n",
       "  'expected_output': 'rce\\nqtgpskkjfvg title dalle 2 explained descriptio'},\n",
       " {'input_text': 'ngs  amplifying our creative potential metadatasource\\nqtgpskkjfvg title dalle 2 explained descriptio',\n",
       "  'expected_output': 'n unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytim'},\n",
       " {'input_text': 'rce\\nqtgpskkjfvg title dalle 2 explained description unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytim',\n",
       "  'expected_output': 'gcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000'},\n",
       " {'input_text': 'n unknown\\nviewcount 853564 thumbnailurl\\nhttpsiytimgcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000',\n",
       "  'expected_output': '000 length 167 author openailangchain document loa'},\n",
       " {'input_text': 'gcomviqtgpskkjfvghq720jpg publishdate\\n20220406 000000 length 167 author openailangchain document loa',\n",
       "  'expected_output': 'der  youtubeloader\\nloader object\\nthe document obje'},\n",
       " {'input_text': '000 length 167 author openailangchain document loader  youtubeloader\\nloader object\\nthe document obje',\n",
       "  'expected_output': 'ct contains the pagecontent which is the transcrip'},\n",
       " {'input_text': 'der  youtubeloader\\nloader object\\nthe document object contains the pagecontent which is the transcrip',\n",
       "  'expected_output': 't extracted\\nfrom the youtube video as well as the '},\n",
       " {'input_text': 'ct contains the pagecontent which is the transcript extracted\\nfrom the youtube video as well as the ',\n",
       "  'expected_output': 'metadata description\\nkeep calm  build ai11abhinav '},\n",
       " {'input_text': 't extracted\\nfrom the youtube video as well as the metadata description\\nkeep calm  build ai11abhinav ',\n",
       "  'expected_output': 'kimothiindexing pipeline loading data\\nexample  loa'},\n",
       " {'input_text': 'metadata description\\nkeep calm  build ai11abhinav kimothiindexing pipeline loading data\\nexample  loa',\n",
       "  'expected_output': 'ding a webpage text using\\nllamaindex reader\\nthis i'},\n",
       " {'input_text': 'kimothiindexing pipeline loading data\\nexample  loading a webpage text using\\nllamaindex reader\\nthis i',\n",
       "  'expected_output': 's a blog published on medium \\nwhat is a finetuned '},\n",
       " {'input_text': 'ding a webpage text using\\nllamaindex reader\\nthis is a blog published on medium \\nwhat is a finetuned ',\n",
       "  'expected_output': 'llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67'},\n",
       " {'input_text': 's a blog published on medium \\nwhat is a finetuned llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67',\n",
       "  'expected_output': 'bf0b5df081\\nbelow is the code using simplewebpagere'},\n",
       " {'input_text': 'llm\\nhttpsmediumcommlearningaiwhatisafinetunedllm67bf0b5df081\\nbelow is the code using simplewebpagere',\n",
       "  'expected_output': 'ader from llamahub\\ndocumentid17761da46a3a4ce58590c'},\n",
       " {'input_text': 'bf0b5df081\\nbelow is the code using simplewebpagereader from llamahub\\ndocumentid17761da46a3a4ce58590c',\n",
       "  'expected_output': '65ee446788f\\nembeddingnone metadata excludedembedme'},\n",
       " {'input_text': 'ader from llamahub\\ndocumentid17761da46a3a4ce58590c65ee446788f\\nembeddingnone metadata excludedembedme',\n",
       "  'expected_output': 'tadatakeys\\nexcludedllmmetadatakeys relationships\\nh'},\n",
       " {'input_text': '65ee446788f\\nembeddingnone metadata excludedembedmetadatakeys\\nexcludedllmmetadatakeys relationships\\nh',\n",
       "  'expected_output': 'ash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca25'},\n",
       " {'input_text': 'tadatakeys\\nexcludedllmmetadatakeys relationships\\nhash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca25',\n",
       "  'expected_output': '6fab9fa363e0\\n9ed85 textdoctype htmlhtml langenhead'},\n",
       " {'input_text': 'ash6471b3ffe4d3abb1aba2ca99d1d0448e2c3cbd157ddca256fab9fa363e0\\n9ed85 textdoctype htmlhtml langenhead',\n",
       "  'expected_output': 'title data\\nrhtruewhat is a finetuned llm finetunin'},\n",
       " {'input_text': '6fab9fa363e0\\n9ed85 textdoctype htmlhtml langenheadtitle data\\nrhtruewhat is a finetuned llm finetunin',\n",
       "  'expected_output': 'g large language models\\n by abhinav kimothi  \\n\\nbod'},\n",
       " {'input_text': 'title data\\nrhtruewhat is a finetuned llm finetuning large language models\\n by abhinav kimothi  \\n\\nbod',\n",
       "  'expected_output': 'yhtml startcharidxnone endcharidxnone\\ntexttemplate'},\n",
       " {'input_text': 'g large language models\\n by abhinav kimothi  \\n\\nbodyhtml startcharidxnone endcharidxnone\\ntexttemplate',\n",
       "  'expected_output': 'metadatastrnncontent metadatatemplatekey\\nvalue met'},\n",
       " {'input_text': 'yhtml startcharidxnone endcharidxnone\\ntexttemplatemetadatastrnncontent metadatatemplatekey\\nvalue met',\n",
       "  'expected_output': 'adataseperatornllamaindex llamahub web page reader'},\n",
       " {'input_text': 'metadatastrnncontent metadatatemplatekey\\nvalue metadataseperatornllamaindex llamahub web page reader',\n",
       "  'expected_output': '\\nthe llamaindex document object contains more  att'},\n",
       " {'input_text': 'adataseperatornllamaindex llamahub web page reader\\nthe llamaindex document object contains more  att',\n",
       "  'expected_output': 'ributes than a langchain\\ndocument  apart from text'},\n",
       " {'input_text': '\\nthe llamaindex document object contains more  attributes than a langchain\\ndocument  apart from text',\n",
       "  'expected_output': ' and metadata it also has id templates and other\\nc'},\n",
       " {'input_text': 'ributes than a langchain\\ndocument  apart from text and metadata it also has id templates and other\\nc',\n",
       "  'expected_output': 'ustomizations availableloader object\\nkeep calm  bu'},\n",
       " {'input_text': ' and metadata it also has id templates and other\\ncustomizations availableloader object\\nkeep calm  bu',\n",
       "  'expected_output': 'ild ai12abhinav kimothiindexing pipeline loading d'},\n",
       " {'input_text': 'ustomizations availableloader object\\nkeep calm  build ai12abhinav kimothiindexing pipeline loading d',\n",
       "  'expected_output': 'ata\\nboth langchain and llamaindex offer loader int'},\n",
       " {'input_text': 'ild ai12abhinav kimothiindexing pipeline loading data\\nboth langchain and llamaindex offer loader int',\n",
       "  'expected_output': 'egrations with more than a\\nhundred data sources an'},\n",
       " {'input_text': 'ata\\nboth langchain and llamaindex offer loader integrations with more than a\\nhundred data sources an',\n",
       "  'expected_output': 'd the list keeps on growing\\nlangchain document loa'},\n",
       " {'input_text': 'egrations with more than a\\nhundred data sources and the list keeps on growing\\nlangchain document loa',\n",
       "  'expected_output': 'ders llamahub data loaders\\nllamaindex provides dat'},\n",
       " {'input_text': 'd the list keeps on growing\\nlangchain document loaders llamahub data loaders\\nllamaindex provides dat',\n",
       "  'expected_output': 'a loaders via\\nllamahublangchain provides integrati'},\n",
       " {'input_text': 'ders llamahub data loaders\\nllamaindex provides data loaders via\\nllamahublangchain provides integrati',\n",
       "  'expected_output': 'ons with a\\nvariety of sources\\nthese document loade'},\n",
       " {'input_text': 'a loaders via\\nllamahublangchain provides integrations with a\\nvariety of sources\\nthese document loade',\n",
       "  'expected_output': 'rs are particularly helpful in quickly making conn'},\n",
       " {'input_text': 'ons with a\\nvariety of sources\\nthese document loaders are particularly helpful in quickly making conn',\n",
       "  'expected_output': 'ections\\nand accessing information for specific sou'},\n",
       " {'input_text': 'rs are particularly helpful in quickly making connections\\nand accessing information for specific sou',\n",
       "  'expected_output': 'rces custom loaders can also be\\ndeveloped\\nit is wo'},\n",
       " {'input_text': 'ections\\nand accessing information for specific sources custom loaders can also be\\ndeveloped\\nit is wo',\n",
       "  'expected_output': 'rthwhile exploring documentation for both\\nloading '},\n",
       " {'input_text': 'rces custom loaders can also be\\ndeveloped\\nit is worthwhile exploring documentation for both\\nloading ',\n",
       "  'expected_output': 'documents from a list of sources may turn out to b'},\n",
       " {'input_text': 'rthwhile exploring documentation for both\\nloading documents from a list of sources may turn out to b',\n",
       "  'expected_output': 'e a complicated\\nprocess make sure to plan for all '},\n",
       " {'input_text': 'documents from a list of sources may turn out to be a complicated\\nprocess make sure to plan for all ',\n",
       "  'expected_output': 'the sources and loaders in advance\\nmore often than'},\n",
       " {'input_text': 'e a complicated\\nprocess make sure to plan for all the sources and loaders in advance\\nmore often than',\n",
       "  'expected_output': ' naught transformationscleanups to the loaded data'},\n",
       " {'input_text': 'the sources and loaders in advance\\nmore often than naught transformationscleanups to the loaded data',\n",
       "  'expected_output': ' will\\nbe required like removing duplicate content '},\n",
       " {'input_text': ' naught transformationscleanups to the loaded data will\\nbe required like removing duplicate content ',\n",
       "  'expected_output': 'html parsing etc langchain\\nalso provides a variety'},\n",
       " {'input_text': ' will\\nbe required like removing duplicate content html parsing etc langchain\\nalso provides a variety',\n",
       "  'expected_output': ' of document transformershttpspythonlangchaincomdo'},\n",
       " {'input_text': 'html parsing etc langchain\\nalso provides a variety of document transformershttpspythonlangchaincomdo',\n",
       "  'expected_output': 'csgetstartedintroductionhttpsdocsllamaindexaiensta'},\n",
       " {'input_text': ' of document transformershttpspythonlangchaincomdocsgetstartedintroductionhttpsdocsllamaindexaiensta',\n",
       "  'expected_output': 'ble llamaindex\\nlangchain\\nkeep calm  build ai13abhi'},\n",
       " {'input_text': 'csgetstartedintroductionhttpsdocsllamaindexaienstable llamaindex\\nlangchain\\nkeep calm  build ai13abhi',\n",
       "  'expected_output': 'nav kimothiindexing pipeline document splitting\\ndo'},\n",
       " {'input_text': 'ble llamaindex\\nlangchain\\nkeep calm  build ai13abhinav kimothiindexing pipeline document splitting\\ndo',\n",
       "  'expected_output': 'cument splitting\\nonce the data is loaded the next '},\n",
       " {'input_text': 'nav kimothiindexing pipeline document splitting\\ndocument splitting\\nonce the data is loaded the next ',\n",
       "  'expected_output': 'step in the indexing pipeline is splitting the\\ndoc'},\n",
       " {'input_text': 'cument splitting\\nonce the data is loaded the next step in the indexing pipeline is splitting the\\ndoc',\n",
       "  'expected_output': 'uments into manageable chunks the question arises '},\n",
       " {'input_text': 'step in the indexing pipeline is splitting the\\ndocuments into manageable chunks the question arises ',\n",
       "  'expected_output': 'around the need of this\\nstep why is splitting of d'},\n",
       " {'input_text': 'uments into manageable chunks the question arises around the need of this\\nstep why is splitting of d',\n",
       "  'expected_output': 'ocuments necessary there are two reasons for that '},\n",
       " {'input_text': 'around the need of this\\nstep why is splitting of documents necessary there are two reasons for that ',\n",
       "  'expected_output': ' \\nease of search context window size\\nlarge chunks '},\n",
       " {'input_text': 'ocuments necessary there are two reasons for that  \\nease of search context window size\\nlarge chunks ',\n",
       "  'expected_output': 'of data are harder to\\nsearch over splitting data i'},\n",
       " {'input_text': ' \\nease of search context window size\\nlarge chunks of data are harder to\\nsearch over splitting data i',\n",
       "  'expected_output': 'nto\\nsmaller chunks therefore helps in\\nbetter index'},\n",
       " {'input_text': 'of data are harder to\\nsearch over splitting data into\\nsmaller chunks therefore helps in\\nbetter index',\n",
       "  'expected_output': 'ationllms allow only a finite number of\\ntokens in '},\n",
       " {'input_text': 'nto\\nsmaller chunks therefore helps in\\nbetter indexationllms allow only a finite number of\\ntokens in ',\n",
       "  'expected_output': 'prompts and completions the\\ncontext therefore cann'},\n",
       " {'input_text': 'ationllms allow only a finite number of\\ntokens in prompts and completions the\\ncontext therefore cann',\n",
       "  'expected_output': 'ot be larger than\\nwhat the context window permits\\n'},\n",
       " {'input_text': 'prompts and completions the\\ncontext therefore cannot be larger than\\nwhat the context window permits\\n',\n",
       "  'expected_output': 'chunking strategies \\nwhile splitting documents int'},\n",
       " {'input_text': 'ot be larger than\\nwhat the context window permits\\nchunking strategies \\nwhile splitting documents int',\n",
       "  'expected_output': 'o chunks might sound a simple concept there are\\nce'},\n",
       " {'input_text': 'chunking strategies \\nwhile splitting documents into chunks might sound a simple concept there are\\nce',\n",
       "  'expected_output': 'rtain best practices that researchers have discove'},\n",
       " {'input_text': 'o chunks might sound a simple concept there are\\ncertain best practices that researchers have discove',\n",
       "  'expected_output': 'red there are a few\\nconsiderations that may influe'},\n",
       " {'input_text': 'rtain best practices that researchers have discovered there are a few\\nconsiderations that may influe',\n",
       "  'expected_output': 'nce the overall chunking strategy \\nnature of conte'},\n",
       " {'input_text': 'red there are a few\\nconsiderations that may influence the overall chunking strategy \\nnature of conte',\n",
       "  'expected_output': 'nt\\nconsider whether you are working with lengthy d'},\n",
       " {'input_text': 'nce the overall chunking strategy \\nnature of content\\nconsider whether you are working with lengthy d',\n",
       "  'expected_output': 'ocuments such as articles or\\nbooks or shorter cont'},\n",
       " {'input_text': 'nt\\nconsider whether you are working with lengthy documents such as articles or\\nbooks or shorter cont',\n",
       "  'expected_output': 'ent like tweets or instant messages the chosen mod'},\n",
       " {'input_text': 'ocuments such as articles or\\nbooks or shorter content like tweets or instant messages the chosen mod',\n",
       "  'expected_output': 'el for\\nyour goal and consequently the appropriate '},\n",
       " {'input_text': 'ent like tweets or instant messages the chosen model for\\nyour goal and consequently the appropriate ',\n",
       "  'expected_output': 'chunking strategy depend on your\\nresponse\\nembeddin'},\n",
       " {'input_text': 'el for\\nyour goal and consequently the appropriate chunking strategy depend on your\\nresponse\\nembeddin',\n",
       "  'expected_output': 'g model being used\\nwe will discuss embeddings in d'},\n",
       " {'input_text': 'chunking strategy depend on your\\nresponse\\nembedding model being used\\nwe will discuss embeddings in d',\n",
       "  'expected_output': 'etail in the next section but the choice of\\nembedd'},\n",
       " {'input_text': 'g model being used\\nwe will discuss embeddings in detail in the next section but the choice of\\nembedd',\n",
       "  'expected_output': 'ing model also dictates the chunking strategy some'},\n",
       " {'input_text': 'etail in the next section but the choice of\\nembedding model also dictates the chunking strategy some',\n",
       "  'expected_output': ' models perform\\nbetter with chunks of specific len'},\n",
       " {'input_text': 'ing model also dictates the chunking strategy some models perform\\nbetter with chunks of specific len',\n",
       "  'expected_output': 'gth\\nexpected length and complexity of user queries'},\n",
       " {'input_text': ' models perform\\nbetter with chunks of specific length\\nexpected length and complexity of user queries',\n",
       "  'expected_output': '\\ndetermine whether the content will be short and s'},\n",
       " {'input_text': 'gth\\nexpected length and complexity of user queries\\ndetermine whether the content will be short and s',\n",
       "  'expected_output': 'pecific or long and complex\\nthis factor will influ'},\n",
       " {'input_text': '\\ndetermine whether the content will be short and specific or long and complex\\nthis factor will influ',\n",
       "  'expected_output': 'ence the approach to chunking the content ensuring'},\n",
       " {'input_text': 'pecific or long and complex\\nthis factor will influence the approach to chunking the content ensuring',\n",
       "  'expected_output': ' a closer\\ncorrelation between the embedded query a'},\n",
       " {'input_text': 'ence the approach to chunking the content ensuring a closer\\ncorrelation between the embedded query a',\n",
       "  'expected_output': 'nd the embedded chunks\\napplication specific requir'},\n",
       " {'input_text': ' a closer\\ncorrelation between the embedded query and the embedded chunks\\napplication specific requir',\n",
       "  'expected_output': 'ements\\nthe application use case such as semantic s'},\n",
       " {'input_text': 'nd the embedded chunks\\napplication specific requirements\\nthe application use case such as semantic s',\n",
       "  'expected_output': 'earch question answering\\nsummarization or other pu'},\n",
       " {'input_text': 'ements\\nthe application use case such as semantic search question answering\\nsummarization or other pu',\n",
       "  'expected_output': 'rposes will also determine how text should be\\nchun'},\n",
       " {'input_text': 'earch question answering\\nsummarization or other purposes will also determine how text should be\\nchun',\n",
       "  'expected_output': 'ked if the results need to be input into another l'},\n",
       " {'input_text': 'rposes will also determine how text should be\\nchunked if the results need to be input into another l',\n",
       "  'expected_output': 'anguage model with a token\\nlimit it is crucial to '},\n",
       " {'input_text': 'ked if the results need to be input into another language model with a token\\nlimit it is crucial to ',\n",
       "  'expected_output': 'factor this into your decisionmaking process\\nkeep '},\n",
       " {'input_text': 'anguage model with a token\\nlimit it is crucial to factor this into your decisionmaking process\\nkeep ',\n",
       "  'expected_output': 'calm  build ai14abhinav kimothiindexing pipeline d'},\n",
       " {'input_text': 'factor this into your decisionmaking process\\nkeep calm  build ai14abhinav kimothiindexing pipeline d',\n",
       "  'expected_output': 'ocument splitting\\nchunking methods \\ndepending on t'},\n",
       " {'input_text': 'calm  build ai14abhinav kimothiindexing pipeline document splitting\\nchunking methods \\ndepending on t',\n",
       "  'expected_output': 'he aforementioned considerations a number of text '},\n",
       " {'input_text': 'ocument splitting\\nchunking methods \\ndepending on the aforementioned considerations a number of text ',\n",
       "  'expected_output': 'splitters are\\navailable at a broad level text spli'},\n",
       " {'input_text': 'he aforementioned considerations a number of text splitters are\\navailable at a broad level text spli',\n",
       "  'expected_output': 'tters operate in the following manner\\ndivide the t'},\n",
       " {'input_text': 'splitters are\\navailable at a broad level text splitters operate in the following manner\\ndivide the t',\n",
       "  'expected_output': 'ext into compact semantically meaningful units oft'},\n",
       " {'input_text': 'tters operate in the following manner\\ndivide the text into compact semantically meaningful units oft',\n",
       "  'expected_output': 'en sentences \\nmerge these smaller units into large'},\n",
       " {'input_text': 'ext into compact semantically meaningful units often sentences \\nmerge these smaller units into large',\n",
       "  'expected_output': 'r chunks until a specific size is achieved\\nmeasure'},\n",
       " {'input_text': 'en sentences \\nmerge these smaller units into larger chunks until a specific size is achieved\\nmeasure',\n",
       "  'expected_output': 'd by a length function \\nupon reaching the predeter'},\n",
       " {'input_text': 'r chunks until a specific size is achieved\\nmeasured by a length function \\nupon reaching the predeter',\n",
       "  'expected_output': 'mined size treat that chunk as an independent\\nsegm'},\n",
       " {'input_text': 'd by a length function \\nupon reaching the predetermined size treat that chunk as an independent\\nsegm',\n",
       "  'expected_output': 'ent of text  thereafter start creating a new text '},\n",
       " {'input_text': 'mined size treat that chunk as an independent\\nsegment of text  thereafter start creating a new text ',\n",
       "  'expected_output': 'chunk with some degree\\nof overlap to maintain cont'},\n",
       " {'input_text': 'ent of text  thereafter start creating a new text chunk with some degree\\nof overlap to maintain cont',\n",
       "  'expected_output': 'extual continuity between chunks\\ntwo areas to focu'},\n",
       " {'input_text': 'chunk with some degree\\nof overlap to maintain contextual continuity between chunks\\ntwo areas to focu',\n",
       "  'expected_output': 's on therefore are \\nhow the text is split how the '},\n",
       " {'input_text': 'extual continuity between chunks\\ntwo areas to focus on therefore are \\nhow the text is split how the ',\n",
       "  'expected_output': 'chunk size is measured\\na very common approach is w'},\n",
       " {'input_text': 's on therefore are \\nhow the text is split how the chunk size is measured\\na very common approach is w',\n",
       "  'expected_output': 'here we predetermine the size of the text chunks \\n'},\n",
       " {'input_text': 'chunk size is measured\\na very common approach is where we predetermine the size of the text chunks \\n',\n",
       "  'expected_output': 'additionally we can specify the overlap between ch'},\n",
       " {'input_text': 'here we predetermine the size of the text chunks \\nadditionally we can specify the overlap between ch',\n",
       "  'expected_output': 'unks remember overlap is\\npreferred to maintain con'},\n",
       " {'input_text': 'additionally we can specify the overlap between chunks remember overlap is\\npreferred to maintain con',\n",
       "  'expected_output': 'textual continuity between chunks \\nthis approach i'},\n",
       " {'input_text': 'unks remember overlap is\\npreferred to maintain contextual continuity between chunks \\nthis approach i',\n",
       "  'expected_output': 's simple and cheap and is therefore widely used le'},\n",
       " {'input_text': 'textual continuity between chunks \\nthis approach is simple and cheap and is therefore widely used le',\n",
       "  'expected_output': 'ts look at\\nsome examples  \\nkeep calm  build ai15ab'},\n",
       " {'input_text': 's simple and cheap and is therefore widely used lets look at\\nsome examples  \\nkeep calm  build ai15ab',\n",
       "  'expected_output': 'hinav kimothiindexing pipeline document splitting\\n'},\n",
       " {'input_text': 'ts look at\\nsome examples  \\nkeep calm  build ai15abhinav kimothiindexing pipeline document splitting\\n',\n",
       "  'expected_output': 'split by character\\nin this approach the text is sp'},\n",
       " {'input_text': 'hinav kimothiindexing pipeline document splitting\\nsplit by character\\nin this approach the text is sp',\n",
       "  'expected_output': 'lit based on a character and the chunk size is\\nmea'},\n",
       " {'input_text': 'split by character\\nin this approach the text is split based on a character and the chunk size is\\nmea',\n",
       "  'expected_output': 'sured by the number of characters\\nexample text  al'},\n",
       " {'input_text': 'lit based on a character and the chunk size is\\nmeasured by the number of characters\\nexample text  al',\n",
       "  'expected_output': 'iceinwonderlandtxt the book in txt format\\nusing la'},\n",
       " {'input_text': 'sured by the number of characters\\nexample text  aliceinwonderlandtxt the book in txt format\\nusing la',\n",
       "  'expected_output': 'ngchains charactertextsplitter\\ntexts0 \\ntitle alice'},\n",
       " {'input_text': 'iceinwonderlandtxt the book in txt format\\nusing langchains charactertextsplitter\\ntexts0 \\ntitle alice',\n",
       "  'expected_output': 's adventures in wonderlandnauthor lewis carrollnnn'},\n",
       " {'input_text': 'ngchains charactertextsplitter\\ntexts0 \\ntitle alices adventures in wonderlandnauthor lewis carrollnnn',\n",
       "  'expected_output': ' chapter i n down the\\nrabbithole nn alice was begi'},\n",
       " {'input_text': 's adventures in wonderlandnauthor lewis carrollnnn chapter i n down the\\nrabbithole nn alice was begi',\n",
       "  'expected_output': 'nning to get very tired of sitting by her sisterno'},\n",
       " {'input_text': ' chapter i n down the\\nrabbithole nn alice was beginning to get very tired of sitting by her sisterno',\n",
       "  'expected_output': 'n the bank and of\\nhaving nothing to do once or twi'},\n",
       " {'input_text': 'nning to get very tired of sitting by her sisternon the bank and of\\nhaving nothing to do once or twi',\n",
       "  'expected_output': 'ce she hadnpeeped into the book her sister was rea'},\n",
       " {'input_text': 'n the bank and of\\nhaving nothing to do once or twice she hadnpeeped into the book her sister was rea',\n",
       "  'expected_output': 'ding but it\\nhad nonpictures or conversations in it'},\n",
       " {'input_text': 'ce she hadnpeeped into the book her sister was reading but it\\nhad nonpictures or conversations in it',\n",
       "  'expected_output': ' and what is the use of a booknthought alice witho'},\n",
       " {'input_text': 'ding but it\\nhad nonpictures or conversations in it and what is the use of a booknthought alice witho',\n",
       "  'expected_output': 'ut\\npictures or conversationnn so she was consideri'},\n",
       " {'input_text': ' and what is the use of a booknthought alice without\\npictures or conversationnn so she was consideri',\n",
       "  'expected_output': 'ng in her own mind as well as she couldnfor\\nthe ho'},\n",
       " {'input_text': 'ut\\npictures or conversationnn so she was considering in her own mind as well as she couldnfor\\nthe ho',\n",
       "  'expected_output': 't day made her feel very sleepy and stupid whether'},\n",
       " {'input_text': 'ng in her own mind as well as she couldnfor\\nthe hot day made her feel very sleepy and stupid whether',\n",
       "  'expected_output': 'nthe pleasure of making a daisychain\\nwould be wort'},\n",
       " {'input_text': 't day made her feel very sleepy and stupid whethernthe pleasure of making a daisychain\\nwould be wort',\n",
       "  'expected_output': 'h the troublenof getting up and picking the daisie'},\n",
       " {'input_text': 'nthe pleasure of making a daisychain\\nwould be worth the troublenof getting up and picking the daisie',\n",
       "  'expected_output': 's when suddenly a whitenrabbit\\nwith pink eyes ran '},\n",
       " {'input_text': 'h the troublenof getting up and picking the daisies when suddenly a whitenrabbit\\nwith pink eyes ran ',\n",
       "  'expected_output': 'close by hernn there was nothing so very remarkabl'},\n",
       " {'input_text': 's when suddenly a whitenrabbit\\nwith pink eyes ran close by hernn there was nothing so very remarkabl',\n",
       "  'expected_output': 'e in that nor did\\nalicenthink it so very much out '},\n",
       " {'input_text': 'close by hernn there was nothing so very remarkable in that nor did\\nalicenthink it so very much out ',\n",
       "  'expected_output': 'of the way to hear the rabbit say tonitself oh dea'},\n",
       " {'input_text': 'e in that nor did\\nalicenthink it so very much out of the way to hear the rabbit say tonitself oh dea',\n",
       "  'expected_output': 'r oh dear i\\nshall be late when she thoughtnit over'},\n",
       " {'input_text': 'of the way to hear the rabbit say tonitself oh dear oh dear i\\nshall be late when she thoughtnit over',\n",
       "  'expected_output': ' afterwards it occurred to her that she ought to\\nh'},\n",
       " {'input_text': 'r oh dear i\\nshall be late when she thoughtnit over afterwards it occurred to her that she ought to\\nh',\n",
       "  'expected_output': 'avenwondered at this but at the time it all seemed'},\n",
       " {'input_text': ' afterwards it occurred to her that she ought to\\nhavenwondered at this but at the time it all seemed',\n",
       "  'expected_output': ' quite naturalnbut when the rabbit actually\\ntook a'},\n",
       " {'input_text': 'avenwondered at this but at the time it all seemed quite naturalnbut when the rabbit actually\\ntook a',\n",
       "  'expected_output': ' watch out of its waistcoatnpocket and looked at i'},\n",
       " {'input_text': ' quite naturalnbut when the rabbit actually\\ntook a watch out of its waistcoatnpocket and looked at i',\n",
       "  'expected_output': 't and then hurried on alice\\nstarted tonher feet fo'},\n",
       " {'input_text': ' watch out of its waistcoatnpocket and looked at it and then hurried on alice\\nstarted tonher feet fo',\n",
       "  'expected_output': 'r it flashed across her mind that she had nevernbe'},\n",
       " {'input_text': 't and then hurried on alice\\nstarted tonher feet for it flashed across her mind that she had nevernbe',\n",
       "  'expected_output': 'fore seen a rabbit with\\neither a waistcoatpocket o'},\n",
       " {'input_text': 'r it flashed across her mind that she had nevernbefore seen a rabbit with\\neither a waistcoatpocket o',\n",
       "  'expected_output': 'r a watch tontake out of it and burning with curio'},\n",
       " {'input_text': 'fore seen a rabbit with\\neither a waistcoatpocket or a watch tontake out of it and burning with curio',\n",
       "  'expected_output': 'sity she ran across\\nthenfield after it and fortuna'},\n",
       " {'input_text': 'r a watch tontake out of it and burning with curiosity she ran across\\nthenfield after it and fortuna',\n",
       "  'expected_output': 'tely was just in time to see it popndown a large r'},\n",
       " {'input_text': 'sity she ran across\\nthenfield after it and fortunately was just in time to see it popndown a large r',\n",
       "  'expected_output': 'abbithole under the\\nhedgenn in another moment down'},\n",
       " {'input_text': 'tely was just in time to see it popndown a large rabbithole under the\\nhedgenn in another moment down',\n",
       "  'expected_output': ' went alice after it never oncenconsidering how in'},\n",
       " {'input_text': 'abbithole under the\\nhedgenn in another moment down went alice after it never oncenconsidering how in',\n",
       "  'expected_output': ' the world\\nshe was to get out againnn the rabbitho'},\n",
       " {'input_text': ' went alice after it never oncenconsidering how in the world\\nshe was to get out againnn the rabbitho',\n",
       "  'expected_output': 'le went straight on like a tunnel for some waynand'},\n",
       " {'input_text': ' the world\\nshe was to get out againnn the rabbithole went straight on like a tunnel for some waynand',\n",
       "  'expected_output': '\\nthen dipped suddenly down so suddenly that alice '},\n",
       " {'input_text': 'le went straight on like a tunnel for some waynand\\nthen dipped suddenly down so suddenly that alice ',\n",
       "  'expected_output': 'had not anmoment to think about stopping\\nherself b'},\n",
       " {'input_text': '\\nthen dipped suddenly down so suddenly that alice had not anmoment to think about stopping\\nherself b',\n",
       "  'expected_output': 'efore she found herselfnfalling down a very deep w'},\n",
       " {'input_text': 'had not anmoment to think about stopping\\nherself before she found herselfnfalling down a very deep w',\n",
       "  'expected_output': 'ell\\ntexts1\\nin another moment down went alice after'},\n",
       " {'input_text': 'efore she found herselfnfalling down a very deep well\\ntexts1\\nin another moment down went alice after',\n",
       "  'expected_output': ' it never oncenconsidering how in the world she wa'},\n",
       " {'input_text': 'ell\\ntexts1\\nin another moment down went alice after it never oncenconsidering how in the world she wa',\n",
       "  'expected_output': 's to\\nget out againnn the rabbithole went straight '},\n",
       " {'input_text': ' it never oncenconsidering how in the world she was to\\nget out againnn the rabbithole went straight ',\n",
       "  'expected_output': 'on like a tunnel for some waynand then dipped\\nsudd'},\n",
       " {'input_text': 's to\\nget out againnn the rabbithole went straight on like a tunnel for some waynand then dipped\\nsudd',\n",
       "  'expected_output': 'enly down so suddenly that alice had not anmoment '},\n",
       " {'input_text': 'on like a tunnel for some waynand then dipped\\nsuddenly down so suddenly that alice had not anmoment ',\n",
       "  'expected_output': 'to think about stopping herself before\\nshe found h'},\n",
       " {'input_text': 'enly down so suddenly that alice had not anmoment to think about stopping herself before\\nshe found h',\n",
       "  'expected_output': 'erselfnfalling down a very deep wellnn either the '},\n",
       " {'input_text': 'to think about stopping herself before\\nshe found herselfnfalling down a very deep wellnn either the ',\n",
       "  'expected_output': 'well was very deep or she fell very\\nslowly for she'},\n",
       " {'input_text': 'erselfnfalling down a very deep wellnn either the well was very deep or she fell very\\nslowly for she',\n",
       "  'expected_output': 'nhad plenty of time as she went down to look about'},\n",
       " {'input_text': 'well was very deep or she fell very\\nslowly for shenhad plenty of time as she went down to look about',\n",
       "  'expected_output': ' her and tonwonder what was\\ngoing to happen next f'},\n",
       " {'input_text': 'nhad plenty of time as she went down to look about her and tonwonder what was\\ngoing to happen next f',\n",
       "  'expected_output': 'irst she tried to lookndown and make out what she '},\n",
       " {'input_text': ' her and tonwonder what was\\ngoing to happen next first she tried to lookndown and make out what she ',\n",
       "  'expected_output': 'was coming to but it\\nwas too dark tonsee anything '},\n",
       " {'input_text': 'irst she tried to lookndown and make out what she was coming to but it\\nwas too dark tonsee anything ',\n",
       "  'expected_output': 'then she looked at the sides of the well andnnotic'},\n",
       " {'input_text': 'was coming to but it\\nwas too dark tonsee anything then she looked at the sides of the well andnnotic',\n",
       "  'expected_output': 'ed that they were\\nfilled with cupboards and booksh'},\n",
       " {'input_text': 'then she looked at the sides of the well andnnoticed that they were\\nfilled with cupboards and booksh',\n",
       "  'expected_output': 'elvesnhere and there she saw maps and pictures hun'},\n",
       " {'input_text': 'ed that they were\\nfilled with cupboards and bookshelvesnhere and there she saw maps and pictures hun',\n",
       "  'expected_output': 'g upon\\npegs shentook down a jar from one of the sh'},\n",
       " {'input_text': 'elvesnhere and there she saw maps and pictures hung upon\\npegs shentook down a jar from one of the sh',\n",
       "  'expected_output': 'elves as she passed it wasnlabelled orange\\nmarmala'},\n",
       " {'input_text': 'g upon\\npegs shentook down a jar from one of the shelves as she passed it wasnlabelled orange\\nmarmala',\n",
       "  'expected_output': 'de but to her great disappointment itnwas empty sh'},\n",
       " {'input_text': 'elves as she passed it wasnlabelled orange\\nmarmalade but to her great disappointment itnwas empty sh',\n",
       "  'expected_output': 'e did not like to drop the jar for\\nfear of killing'},\n",
       " {'input_text': 'de but to her great disappointment itnwas empty she did not like to drop the jar for\\nfear of killing',\n",
       "  'expected_output': 'nsomebody so managed to put it into one of the cup'},\n",
       " {'input_text': 'e did not like to drop the jar for\\nfear of killingnsomebody so managed to put it into one of the cup',\n",
       "  'expected_output': 'boards as shenfell past itoverlapchunk 1\\nchunk 2\\nk'},\n",
       " {'input_text': 'nsomebody so managed to put it into one of the cupboards as shenfell past itoverlapchunk 1\\nchunk 2\\nk',\n",
       "  'expected_output': 'eep calm  build ai16abhinav kimothiindexing pipeli'},\n",
       " {'input_text': 'boards as shenfell past itoverlapchunk 1\\nchunk 2\\nkeep calm  build ai16abhinav kimothiindexing pipeli',\n",
       "  'expected_output': 'ne document splitting\\nlets find out how many chunk'},\n",
       " {'input_text': 'eep calm  build ai16abhinav kimothiindexing pipeline document splitting\\nlets find out how many chunk',\n",
       "  'expected_output': 's were created \\ntotal number of chunks created  93'},\n",
       " {'input_text': 'ne document splitting\\nlets find out how many chunks were created \\ntotal number of chunks created  93',\n",
       "  'expected_output': '\\nlength of the first chunk is  1777 characters\\nlen'},\n",
       " {'input_text': 's were created \\ntotal number of chunks created  93\\nlength of the first chunk is  1777 characters\\nlen',\n",
       "  'expected_output': 'gth of the last chunk is  816 characters\\nrecursive'},\n",
       " {'input_text': '\\nlength of the first chunk is  1777 characters\\nlength of the last chunk is  816 characters\\nrecursive',\n",
       "  'expected_output': ' split by character\\na subtle variation to splittin'},\n",
       " {'input_text': 'gth of the last chunk is  816 characters\\nrecursive split by character\\na subtle variation to splittin',\n",
       "  'expected_output': 'g by character is recursive split the only differe'},\n",
       " {'input_text': ' split by character\\na subtle variation to splitting by character is recursive split the only differe',\n",
       "  'expected_output': 'nce\\nis that instead of a single character used for'},\n",
       " {'input_text': 'g by character is recursive split the only difference\\nis that instead of a single character used for',\n",
       "  'expected_output': ' splitting this technique uses a list of\\ncharacter'},\n",
       " {'input_text': 'nce\\nis that instead of a single character used for splitting this technique uses a list of\\ncharacter',\n",
       "  'expected_output': 's and tries to split hierarchically till the chunk'},\n",
       " {'input_text': ' splitting this technique uses a list of\\ncharacters and tries to split hierarchically till the chunk',\n",
       "  'expected_output': ' sizes are small enough\\nthis technique is generall'},\n",
       " {'input_text': 's and tries to split hierarchically till the chunk sizes are small enough\\nthis technique is generall',\n",
       "  'expected_output': 'y recommended for generic text\\nexample text  akbus'},\n",
       " {'input_text': ' sizes are small enough\\nthis technique is generally recommended for generic text\\nexample text  akbus',\n",
       "  'expected_output': 'ypersonintrollmtxt \\ntranscript of a youtube video '},\n",
       " {'input_text': 'y recommended for generic text\\nexample text  akbusypersonintrollmtxt \\ntranscript of a youtube video ',\n",
       "  'expected_output': 'by andrej karpathy titled 1hr talk intro to large '},\n",
       " {'input_text': 'ypersonintrollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1hr talk intro to large ',\n",
       "  'expected_output': 'language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnj'},\n",
       " {'input_text': 'by andrej karpathy titled 1hr talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnj',\n",
       "  'expected_output': 'gt9s  \\nusing langchains recursivecharactertextspli'},\n",
       " {'input_text': 'language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing langchains recursivecharactertextspli',\n",
       "  'expected_output': 'tter\\nthis is a generic text that is not formatted '},\n",
       " {'input_text': 'gt9s  \\nusing langchains recursivecharactertextsplitter\\nthis is a generic text that is not formatted ',\n",
       "  'expected_output': 'lets compare the two strategies\\ntotal number of ch'},\n",
       " {'input_text': 'tter\\nthis is a generic text that is not formatted lets compare the two strategies\\ntotal number of ch',\n",
       "  'expected_output': 'unks created  1 \\nlength of the first chunk is  643'},\n",
       " {'input_text': 'lets compare the two strategies\\ntotal number of chunks created  1 \\nlength of the first chunk is  643',\n",
       "  'expected_output': '83 characters\\n length of the last chunk is  64383 '},\n",
       " {'input_text': 'unks created  1 \\nlength of the first chunk is  64383 characters\\n length of the last chunk is  64383 ',\n",
       "  'expected_output': 'characters\\ntext splitter fails to convert the text'},\n",
       " {'input_text': '83 characters\\n length of the last chunk is  64383 characters\\ntext splitter fails to convert the text',\n",
       "  'expected_output': ' into chunks since\\nthere are no nn character prese'},\n",
       " {'input_text': 'characters\\ntext splitter fails to convert the text into chunks since\\nthere are no nn character prese',\n",
       "  'expected_output': 'nt in the raw transcriptwith charactertextsplitter'},\n",
       " {'input_text': ' into chunks since\\nthere are no nn character present in the raw transcriptwith charactertextsplitter',\n",
       "  'expected_output': '\\nkeep calm  build ai17abhinav kimothiindexing pipe'},\n",
       " {'input_text': 'nt in the raw transcriptwith charactertextsplitter\\nkeep calm  build ai17abhinav kimothiindexing pipe',\n",
       "  'expected_output': 'line document splitting\\ntotal number of chunks cre'},\n",
       " {'input_text': '\\nkeep calm  build ai17abhinav kimothiindexing pipeline document splitting\\ntotal number of chunks cre',\n",
       "  'expected_output': 'ated  40\\nlength of the first chunk is  1998 charac'},\n",
       " {'input_text': 'line document splitting\\ntotal number of chunks created  40\\nlength of the first chunk is  1998 charac',\n",
       "  'expected_output': 'ters\\n length of the last chunk is  1967 characters'},\n",
       " {'input_text': 'ated  40\\nlength of the first chunk is  1998 characters\\n length of the last chunk is  1967 characters',\n",
       "  'expected_output': '\\nrecursive text splitter performs well in dealing\\n'},\n",
       " {'input_text': 'ters\\n length of the last chunk is  1967 characters\\nrecursive text splitter performs well in dealing\\n',\n",
       "  'expected_output': 'with generic textwith recursivecharactertextsplitt'},\n",
       " {'input_text': '\\nrecursive text splitter performs well in dealing\\nwith generic textwith recursivecharactertextsplitt',\n",
       "  'expected_output': 'er\\nsplit by tokens\\nfor those well versed with larg'},\n",
       " {'input_text': 'with generic textwith recursivecharactertextsplitter\\nsplit by tokens\\nfor those well versed with larg',\n",
       "  'expected_output': 'e language models tokens is not a new concept\\nall '},\n",
       " {'input_text': 'er\\nsplit by tokens\\nfor those well versed with large language models tokens is not a new concept\\nall ',\n",
       "  'expected_output': 'llms have a token limit in their respective contex'},\n",
       " {'input_text': 'e language models tokens is not a new concept\\nall llms have a token limit in their respective contex',\n",
       "  'expected_output': 't windows which we cannot\\nexceed it is therefore a'},\n",
       " {'input_text': 'llms have a token limit in their respective context windows which we cannot\\nexceed it is therefore a',\n",
       "  'expected_output': ' good idea to count the tokens while creating chun'},\n",
       " {'input_text': 't windows which we cannot\\nexceed it is therefore a good idea to count the tokens while creating chun',\n",
       "  'expected_output': 'ks  all\\nllms also have their tokenizers \\ntiktoken '},\n",
       " {'input_text': ' good idea to count the tokens while creating chunks  all\\nllms also have their tokenizers \\ntiktoken ',\n",
       "  'expected_output': 'tokenizer\\ntiktoken tokenizer has been created by o'},\n",
       " {'input_text': 'ks  all\\nllms also have their tokenizers \\ntiktoken tokenizer\\ntiktoken tokenizer has been created by o',\n",
       "  'expected_output': 'penai for their family of models using\\nthis strate'},\n",
       " {'input_text': 'tokenizer\\ntiktoken tokenizer has been created by openai for their family of models using\\nthis strate',\n",
       "  'expected_output': 'gy the split still happens based on the character '},\n",
       " {'input_text': 'penai for their family of models using\\nthis strategy the split still happens based on the character ',\n",
       "  'expected_output': 'however the length\\nof the chunk is determined by t'},\n",
       " {'input_text': 'gy the split still happens based on the character however the length\\nof the chunk is determined by t',\n",
       "  'expected_output': 'he number of tokens\\nkeep calm  build ai18abhinav k'},\n",
       " {'input_text': 'however the length\\nof the chunk is determined by the number of tokens\\nkeep calm  build ai18abhinav k',\n",
       "  'expected_output': 'imothiindexing pipeline document splitting\\ntotal n'},\n",
       " {'input_text': 'he number of tokens\\nkeep calm  build ai18abhinav kimothiindexing pipeline document splitting\\ntotal n',\n",
       "  'expected_output': 'umber of chunks created  14 \\ntotal number of token'},\n",
       " {'input_text': 'imothiindexing pipeline document splitting\\ntotal number of chunks created  14 \\ntotal number of token',\n",
       "  'expected_output': 's in the document  12865 tokens \\nlength of the fir'},\n",
       " {'input_text': 'umber of chunks created  14 \\ntotal number of tokens in the document  12865 tokens \\nlength of the fir',\n",
       "  'expected_output': 'st chunk is  1014 tokens\\nlength of the last chunk '},\n",
       " {'input_text': 's in the document  12865 tokens \\nlength of the first chunk is  1014 tokens\\nlength of the last chunk ',\n",
       "  'expected_output': 'is  1014 tokens\\ntokenizers are helpful in creating'},\n",
       " {'input_text': 'st chunk is  1014 tokens\\nlength of the last chunk is  1014 tokens\\ntokenizers are helpful in creating',\n",
       "  'expected_output': ' chunks that sit\\nwell in the context window of an '},\n",
       " {'input_text': 'is  1014 tokens\\ntokenizers are helpful in creating chunks that sit\\nwell in the context window of an ',\n",
       "  'expected_output': 'llmexample text  akbusypersonintrollmtxt \\ntranscri'},\n",
       " {'input_text': ' chunks that sit\\nwell in the context window of an llmexample text  akbusypersonintrollmtxt \\ntranscri',\n",
       "  'expected_output': 'pt of a youtube video by andrej karpathy titled 1h'},\n",
       " {'input_text': 'llmexample text  akbusypersonintrollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1h',\n",
       "  'expected_output': 'r talk intro to large language\\nmodels  httpswwwyou'},\n",
       " {'input_text': 'pt of a youtube video by andrej karpathy titled 1hr talk intro to large language\\nmodels  httpswwwyou',\n",
       "  'expected_output': 'tubecomwatchvzjkbmfhnjgt9s  \\nusing langchains toke'},\n",
       " {'input_text': 'r talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing langchains toke',\n",
       "  'expected_output': 'ntextsplitter\\nhugging face tokenizer\\nhugging face '},\n",
       " {'input_text': 'tubecomwatchvzjkbmfhnjgt9s  \\nusing langchains tokentextsplitter\\nhugging face tokenizer\\nhugging face ',\n",
       "  'expected_output': 'has become the goto platform for anyone building a'},\n",
       " {'input_text': 'ntextsplitter\\nhugging face tokenizer\\nhugging face has become the goto platform for anyone building a',\n",
       "  'expected_output': 'pps using llms\\nor even other models all models ava'},\n",
       " {'input_text': 'has become the goto platform for anyone building apps using llms\\nor even other models all models ava',\n",
       "  'expected_output': 'ilable via hugging face are also accompanied\\nby th'},\n",
       " {'input_text': 'pps using llms\\nor even other models all models available via hugging face are also accompanied\\nby th',\n",
       "  'expected_output': 'eir tokenizers\\nkeep calm  build ai19abhinav kimoth'},\n",
       " {'input_text': 'ilable via hugging face are also accompanied\\nby their tokenizers\\nkeep calm  build ai19abhinav kimoth',\n",
       "  'expected_output': 'iindexing pipeline document splitting\\ntexts0 \\nhi e'},\n",
       " {'input_text': 'eir tokenizers\\nkeep calm  build ai19abhinav kimothiindexing pipeline document splitting\\ntexts0 \\nhi e',\n",
       "  'expected_output': 'veryone so recently i gave a 30minute talk on larg'},\n",
       " {'input_text': 'iindexing pipeline document splitting\\ntexts0 \\nhi everyone so recently i gave a 30minute talk on larg',\n",
       "  'expected_output': 'e language\\nmodels just kind of like an intro talk '},\n",
       " {'input_text': 'veryone so recently i gave a 30minute talk on large language\\nmodels just kind of like an intro talk ',\n",
       "  'expected_output': 'um unfortunately that talk\\nwas not recorded but a '},\n",
       " {'input_text': 'e language\\nmodels just kind of like an intro talk um unfortunately that talk\\nwas not recorded but a ',\n",
       "  'expected_output': 'lot of people came to me after the talk and\\nthey t'},\n",
       " {'input_text': 'um unfortunately that talk\\nwas not recorded but a lot of people came to me after the talk and\\nthey t',\n",
       "  'expected_output': 'old me that uh they really liked the talk so i wou'},\n",
       " {'input_text': 'lot of people came to me after the talk and\\nthey told me that uh they really liked the talk so i wou',\n",
       "  'expected_output': 'ld just i\\nthought i would just rerecord it and bas'},\n",
       " {'input_text': 'old me that uh they really liked the talk so i would just i\\nthought i would just rerecord it and bas',\n",
       "  'expected_output': 'ically put it up on\\nyoutube so here we go the busy'},\n",
       " {'input_text': 'ld just i\\nthought i would just rerecord it and basically put it up on\\nyoutube so here we go the busy',\n",
       "  'expected_output': ' persons intro to large language\\nmodels director s'},\n",
       " {'input_text': 'ically put it up on\\nyoutube so here we go the busy persons intro to large language\\nmodels director s',\n",
       "  'expected_output': 'cott okay so lets begin first of all what is a lar'},\n",
       " {'input_text': ' persons intro to large language\\nmodels director scott okay so lets begin first of all what is a lar',\n",
       "  'expected_output': 'ge\\nlanguage model \\ntexts1\\nreally well a large lang'},\n",
       " {'input_text': 'cott okay so lets begin first of all what is a large\\nlanguage model \\ntexts1\\nreally well a large lang',\n",
       "  'expected_output': 'uage model is just two files right um there\\nbe two'},\n",
       " {'input_text': 'ge\\nlanguage model \\ntexts1\\nreally well a large language model is just two files right um there\\nbe two',\n",
       "  'expected_output': ' files in this hypothetical directory so for examp'},\n",
       " {'input_text': 'uage model is just two files right um there\\nbe two files in this hypothetical directory so for examp',\n",
       "  'expected_output': 'le work with\\nthe specific example of the llama 270'},\n",
       " {'input_text': ' files in this hypothetical directory so for example work with\\nthe specific example of the llama 270',\n",
       "  'expected_output': 'b model this is a large\\nlanguage model released by'},\n",
       " {'input_text': 'le work with\\nthe specific example of the llama 270b model this is a large\\nlanguage model released by',\n",
       "  'expected_output': ' meta ai and this is basically the llama\\nseries of'},\n",
       " {'input_text': 'b model this is a large\\nlanguage model released by meta ai and this is basically the llama\\nseries of',\n",
       "  'expected_output': ' language models the second iteration of it and th'},\n",
       " {'input_text': ' meta ai and this is basically the llama\\nseries of language models the second iteration of it and th',\n",
       "  'expected_output': 'is is the\\n70 billion parameter model of uh of this'},\n",
       " {'input_text': ' language models the second iteration of it and this is the\\n70 billion parameter model of uh of this',\n",
       "  'expected_output': ' series so theres multiple\\nmodels uh belonging to '},\n",
       " {'input_text': 'is is the\\n70 billion parameter model of uh of this series so theres multiple\\nmodels uh belonging to ',\n",
       "  'expected_output': 'the lama 2 series uh 7 billion um 13 billion\\n34 bi'},\n",
       " {'input_text': ' series so theres multiple\\nmodels uh belonging to the lama 2 series uh 7 billion um 13 billion\\n34 bi',\n",
       "  'expected_output': 'llion and 70 billion is the the no overlap as spec'},\n",
       " {'input_text': 'the lama 2 series uh 7 billion um 13 billion\\n34 billion and 70 billion is the the no overlap as spec',\n",
       "  'expected_output': 'ifiedchunk 1\\nchunk 2example text  akbusypersonintr'},\n",
       " {'input_text': 'llion and 70 billion is the the no overlap as specifiedchunk 1\\nchunk 2example text  akbusypersonintr',\n",
       "  'expected_output': 'ollmtxt \\ntranscript of a youtube video by andrej k'},\n",
       " {'input_text': 'ifiedchunk 1\\nchunk 2example text  akbusypersonintrollmtxt \\ntranscript of a youtube video by andrej k',\n",
       "  'expected_output': 'arpathy titled 1hr talk intro to large language\\nmo'},\n",
       " {'input_text': 'ollmtxt \\ntranscript of a youtube video by andrej karpathy titled 1hr talk intro to large language\\nmo',\n",
       "  'expected_output': 'dels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusin'},\n",
       " {'input_text': 'arpathy titled 1hr talk intro to large language\\nmodels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusin',\n",
       "  'expected_output': 'g transformers and langchains recursivecharacterte'},\n",
       " {'input_text': 'dels  httpswwwyoutubecomwatchvzjkbmfhnjgt9s  \\nusing transformers and langchains recursivecharacterte',\n",
       "  'expected_output': 'xtsplitter\\nexample tokenizer  gpt2tokenizerfast\\nht'},\n",
       " {'input_text': 'g transformers and langchains recursivecharactertextsplitter\\nexample tokenizer  gpt2tokenizerfast\\nht',\n",
       "  'expected_output': 'tpshuggingfacecodocstransformerstokenizersummary\\nd'},\n",
       " {'input_text': 'xtsplitter\\nexample tokenizer  gpt2tokenizerfast\\nhttpshuggingfacecodocstransformerstokenizersummary\\nd',\n",
       "  'expected_output': 'o take a look at hugging face documents on tokeniz'},\n",
       " {'input_text': 'tpshuggingfacecodocstransformerstokenizersummary\\ndo take a look at hugging face documents on tokeniz',\n",
       "  'expected_output': 'ers\\nkeep calm  build ai20abhinav kimothiindexing p'},\n",
       " {'input_text': 'o take a look at hugging face documents on tokenizers\\nkeep calm  build ai20abhinav kimothiindexing p',\n",
       "  'expected_output': 'ipeline document splitting\\nother tokenizer\\nother l'},\n",
       " {'input_text': 'ers\\nkeep calm  build ai20abhinav kimothiindexing pipeline document splitting\\nother tokenizer\\nother l',\n",
       "  'expected_output': 'ibraries like spacy nltk and sentencetransformers '},\n",
       " {'input_text': 'ipeline document splitting\\nother tokenizer\\nother libraries like spacy nltk and sentencetransformers ',\n",
       "  'expected_output': 'also provide splitters\\nspecialized chunking\\nchunki'},\n",
       " {'input_text': 'ibraries like spacy nltk and sentencetransformers also provide splitters\\nspecialized chunking\\nchunki',\n",
       "  'expected_output': 'ng often aims to keep text with common context tog'},\n",
       " {'input_text': 'also provide splitters\\nspecialized chunking\\nchunking often aims to keep text with common context tog',\n",
       "  'expected_output': 'ether with this in\\nmind we might want to specifica'},\n",
       " {'input_text': 'ng often aims to keep text with common context together with this in\\nmind we might want to specifica',\n",
       "  'expected_output': 'lly honour the structure of the document itself\\nfo'},\n",
       " {'input_text': 'ether with this in\\nmind we might want to specifically honour the structure of the document itself\\nfo',\n",
       "  'expected_output': 'r example html markdown latex or even code\\nexample'},\n",
       " {'input_text': 'lly honour the structure of the document itself\\nfor example html markdown latex or even code\\nexample',\n",
       "  'expected_output': '  httpsmediumcomp29a7e8610843\\nexample html  contex'},\n",
       " {'input_text': 'r example html markdown latex or even code\\nexample  httpsmediumcomp29a7e8610843\\nexample html  contex',\n",
       "  'expected_output': 't is key the significance of rag in language model'},\n",
       " {'input_text': '  httpsmediumcomp29a7e8610843\\nexample html  context is key the significance of rag in language model',\n",
       "  'expected_output': 's\\na blog on medium  httpsmediumcomp29a7e8610843 \\nu'},\n",
       " {'input_text': 't is key the significance of rag in language models\\na blog on medium  httpsmediumcomp29a7e8610843 \\nu',\n",
       "  'expected_output': 'sing langchains htmlheadertextsplitter  recursivec'},\n",
       " {'input_text': 's\\na blog on medium  httpsmediumcomp29a7e8610843 \\nusing langchains htmlheadertextsplitter  recursivec',\n",
       "  'expected_output': 'haractertextsplitter\\n all langchain splitters \\nkee'},\n",
       " {'input_text': 'sing langchains htmlheadertextsplitter  recursivecharactertextsplitter\\n all langchain splitters \\nkee',\n",
       "  'expected_output': 'p calm  build ai21abhinav kimothiindexing pipeline'},\n",
       " {'input_text': 'haractertextsplitter\\n all langchain splitters \\nkeep calm  build ai21abhinav kimothiindexing pipeline',\n",
       "  'expected_output': ' document splitting\\nthings to keep in mind\\nensure '},\n",
       " {'input_text': 'p calm  build ai21abhinav kimothiindexing pipeline document splitting\\nthings to keep in mind\\nensure ',\n",
       "  'expected_output': 'data quality by preprocessing it before determinin'},\n",
       " {'input_text': ' document splitting\\nthings to keep in mind\\nensure data quality by preprocessing it before determinin',\n",
       "  'expected_output': 'g the optimal chunk\\nsize examples include removing'},\n",
       " {'input_text': 'data quality by preprocessing it before determining the optimal chunk\\nsize examples include removing',\n",
       "  'expected_output': ' html tags or eliminating specific elements\\nthat c'},\n",
       " {'input_text': 'g the optimal chunk\\nsize examples include removing html tags or eliminating specific elements\\nthat c',\n",
       "  'expected_output': 'ontribute noise particularly when data is sourced '},\n",
       " {'input_text': ' html tags or eliminating specific elements\\nthat contribute noise particularly when data is sourced ',\n",
       "  'expected_output': 'from the web\\nconsider factors such as content natu'},\n",
       " {'input_text': 'ontribute noise particularly when data is sourced from the web\\nconsider factors such as content natu',\n",
       "  'expected_output': 're eg short messages or lengthy\\ndocuments embeddin'},\n",
       " {'input_text': 'from the web\\nconsider factors such as content nature eg short messages or lengthy\\ndocuments embeddin',\n",
       "  'expected_output': 'g model characteristics and capabilities like toke'},\n",
       " {'input_text': 're eg short messages or lengthy\\ndocuments embedding model characteristics and capabilities like toke',\n",
       "  'expected_output': 'n\\nlimits in choosing chunk sizes aim for a balance'},\n",
       " {'input_text': 'g model characteristics and capabilities like token\\nlimits in choosing chunk sizes aim for a balance',\n",
       "  'expected_output': ' between preserving context\\nand maintaining accura'},\n",
       " {'input_text': 'n\\nlimits in choosing chunk sizes aim for a balance between preserving context\\nand maintaining accura',\n",
       "  'expected_output': 'cy \\ntest different chunk sizes create embeddings f'},\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "if os.path.exists('./google-bert_bert-base-cased'):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('./google-bert_bert-base-cased')\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "    model.save_pretrained('./google-bert_bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForSequenceClassification' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "modelQa = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "qa = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FUT_Novil\\AppData\\Local\\anaconda3\\envs\\hfl\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FUT_Novil\\.cache\\huggingface\\hub\\models--google-bert--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'RAG is a tuning technique that uses a computer to perform a precise tuning of'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe('What are the metrics use to measuer the performace of T5 + Picard Model') #[{'generated_text': 'metric'}]\n",
    "# pipe('What is RAG?') # [{'generated_text': 'racial agglomeration'}]\n",
    "pipe('what are the differences between RAG and Fine tuning?') #[{'generated_text': 'RAG is a tuning technique that uses a computer to perform a precise tuning of'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
